{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889c0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberias importadas correctamente\n",
      "Ruta del repositorio YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
      "Ruta de los modelos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Rutas añadidas al PYTHONPATH: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/ y C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Trabajando en el dispositivo: cpu\n",
      "Cargando la arquitectura del modelo desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config/yolov3.cfg (con classes=80)\n",
      "Modelo YOLOv3 cargado correctamente en el dispositivo:  cpu\n",
      "Intentando cargar pesos pre-entrenados desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/yolov3.weights\n",
      "Pesos pre-entrenados cargados con éxito.\n",
      "\n",
      "Adaptando las capas de predicción a 3 clases...\n",
      "Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\n",
      "\n",
      "Configurando capas para Fine-Tuning:\n",
      "\n",
      "Verificación de capas que se entrenarán ('requires_grad=True'):\n",
      "module_list.35.conv_35.weight\n",
      "module_list.35.batch_norm_35.weight\n",
      "module_list.35.batch_norm_35.bias\n",
      "module_list.37.conv_37.weight\n",
      "module_list.37.batch_norm_37.weight\n",
      "module_list.37.batch_norm_37.bias\n",
      "module_list.38.conv_38.weight\n",
      "module_list.38.batch_norm_38.weight\n",
      "module_list.38.batch_norm_38.bias\n",
      "module_list.39.conv_39.weight\n",
      "module_list.39.batch_norm_39.weight\n",
      "module_list.39.batch_norm_39.bias\n",
      "module_list.41.conv_41.weight\n",
      "module_list.41.batch_norm_41.weight\n",
      "module_list.41.batch_norm_41.bias\n",
      "module_list.42.conv_42.weight\n",
      "module_list.42.batch_norm_42.weight\n",
      "module_list.42.batch_norm_42.bias\n",
      "module_list.44.conv_44.weight\n",
      "module_list.44.batch_norm_44.weight\n",
      "module_list.44.batch_norm_44.bias\n",
      "module_list.45.conv_45.weight\n",
      "module_list.45.batch_norm_45.weight\n",
      "module_list.45.batch_norm_45.bias\n",
      "module_list.47.conv_47.weight\n",
      "module_list.47.batch_norm_47.weight\n",
      "module_list.47.batch_norm_47.bias\n",
      "module_list.48.conv_48.weight\n",
      "module_list.48.batch_norm_48.weight\n",
      "module_list.48.batch_norm_48.bias\n",
      "module_list.50.conv_50.weight\n",
      "module_list.50.batch_norm_50.weight\n",
      "module_list.50.batch_norm_50.bias\n",
      "module_list.51.conv_51.weight\n",
      "module_list.51.batch_norm_51.weight\n",
      "module_list.51.batch_norm_51.bias\n",
      "module_list.53.conv_53.weight\n",
      "module_list.53.batch_norm_53.weight\n",
      "module_list.53.batch_norm_53.bias\n",
      "module_list.54.conv_54.weight\n",
      "module_list.54.batch_norm_54.weight\n",
      "module_list.54.batch_norm_54.bias\n",
      "module_list.56.conv_56.weight\n",
      "module_list.56.batch_norm_56.weight\n",
      "module_list.56.batch_norm_56.bias\n",
      "module_list.57.conv_57.weight\n",
      "module_list.57.batch_norm_57.weight\n",
      "module_list.57.batch_norm_57.bias\n",
      "module_list.59.conv_59.weight\n",
      "module_list.59.batch_norm_59.weight\n",
      "module_list.59.batch_norm_59.bias\n",
      "module_list.60.conv_60.weight\n",
      "module_list.60.batch_norm_60.weight\n",
      "module_list.60.batch_norm_60.bias\n",
      "module_list.62.conv_62.weight\n",
      "module_list.62.batch_norm_62.weight\n",
      "module_list.62.batch_norm_62.bias\n",
      "module_list.63.conv_63.weight\n",
      "module_list.63.batch_norm_63.weight\n",
      "module_list.63.batch_norm_63.bias\n",
      "module_list.64.conv_64.weight\n",
      "module_list.64.batch_norm_64.weight\n",
      "module_list.64.batch_norm_64.bias\n",
      "module_list.66.conv_66.weight\n",
      "module_list.66.batch_norm_66.weight\n",
      "module_list.66.batch_norm_66.bias\n",
      "module_list.67.conv_67.weight\n",
      "module_list.67.batch_norm_67.weight\n",
      "module_list.67.batch_norm_67.bias\n",
      "module_list.69.conv_69.weight\n",
      "module_list.69.batch_norm_69.weight\n",
      "module_list.69.batch_norm_69.bias\n",
      "module_list.70.conv_70.weight\n",
      "module_list.70.batch_norm_70.weight\n",
      "module_list.70.batch_norm_70.bias\n",
      "module_list.72.conv_72.weight\n",
      "module_list.72.batch_norm_72.weight\n",
      "module_list.72.batch_norm_72.bias\n",
      "module_list.73.conv_73.weight\n",
      "module_list.73.batch_norm_73.weight\n",
      "module_list.73.batch_norm_73.bias\n",
      "module_list.75.conv_75.weight\n",
      "module_list.75.batch_norm_75.weight\n",
      "module_list.75.batch_norm_75.bias\n",
      "module_list.76.conv_76.weight\n",
      "module_list.76.batch_norm_76.weight\n",
      "module_list.76.batch_norm_76.bias\n",
      "module_list.77.conv_77.weight\n",
      "module_list.77.batch_norm_77.weight\n",
      "module_list.77.batch_norm_77.bias\n",
      "module_list.78.conv_78.weight\n",
      "module_list.78.batch_norm_78.weight\n",
      "module_list.78.batch_norm_78.bias\n",
      "module_list.79.conv_79.weight\n",
      "module_list.79.batch_norm_79.weight\n",
      "module_list.79.batch_norm_79.bias\n",
      "module_list.80.conv_80.weight\n",
      "module_list.80.batch_norm_80.weight\n",
      "module_list.80.batch_norm_80.bias\n",
      "module_list.81.0.weight\n",
      "module_list.81.0.bias\n",
      "module_list.84.conv_84.weight\n",
      "module_list.84.batch_norm_84.weight\n",
      "module_list.84.batch_norm_84.bias\n",
      "module_list.87.conv_87.weight\n",
      "module_list.87.batch_norm_87.weight\n",
      "module_list.87.batch_norm_87.bias\n",
      "module_list.88.conv_88.weight\n",
      "module_list.88.batch_norm_88.weight\n",
      "module_list.88.batch_norm_88.bias\n",
      "module_list.89.conv_89.weight\n",
      "module_list.89.batch_norm_89.weight\n",
      "module_list.89.batch_norm_89.bias\n",
      "module_list.90.conv_90.weight\n",
      "module_list.90.batch_norm_90.weight\n",
      "module_list.90.batch_norm_90.bias\n",
      "module_list.91.conv_91.weight\n",
      "module_list.91.batch_norm_91.weight\n",
      "module_list.91.batch_norm_91.bias\n",
      "module_list.92.conv_92.weight\n",
      "module_list.92.batch_norm_92.weight\n",
      "module_list.92.batch_norm_92.bias\n",
      "module_list.93.0.weight\n",
      "module_list.93.0.bias\n",
      "module_list.96.conv_96.weight\n",
      "module_list.96.batch_norm_96.weight\n",
      "module_list.96.batch_norm_96.bias\n",
      "module_list.99.conv_99.weight\n",
      "module_list.99.batch_norm_99.weight\n",
      "module_list.99.batch_norm_99.bias\n",
      "module_list.100.conv_100.weight\n",
      "module_list.100.batch_norm_100.weight\n",
      "module_list.100.batch_norm_100.bias\n",
      "module_list.101.conv_101.weight\n",
      "module_list.101.batch_norm_101.weight\n",
      "module_list.101.batch_norm_101.bias\n",
      "module_list.102.conv_102.weight\n",
      "module_list.102.batch_norm_102.weight\n",
      "module_list.102.batch_norm_102.bias\n",
      "module_list.103.conv_103.weight\n",
      "module_list.103.batch_norm_103.weight\n",
      "module_list.103.batch_norm_103.bias\n",
      "module_list.104.conv_104.weight\n",
      "module_list.104.batch_norm_104.weight\n",
      "module_list.104.batch_norm_104.bias\n",
      "module_list.105.0.weight\n",
      "module_list.105.0.bias\n",
      "\n",
      "Total de parámetros entrenables: 58.63 M\n",
      "Total de parámetros congelados: 2.91 M\n",
      "Total de parámetros en el modelo: 61.53 M\n",
      "\n",
      "Realizando una pasada hacia adelante para verificar la configuración del modelo...\n",
      "\n",
      "Shape de la salida del modelo después de cargar pesos y adaptar a 3 clases (en modo EVAL):\n",
      "  Escala 13x13: torch.Size([1, 507, 8]) (Esperado: [N, 3*13*13, 5+C])\n",
      "  Escala 26x26: torch.Size([1, 2028, 8]) (Esperado: [N, 3*26*26, 5+C])\n",
      "  Escala 52x52: torch.Size([1, 8112, 8]) (Esperado: [N, 3*52*52, 5+C])\n",
      "¡Las dimensiones de salida para 3 clases son correctas en modo EVAL!\n",
      "\n",
      "--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\n"
     ]
    }
   ],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos\n",
    "\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "\n",
    "# Ruta a los pesos preentrenados yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "print(\"Liberias importadas correctamente\")\n",
    "\n",
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer \n",
    "\n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES_YOUR_DATASET = 3\n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = 416 \n",
    "\n",
    "# Definimos los anchor masks para tus 3 clases (placeholder hasta definir los adecuados con KMeans)\n",
    "# YOLOv3 usa 9 anchor boxes en total, divididos en 3 grupos de 3 para cada escala.\n",
    "# Estos son los INDICES de los anchors. Los valores reales los calculaamos con K-Means.\n",
    "# Ejemplo: si los 9 anchors se ordenan de menor a mayor área, los grandes (indices 6,7,8) van a la escala 13x13.\n",
    "#Anchor Boxes Calculadas (Formato para YOLOv3Loss): [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
    "\n",
    "DUMMY_ANCHORS_MASKS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# PASO 3: Adapatacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "# Esto debemos hacerlo DESPUÉS de haber cargado los pesos del modelo de 80 clases.\n",
    "\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0 # Para asignar los nuevos YOLOLayer a la lista correcta\n",
    "\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        # i es el índice de la capa YOLOLayer en model.module_defs y module_list\n",
    "        \n",
    "        # 1. Reemplazar la capa Conv2d de predicción final\n",
    "        pred_conv_sequential_idx = i - 1 \n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0] \n",
    "        \n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        \n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        \n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels, \n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True \n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        \n",
    "        # 2. Reemplazar la instancia de YOLOLayer en model.module_list y model.yolo_layers\n",
    "        # ¡CORREGIDO! La YOLOLayer está en module_list[i]\n",
    "        \n",
    "        # Obtenemos los anchors y stride de la instancia antigua para la nueva YOLOLayer\n",
    "        # Esto es correcto ya que estos atributos sí existen en yolo_layer_old_instance\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        \n",
    "        # Creamos una NUEVA instancia de YOLOLayer\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        \n",
    "        # Sustituimos la YOLOLayer antigua en el `module_list` del modelo\n",
    "        # Esto es crucial porque el forward de Darknet itera sobre module_list\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer) # Reemplaza el Sequential que contiene la YOLOLayer antigua\n",
    "        \n",
    "        # También actualizamos la referencia en `model.yolo_layers`\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        \n",
    "        yolo_layer_index_in_model_yolo_layers += 1 \n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# PASO 4: Congelamos las capas para Fine-Tuning\n",
    "# Es CRUCIAL congelar la mayoría de las capas del backbone (Darknet-53)\n",
    "# y dejar entrenables las capas del head (las que predicen las cajas).\n",
    "# Esto evita que el modelo \"olvide\" lo que aprendió en el dataset grande.\n",
    "\n",
    "print(\"\\nConfigurando capas para Fine-Tuning:\")\n",
    "# Iteramos a través de los módulos y parámetros del modelo.\n",
    "# Las primeras ~74-75 capas de su `module_list` corresponden al backbone (Darknet-53 puro).\n",
    "# Las capas posteriores (más de 75) son parte del head de YOLOv3 y deben ser entrenables.\n",
    "# Las capas YOLOLayer en sí no tienen parámetros entrenables, pero sus capas `conv` previas sí.\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if i < 75:  # Índices de las capas del backbone (heurístico, basado en estructura Darknet-53)\n",
    "        param.requires_grad = False  # Congelar\n",
    "    else:\n",
    "        param.requires_grad = True   # Descongelar (para el head)\n",
    "    \n",
    "    # Línea para depuración: puedes descomentarla para ver el estado de cada capa\n",
    "    # print(f\"  Capa: {name}, Entrenable: {param.requires_grad}\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "# En modo eval(), YOLOLayer devuelve predicciones decodificadas y aplanadas.\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "# Modo EVAL: No se entrena, solo se evalúa la salida del modelo.\n",
    "\n",
    "model.eval() \n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device) \n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "# Ajustamos las expectativas de forma para reflejar la salida aplanada y decodificada\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\") \n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "\n",
    "# No necesitamos los asserts basados en el formato crudo aquí, ya que el formato de evaluación es diferente\n",
    "# Los asserts que teníamos antes son para el formato crudo (en modo train)\n",
    "# expected_output_channels = 3 * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "# assert predictions[0].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "# assert predictions[1].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "# assert predictions[2].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\")\n",
    "\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
