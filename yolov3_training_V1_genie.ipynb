{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bf0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla global fijada en 1234\n"
     ]
    }
   ],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos (celdas de sangre)\n",
    "# Basado en el repositorio de Manuel Garcia UEM Junio 2025\n",
    "\n",
    "#1. Reproducibilidad total\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(f\"Semilla global fijada en {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5330d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberias importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos (celdas de sangre)\n",
    "# Basado en el repositorio de Manuel Garcia UEM Junio 2025\n",
    "\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "\n",
    "# Ruta a los pesos preentrenados yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "print(\"Liberias importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecba2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de PyTorch: 2.3.1\n",
      "Versión de NumPy: 1.26.4\n",
      "Versión de sys: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "Versión de os: nt\n",
      "No se detecta GPU, se usará la CPU para el entrenamiento.\n",
      "Trabajando en el dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "# Verificamos que PyTorch está instalado y la versión\n",
    "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Verificamos que NumPy está instalado y la versión\n",
    "print(f\"Versión de NumPy: {np.__version__}\")\n",
    "\n",
    "# Verificamos que sys está instalado y la versión\n",
    "print(f\"Versión de sys: {sys.version}\")\n",
    "# Verificamos que os está instalado y la versión\n",
    "print(f\"Versión de os: {os.name}\")\n",
    "# Verificamos que la GPU está disponible (si es que se va a usar)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU disponible para PyTorch.\")\n",
    "    print(f\"Dispositivo actual: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No se detecta GPU, se usará la CPU para el entrenamiento.\")\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69af246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta del repositorio YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
      "Ruta de los modelos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Ruta del archivo de configuración YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config/yolov3.cfg\n",
      "Ruta del archivo de pesos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/yolov3.weights\n",
      "Rutas añadidas al PYTHONPATH: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/ y C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n"
     ]
    }
   ],
   "source": [
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de configuración YOLOv3: {CONFIG_PATH}\")\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de pesos YOLOv3: {WEIGHTS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82919b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trabajando en el dispositivo: cpu\n",
      "Cargando la arquitectura del modelo desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config/yolov3.cfg (con classes=80)\n",
      "Modelo YOLOv3 cargado correctamente en el dispositivo:  cpu\n",
      "Intentando cargar pesos pre-entrenados desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/yolov3.weights\n",
      "Pesos pre-entrenados cargados con éxito.\n",
      "\n",
      "Adaptando las capas de predicción a 3 clases...\n",
      "Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\n",
      "\n",
      "Configurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[82]\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[94]\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[106]\n",
      "\n",
      "Verificación de capas que se entrenarán ('requires_grad=True'):\n",
      "module_list.81.0.weight\n",
      "module_list.81.0.bias\n",
      "module_list.93.0.weight\n",
      "module_list.93.0.bias\n",
      "module_list.105.0.weight\n",
      "module_list.105.0.bias\n",
      "\n",
      "Total de parámetros entrenables: 0.04 M\n",
      "Total de parámetros congelados: 61.49 M\n",
      "Total de parámetros en el modelo: 61.53 M\n",
      "\n",
      "Realizando una pasada hacia adelante para verificar la configuración del modelo...\n",
      "\n",
      "Shape de la salida del modelo después de cargar pesos y adaptar a 3 clases (en modo EVAL):\n",
      "  Escala 13x13: torch.Size([1, 3, 13, 13, 8]) (Esperado: [N, 3*13*13, 5+C])\n",
      "  Escala 26x26: torch.Size([1, 3, 26, 26, 8]) (Esperado: [N, 3*26*26, 5+C])\n",
      "  Escala 52x52: torch.Size([1, 3, 52, 52, 8]) (Esperado: [N, 3*52*52, 5+C])\n",
      "¡Las dimensiones de salida para 3 clases son correctas en modo EVAL!\n",
      "\n",
      "\n",
      "--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\n"
     ]
    }
   ],
   "source": [
    "# Definicion del modelo\n",
    "    \n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES_YOUR_DATASET = 3\n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = 416 \n",
    "\n",
    "# Definimos los anchor masks para tus 3 clases (placeholder hasta definir los adecuados con KMeans)\n",
    "# YOLOv3 usa 9 anchor boxes en total, divididos en 3 grupos de 3 para cada escala.\n",
    "# Estos son los INDICES de los anchors. Los valores reales los calculaamos con K-Means.\n",
    "# Ejemplo: si los 9 anchors se ordenan de menor a mayor área, los grandes (indices 6,7,8) van a la escala 13x13.\n",
    "#Anchor Boxes Calculadas (Formato para YOLOv3Loss): [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
    "\n",
    "ANCHORS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# ... (all your setup code above remains unchanged)\n",
    "\n",
    "# PASO 3: Adaptacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0]\n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels,\n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True\n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer)\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        yolo_layer_index_in_model_yolo_layers += 1\n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# --- NUEVA SECCIÓN: CONGELAR TODAS LAS CAPAS, SOLO DESCONGELAR LAS ULTIMAS Conv2d DE PREDICCIÓN ---\n",
    "print(\"\\nConfigurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\")\n",
    "\n",
    "# Congelar todos los parámetros por defecto\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Solo las Conv2d antes de YOLOLayer quedan como entrenables\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        conv_seq = model.module_list[pred_conv_sequential_idx]\n",
    "        # Buscar la capa Conv2d dentro del Sequential\n",
    "        for layer in conv_seq:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"  Descongelada capa Conv2d antes del YOLOLayer en module_list[{i}]\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\")\n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\\n\")\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d34568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase BloodCellDataset definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            original_h, original_w, _ = image.shape\n",
    "            #print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "            bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "            #print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "            #if bboxes_pixel:\n",
    "            #    print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "            # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "            # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "            bboxes_normalized_initial = []\n",
    "            class_labels = [] # class_labels se mantiene\n",
    "            for bbox_px in bboxes_pixel:\n",
    "                xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "                # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "                xmin_norm = xmin_px / original_w\n",
    "                ymin_norm = ymin_px / original_h\n",
    "                xmax_norm = xmax_px / original_w\n",
    "                ymax_norm = ymax_px / original_h\n",
    "            \n",
    "                bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "                class_labels.append(class_id)\n",
    "        \n",
    "            #print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "            #if bboxes_normalized_initial:\n",
    "            #    print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "            if self.transform:\n",
    "                # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "                # Se espera que devuelva coordenadas normalizadas también.\n",
    "                transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "                image = transformed['image']\n",
    "                bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "                class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "            #print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "            #if bboxes_transformed_raw:\n",
    "            #    print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "            # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "            # Si Albumentations funciona como se espera con format='albumentations',\n",
    "            # este paso ya no es necesario.\n",
    "            bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "            #print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "            #if bboxes:\n",
    "            #    print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "            # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "            # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "            yolo_bboxes = []\n",
    "            for i, bbox in enumerate(bboxes):\n",
    "                x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "                # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "                x_min = max(0.0, min(1.0, x_min))\n",
    "                y_min = max(0.0, min(1.0, y_min))\n",
    "                x_max = max(0.0, min(1.0, x_max))\n",
    "                y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "                center_x = (x_min + x_max) / 2\n",
    "                width = x_max - x_min\n",
    "                center_y = (y_min + y_max) / 2\n",
    "                height = y_max - y_min\n",
    "            \n",
    "                # Este filtrado ya estaba, pero el error ocurría antes\n",
    "                if width <= 0 or height <= 0:\n",
    "                    print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                    continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "                yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "            #print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "            #if yolo_bboxes:\n",
    "            #    print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "            if len(yolo_bboxes) == 0:\n",
    "                # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "                yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "            else:\n",
    "                yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "            return image, yolo_bboxes\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error en __getitem__ para idx {idx}: {e}\")\n",
    "            return torch.zeros((3, *self.image_size)), torch.zeros((0, 5))\n",
    "        \n",
    "print(\"Clase BloodCellDataset definida correctamente.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fb554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformaciones de Albumentations definidas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO CON AUMENTACIÓN DE COLOR/APARIENCIA ---\n",
    "train_transforms = A.Compose([\n",
    "    # Redimensionamiento y Relleno\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    \n",
    "    # Transformaciones de Color y Apariencia\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1), # Asegúrate de que blur_limit es impar y no demasiado grande\n",
    "    \n",
    "    # Normalización y Conversión a Tensor\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # El formato 'albumentations' espera y devuelve normalizado [0,1]\n",
    "\n",
    "# Las transformaciones de validación/prueba se mantienen minimalistas\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) \n",
    "\n",
    "print(\"Transformaciones de Albumentations definidas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8983c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función collate_fn definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "print(\"Función collate_fn definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60815898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 13 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([0.0000, 0.8117, 0.5219, 0.2109, 0.1688])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "# RUTAS A LOS DATOS\n",
    "DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "# Parámetros de la división\n",
    "TEST_SPLIT_RATIO = 0.15    \n",
    "VAL_SPLIT_RATIO = 0.15     \n",
    "RANDOM_SEED = SEED           \n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "# --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "# Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "all_image_filenames = full_df['filename'].unique().tolist()\n",
    "print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "# --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "train_val_filenames, test_filenames = train_test_split(\n",
    "    all_image_filenames, \n",
    "    test_size=TEST_SPLIT_RATIO, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "    \n",
    "train_filenames, val_filenames = train_test_split(\n",
    "    train_val_filenames, \n",
    "    test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "# --- Crear DataFrames de anotaciones para cada split ---\n",
    "train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "# --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "train_dataset = BloodCellDataset(\n",
    "    data_root=DATA_ROOT,\n",
    "    annotations_df=train_df, \n",
    "    image_size=YOLO_INPUT_SIZE,\n",
    "    transform=train_transforms\n",
    ")\n",
    "val_dataset = BloodCellDataset(\n",
    "    data_root=DATA_ROOT,\n",
    "    annotations_df=val_df, \n",
    "    image_size=YOLO_INPUT_SIZE,\n",
    "    transform=val_test_transforms \n",
    ")\n",
    "test_dataset = BloodCellDataset(\n",
    "    data_root=DATA_ROOT,\n",
    "    annotations_df=test_df, \n",
    "    image_size=YOLO_INPUT_SIZE,\n",
    "    transform=val_test_transforms \n",
    ")\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()  # Solo True si hay GPU\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "# --- Verificación de la carga de un lote de entrenamiento ---\n",
    "print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "MAX_BATCHES_TO_CHECK = 10 \n",
    "found_image_with_boxes = False\n",
    "\n",
    "for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "    # Buscar una imagen con cajas en el lote actual\n",
    "    for img_idx in range(len(targets)):\n",
    "        if targets[img_idx].numel() > 0: \n",
    "            print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "            print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "            print(targets[img_idx][0])\n",
    "                \n",
    "            # --- Lógica de visualización ---\n",
    "            mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "            std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "            img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "            img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "            img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "            img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "            CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "            CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "            print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "            for bbox_yolo in targets[img_idx].tolist():\n",
    "                class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                x_min_norm = cx - w/2\n",
    "                y_min_norm = cy - h/2\n",
    "                x_max_norm = cx + w/2\n",
    "                y_max_norm = cy + h/2\n",
    "\n",
    "                x_min_px = int(x_min_norm * img_w)\n",
    "                y_min_px = int(y_min_norm * img_h)\n",
    "                x_max_px = int(x_max_norm * img_w)\n",
    "                y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                text_x = x_min_px\n",
    "                text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                            (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "            cv2.waitKey(0) \n",
    "            cv2.destroyAllWindows()\n",
    "                \n",
    "            found_image_with_boxes = True\n",
    "            break \n",
    "        \n",
    "    if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "        break \n",
    "\n",
    "if not found_image_with_boxes:\n",
    "    print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "    print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "    print(\"Considera revisar:\")\n",
    "    print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "    print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "    print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "    print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe328fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Función de Intersection over Union (IoU) definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# IOU\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    else:\n",
    "        raise ValueError(\"box_format debe ser 'midpoint' o 'corners'\")\n",
    "\n",
    "    x1_inter = torch.max(box1_x1, box2_x1)\n",
    "    y1_inter = torch.max(box1_y1, box2_y1)\n",
    "    x2_inter = torch.min(box1_x2, box2_x2)\n",
    "    y2_inter = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2_inter - x1_inter).clamp(0) * (y2_inter - y1_inter).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union = box1_area + box2_area - intersection + 1e-6\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "print(\"\\nFunción de Intersection over Union (IoU) definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c7014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase YOLOv3Loss definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Funcion de Perdida de YOLOV3\n",
    "\n",
    "class YOLOv3Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size=(416, 416),\n",
    "                lambda_coord=1.0, lambda_noobj=1.0, lambda_obj=1.0, lambda_class=1.0,\n",
    "                ignore_iou_threshold=0.5, device=None):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.lambda_class = lambda_class\n",
    "        self.ignore_iou_threshold = ignore_iou_threshold\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]))\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        obj_loss = 0\n",
    "        noobj_loss = 0\n",
    "        box_loss = 0\n",
    "        class_loss = 0\n",
    "\n",
    "        # Convert anchors to tensor only once, on correct device\n",
    "        anchors_tensor = [torch.tensor(a, dtype=torch.float32, device=self.device) for a in self.anchors]\n",
    "\n",
    "        for scale_idx, prediction in enumerate(predictions):\n",
    "            #print(f\"DEBUG: Shape prediction[{scale_idx}]: {prediction.shape}\")\n",
    "            #if prediction.dim() != 4:\n",
    "            #    print(f\"WARNING: prediction[{scale_idx}] tiene {prediction.dim()} dimensiones: {prediction.shape}\")\n",
    "            #continue  # o lanza una excepción explicativa\n",
    "            # ... sigue con permute y reshape ...\n",
    "            # prediction: (N, 3*(5+C), H, W)\n",
    "            prediction = prediction.permute(0, 2, 3, 1).reshape(\n",
    "                prediction.shape[0], prediction.shape[2], prediction.shape[3], 3, self.num_classes + 5\n",
    "            )\n",
    "\n",
    "            pred_x_y = prediction[..., 0:2]\n",
    "            pred_w_h = prediction[..., 2:4]\n",
    "            pred_obj = prediction[..., 4:5]\n",
    "            pred_class = prediction[..., 5:]\n",
    "\n",
    "            N, grid_h, grid_w, num_anchors, _ = prediction.shape\n",
    "\n",
    "            anchors_current_scale = anchors_tensor[scale_idx].reshape(1, 1, 1, num_anchors, 2)\n",
    "\n",
    "            # Inicializar todo en el device correcto\n",
    "            target_obj_mask = torch.zeros((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            target_noobj_mask = torch.ones((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            tx = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            ty = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            tw = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            th = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            target_class_one_hot = torch.zeros((N, grid_h, grid_w, num_anchors, self.num_classes), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Vectorizar asignación de anchors\n",
    "            if targets.numel() > 0:\n",
    "                # targets: (num_true_boxes_in_batch, 6)\n",
    "                img_ids = targets[:, 0].long()\n",
    "                class_ids = targets[:, 1].long()\n",
    "                x_gt_norm = targets[:, 2]\n",
    "                y_gt_norm = targets[:, 3]\n",
    "                w_gt_norm = targets[:, 4]\n",
    "                h_gt_norm = targets[:, 5]\n",
    "\n",
    "                x_center_grid = x_gt_norm * grid_w\n",
    "                y_center_grid = y_gt_norm * grid_h\n",
    "                cell_x = x_center_grid.long()\n",
    "                cell_y = y_center_grid.long()\n",
    "\n",
    "                # Filtrar targets fuera de grid\n",
    "                grid_mask = (cell_x >= 0) & (cell_x < grid_w) & (cell_y >= 0) & (cell_y < grid_h) & (img_ids >= 0) & (img_ids < N)\n",
    "                img_ids = img_ids[grid_mask]\n",
    "                class_ids = class_ids[grid_mask]\n",
    "                cell_x = cell_x[grid_mask]\n",
    "                cell_y = cell_y[grid_mask]\n",
    "                x_center_grid = x_center_grid[grid_mask]\n",
    "                y_center_grid = y_center_grid[grid_mask]\n",
    "                w_gt_norm = w_gt_norm[grid_mask]\n",
    "                h_gt_norm = h_gt_norm[grid_mask]\n",
    "\n",
    "                # Anchors assignment (vectorized)\n",
    "                w_gt_pix = w_gt_norm * self.img_size[0]\n",
    "                h_gt_pix = h_gt_norm * self.img_size[1]\n",
    "                gt_box_dims = torch.stack([\n",
    "                    torch.zeros_like(w_gt_pix), torch.zeros_like(h_gt_pix), w_gt_pix, h_gt_pix\n",
    "                ], dim=1)  # (num_boxes, 4)\n",
    "\n",
    "                anchor_boxes_for_iou = torch.zeros((num_anchors, 4), device=self.device)\n",
    "                anchor_boxes_for_iou[:, 2] = anchors_current_scale[0,0,0,:,0]\n",
    "                anchor_boxes_for_iou[:, 3] = anchors_current_scale[0,0,0,:,1]\n",
    "\n",
    "                # Expand gt_box_dims for broadcasting\n",
    "                gt_box_dims_exp = gt_box_dims.unsqueeze(1).expand(-1, num_anchors, 4)  # (num_boxes, num_anchors, 4)\n",
    "                anchors_exp = anchor_boxes_for_iou.unsqueeze(0).expand(gt_box_dims.size(0), -1, 4)  # (num_boxes, num_anchors, 4)\n",
    "\n",
    "                ious = intersection_over_union(gt_box_dims_exp, anchors_exp, box_format=\"corners\").squeeze(-1)  # (num_boxes, num_anchors)\n",
    "                best_iou_anchor_idx = torch.argmax(ious, dim=1)  # (num_boxes,)\n",
    "\n",
    "                for idx in range(img_ids.shape[0]):\n",
    "                    i = img_ids[idx]\n",
    "                    c = class_ids[idx]\n",
    "                    cx = cell_x[idx]\n",
    "                    cy = cell_y[idx]\n",
    "                    best_anchor = best_iou_anchor_idx[idx]\n",
    "                    # Masks\n",
    "                    target_obj_mask[i, cy, cx, best_anchor] = 1.0\n",
    "                    target_noobj_mask[i, cy, cx, best_anchor] = 0.0\n",
    "                    # Coordinates\n",
    "                    tx[i, cy, cx, best_anchor] = x_center_grid[idx] - cx\n",
    "                    ty[i, cy, cx, best_anchor] = y_center_grid[idx] - cy\n",
    "                    tw[i, cy, cx, best_anchor] = torch.log(w_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,0] + 1e-16)\n",
    "                    th[i, cy, cx, best_anchor] = torch.log(h_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,1] + 1e-16)\n",
    "                    # Class\n",
    "                    target_class_one_hot[i, cy, cx, best_anchor, c] = 1.0\n",
    "                    # Ignore anchors with high IoU\n",
    "                    for anchor_idx_other in range(num_anchors):\n",
    "                        if anchor_idx_other == best_anchor:\n",
    "                            continue\n",
    "                        if ious[idx, anchor_idx_other] > self.ignore_iou_threshold:\n",
    "                            target_noobj_mask[i, cy, cx, anchor_idx_other] = 0.0\n",
    "\n",
    "            loss_x = self.bce(pred_x_y[..., 0][target_obj_mask.bool()], tx[target_obj_mask.bool()])\n",
    "            loss_y = self.bce(pred_x_y[..., 1][target_obj_mask.bool()], ty[target_obj_mask.bool()])\n",
    "            loss_w = self.mse(pred_w_h[..., 0][target_obj_mask.bool()], tw[target_obj_mask.bool()])\n",
    "            loss_h = self.mse(pred_w_h[..., 1][target_obj_mask.bool()], th[target_obj_mask.bool()])\n",
    "            box_loss += (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "            loss_obj = self.bce(pred_obj[target_obj_mask.bool()], target_obj_mask[target_obj_mask.bool()].float().unsqueeze(-1))\n",
    "            loss_noobj = self.bce(pred_obj[target_noobj_mask.bool()], target_noobj_mask[target_noobj_mask.bool()].float().unsqueeze(-1))\n",
    "            obj_loss += loss_obj\n",
    "            noobj_loss += loss_noobj\n",
    "\n",
    "            loss_class = self.bce(pred_class[target_obj_mask.bool()], target_class_one_hot[target_obj_mask.bool()])\n",
    "            class_loss += loss_class\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + self.lambda_obj * obj_loss\n",
    "            + self.lambda_noobj * noobj_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "        return total_loss, {\"box_loss\": box_loss, \"obj_loss\": obj_loss, \"noobj_loss\": noobj_loss, \"class_loss\": class_loss}\n",
    "\n",
    "print(\"Clase YOLOv3Loss definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ef7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función adapt_predictions_for_loss definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Adaptación de las predicciones para la función de pérdida\n",
    "# Esta función adapta las predicciones del modelo para que coincidan con el formato esperado por la función de pérdida.\n",
    "\n",
    "def adapt_predictions_for_loss(predictions, num_classes):\n",
    "    adapted = []\n",
    "    for idx, p in enumerate(predictions):\n",
    "        if p.dim() != 5:\n",
    "            print(f\"WARNING: predictions[{idx}] tiene {p.dim()} dimensiones y shape {p.shape} (esperado: 5D). Se ignora este tensor.\")\n",
    "            continue  # salta este tensor\n",
    "        B, A, H, W, C = p.shape  # C=5+num_classes\n",
    "        p = p.permute(0, 1, 4, 2, 3).contiguous()  # [B, A, 5+C, H, W]\n",
    "        p = p.view(B, -1, H, W)  # [B, A*(5+C), H, W]\n",
    "        adapted.append(p)\n",
    "    return adapted\n",
    "\n",
    "print(\"Función adapt_predictions_for_loss definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a774cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors: [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
      "Anchors personalizados definidos correctamente.\n",
      "\n",
      "Comprobando dataset y dataloaders en C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset...\n",
      "               filename cell_type  xmin  xmax  ymin  ymax\n",
      "0  BloodImage_00000.jpg       WBC   260   491   177   376\n",
      "1  BloodImage_00000.jpg       RBC    78   184   336   435\n",
      "2  BloodImage_00000.jpg       RBC    63   169   237   336\n",
      "3  BloodImage_00000.jpg       RBC   214   320   362   461\n",
      "4  BloodImage_00000.jpg       RBC   414   506   352   445\n",
      "Total de imágenes no diferentes de entrenamiento: 3435\n",
      "                 filename cell_type  xmin  xmax  ymin  ymax\n",
      "163  BloodImage_00009.jpg       WBC    23   255   137   423\n",
      "164  BloodImage_00009.jpg       RBC   441   575   324   423\n",
      "165  BloodImage_00009.jpg       RBC   372   468   192   279\n",
      "166  BloodImage_00009.jpg       RBC   305   401   213   300\n",
      "167  BloodImage_00009.jpg       RBC   226   324   288   439\n",
      "Total de imágenes no diferentes de validación: 705\n",
      "                 filename cell_type  xmin  xmax  ymin  ymax\n",
      "125  BloodImage_00007.jpg       WBC   193   387    92   285\n",
      "126  BloodImage_00007.jpg       RBC    17   134   298   402\n",
      "127  BloodImage_00007.jpg       RBC    64   175   372   479\n",
      "128  BloodImage_00007.jpg       RBC   119   230   330   437\n",
      "129  BloodImage_00007.jpg       RBC   169   259   265   374\n",
      "Total de imágenes no diferentes  de prueba: 748\n",
      "\n",
      "Función de pérdida y optimizador definidos correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo YOLO Version 3\n",
    "\n",
    "# PASO 1: Anchors personalizados\n",
    "print(f\"Anchors: {ANCHORS}\")\n",
    "print(\"Anchors personalizados definidos correctamente.\")\n",
    "print()\n",
    "\n",
    "# PASO2: Paths y parámetros\n",
    "DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv')\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # Ajusta según tu máquina\n",
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = YOLO_INPUT_SIZE\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PASO 3: Comprobar dataset y dataloaders\n",
    "print(f\"Comprobando dataset y dataloaders en {DATA_ROOT}...\")\n",
    "\n",
    "print(train_df.head())\n",
    "print(f\"Total de imágenes no diferentes de entrenamiento: {len(train_df)}\")\n",
    "\n",
    "print(val_df.head())\n",
    "print(f\"Total de imágenes no diferentes de validación: {len(val_df)}\")\n",
    "print(test_df.head())\n",
    "print(f\"Total de imágenes no diferentes  de prueba: {len(test_df)}\")\n",
    "\n",
    "# PASO 4: Pérdida y optimizador\n",
    "\n",
    "loss_fn = YOLOv3Loss(\n",
    "    anchors=ANCHORS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_size=IMG_SIZE,\n",
    "    lambda_coord=1.0,\n",
    "    lambda_noobj=1.0,\n",
    "    lambda_obj=1.0,\n",
    "    lambda_class=1.0,\n",
    "    ignore_iou_threshold=0.5,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Solo parámetros entrenables (solo los heads)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "print()\n",
    "print(\"Función de pérdida y optimizador definidos correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93944206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de entrenamiento y validación definidas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo YOLO Version 3\n",
    "\n",
    "# PASO 5: Funciones del Training Loop\n",
    "def train_one_epoch(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Entrenando\"):\n",
    "        if images.shape[0] == 0:\n",
    "            print(\"Batch vacío saltado (entrenamiento)\")\n",
    "            continue  # Salta este batch vacío\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        if all_targets.numel() == 0:\n",
    "            print(\"Batch de imágenes sin ningún target: saltado.\")\n",
    "            continue\n",
    "        predictions = model(images)\n",
    "        #print([p.shape for p in predictions])\n",
    "        predictions = adapt_predictions_for_loss(predictions, num_classes=NUM_CLASSES)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #running_loss += loss.item()\n",
    "        #box_loss += components['box_loss'].item()\n",
    "        #obj_loss += components['obj_loss'].item()\n",
    "        #noobj_loss += components['noobj_loss'].item()\n",
    "        #class_loss += components['class_loss'].item()\n",
    "        running_loss += float(loss)\n",
    "        box_loss += float(components['box_loss'])\n",
    "        obj_loss += float(components['obj_loss'])\n",
    "        noobj_loss += float(components['noobj_loss'])\n",
    "        class_loss += float(components['class_loss'])\n",
    "    n = len(loader)\n",
    "    if n == 0:\n",
    "        return 0, 0, 0, 0, 0\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Validando\"):\n",
    "        if images.shape[0] == 0:\n",
    "            print(\"Batch vacío saltado (validacion)\")\n",
    "            continue  # Salta este batch vacío\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        if all_targets.numel() == 0:\n",
    "            print(\"Batch de imágenes sin ningún target: saltado.\")\n",
    "            continue\n",
    "        predictions = model(images)\n",
    "        #print([p.shape for p in predictions])\n",
    "        predictions = adapt_predictions_for_loss(predictions, num_classes=NUM_CLASSES)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        #running_loss += loss.item()\n",
    "        #box_loss += components['box_loss'].item()\n",
    "        #obj_loss += components['obj_loss'].item()\n",
    "        #noobj_loss += components['noobj_loss'].item()\n",
    "        #class_loss += components['class_loss'].item()\n",
    "        running_loss += float(loss)\n",
    "        box_loss += float(components['box_loss'])\n",
    "        obj_loss += float(components['obj_loss'])\n",
    "        noobj_loss += float(components['noobj_loss'])\n",
    "        class_loss += float(components['class_loss'])\n",
    "    n = len(loader)\n",
    "    if n == 0:\n",
    "        return 0, 0, 0, 0, 0\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "print(\"Funciones de entrenamiento y validación definidas correctamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883a9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Época 1/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:58<00:00,  1.84s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11.2074 | Val Loss: 9.4275\n",
      "  [Train] Box: 5.9325 Obj: 1.7231 NoObj: 1.6802 Class: 1.8716\n",
      "  [Val]   Box: 5.3670 Obj: 1.2944 NoObj: 1.2440 Class: 1.5221\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 2/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:30<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.5181 | Val Loss: 7.7988\n",
      "  [Train] Box: 5.1447 Obj: 1.0600 NoObj: 0.9872 Class: 1.3262\n",
      "  [Val]   Box: 5.0131 Obj: 0.8427 NoObj: 0.7805 Class: 1.1625\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 3/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.3319 | Val Loss: 6.9893\n",
      "  [Train] Box: 4.8970 Obj: 0.7142 NoObj: 0.6597 Class: 1.0609\n",
      "  [Val]   Box: 4.8630 Obj: 0.5963 NoObj: 0.5563 Class: 0.9737\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 4/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6695 | Val Loss: 6.4792\n",
      "  [Train] Box: 4.7639 Obj: 0.5208 NoObj: 0.4783 Class: 0.9065\n",
      "  [Val]   Box: 4.7613 Obj: 0.4492 NoObj: 0.4097 Class: 0.8590\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 5/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:36<00:00,  1.14s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:11<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2588 | Val Loss: 6.1192\n",
      "  [Train] Box: 4.6756 Obj: 0.4031 NoObj: 0.3671 Class: 0.8130\n",
      "  [Val]   Box: 4.6725 Obj: 0.3489 NoObj: 0.3201 Class: 0.7777\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 6/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:57<00:00,  1.80s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:05<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.9539 | Val Loss: 5.8715\n",
      "  [Train] Box: 4.6063 Obj: 0.3162 NoObj: 0.2894 Class: 0.7419\n",
      "  [Val]   Box: 4.6108 Obj: 0.2814 NoObj: 0.2569 Class: 0.7224\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 7/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:30<00:00,  1.04it/s]\n",
      "Validando: 100%|██████████| 7/7 [00:05<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7315 | Val Loss: 5.6799\n",
      "  [Train] Box: 4.5451 Obj: 0.2615 NoObj: 0.2372 Class: 0.6876\n",
      "  [Val]   Box: 4.5486 Obj: 0.2371 NoObj: 0.2159 Class: 0.6783\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 8/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.5482 | Val Loss: 5.5186\n",
      "  [Train] Box: 4.4824 Obj: 0.2181 NoObj: 0.1975 Class: 0.6503\n",
      "  [Val]   Box: 4.5019 Obj: 0.1964 NoObj: 0.1793 Class: 0.6410\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 9/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4099 | Val Loss: 5.3904\n",
      "  [Train] Box: 4.4406 Obj: 0.1854 NoObj: 0.1674 Class: 0.6165\n",
      "  [Val]   Box: 4.4615 Obj: 0.1666 NoObj: 0.1525 Class: 0.6098\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 10/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3001 | Val Loss: 5.3036\n",
      "  [Train] Box: 4.4077 Obj: 0.1601 NoObj: 0.1456 Class: 0.5866\n",
      "  [Val]   Box: 4.4362 Obj: 0.1464 NoObj: 0.1331 Class: 0.5880\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 11/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.2061 | Val Loss: 5.2163\n",
      "  [Train] Box: 4.3723 Obj: 0.1410 NoObj: 0.1270 Class: 0.5657\n",
      "  [Val]   Box: 4.4023 Obj: 0.1289 NoObj: 0.1176 Class: 0.5675\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 12/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.1405 | Val Loss: 5.1465\n",
      "  [Train] Box: 4.3547 Obj: 0.1235 NoObj: 0.1126 Class: 0.5496\n",
      "  [Val]   Box: 4.3823 Obj: 0.1137 NoObj: 0.1027 Class: 0.5478\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 13/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.0642 | Val Loss: 5.0918\n",
      "  [Train] Box: 4.3217 Obj: 0.1096 NoObj: 0.0992 Class: 0.5336\n",
      "  [Val]   Box: 4.3666 Obj: 0.1010 NoObj: 0.0914 Class: 0.5328\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 14/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.9969 | Val Loss: 5.0221\n",
      "  [Train] Box: 4.2991 Obj: 0.0974 NoObj: 0.0877 Class: 0.5127\n",
      "  [Val]   Box: 4.3377 Obj: 0.0883 NoObj: 0.0805 Class: 0.5156\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 15/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.9484 | Val Loss: 4.9849\n",
      "  [Train] Box: 4.2790 Obj: 0.0891 NoObj: 0.0812 Class: 0.4990\n",
      "  [Val]   Box: 4.3197 Obj: 0.0837 NoObj: 0.0764 Class: 0.5051\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 16/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.9009 | Val Loss: 4.9464\n",
      "  [Train] Box: 4.2605 Obj: 0.0809 NoObj: 0.0737 Class: 0.4858\n",
      "  [Val]   Box: 4.3090 Obj: 0.0759 NoObj: 0.0696 Class: 0.4918\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 17/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8641 | Val Loss: 4.9093\n",
      "  [Train] Box: 4.2463 Obj: 0.0736 NoObj: 0.0677 Class: 0.4765\n",
      "  [Val]   Box: 4.2960 Obj: 0.0683 NoObj: 0.0621 Class: 0.4828\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 18/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8302 | Val Loss: 4.8758\n",
      "  [Train] Box: 4.2294 Obj: 0.0676 NoObj: 0.0613 Class: 0.4719\n",
      "  [Val]   Box: 4.2810 Obj: 0.0636 NoObj: 0.0582 Class: 0.4730\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 19/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7904 | Val Loss: 4.8461\n",
      "  [Train] Box: 4.2160 Obj: 0.0621 NoObj: 0.0568 Class: 0.4555\n",
      "  [Val]   Box: 4.2705 Obj: 0.0583 NoObj: 0.0535 Class: 0.4638\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 20/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7717 | Val Loss: 4.8166\n",
      "  [Train] Box: 4.2082 Obj: 0.0574 NoObj: 0.0526 Class: 0.4536\n",
      "  [Val]   Box: 4.2581 Obj: 0.0533 NoObj: 0.0485 Class: 0.4567\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 21/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7443 | Val Loss: 4.7904\n",
      "  [Train] Box: 4.1974 Obj: 0.0541 NoObj: 0.0491 Class: 0.4437\n",
      "  [Val]   Box: 4.2475 Obj: 0.0494 NoObj: 0.0458 Class: 0.4477\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 22/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7105 | Val Loss: 4.7715\n",
      "  [Train] Box: 4.1858 Obj: 0.0487 NoObj: 0.0451 Class: 0.4309\n",
      "  [Val]   Box: 4.2421 Obj: 0.0462 NoObj: 0.0429 Class: 0.4404\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 23/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6870 | Val Loss: 4.7537\n",
      "  [Train] Box: 4.1726 Obj: 0.0465 NoObj: 0.0425 Class: 0.4254\n",
      "  [Val]   Box: 4.2344 Obj: 0.0444 NoObj: 0.0411 Class: 0.4338\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 24/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6614 | Val Loss: 4.7372\n",
      "  [Train] Box: 4.1629 Obj: 0.0425 NoObj: 0.0393 Class: 0.4167\n",
      "  [Val]   Box: 4.2321 Obj: 0.0404 NoObj: 0.0370 Class: 0.4277\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 25/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6486 | Val Loss: 4.7126\n",
      "  [Train] Box: 4.1564 Obj: 0.0399 NoObj: 0.0372 Class: 0.4150\n",
      "  [Val]   Box: 4.2169 Obj: 0.0382 NoObj: 0.0355 Class: 0.4221\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 26/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6185 | Val Loss: 4.7009\n",
      "  [Train] Box: 4.1404 Obj: 0.0379 NoObj: 0.0351 Class: 0.4050\n",
      "  [Val]   Box: 4.2149 Obj: 0.0357 NoObj: 0.0330 Class: 0.4173\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 27/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6130 | Val Loss: 4.6817\n",
      "  [Train] Box: 4.1416 Obj: 0.0358 NoObj: 0.0332 Class: 0.4025\n",
      "  [Val]   Box: 4.2038 Obj: 0.0338 NoObj: 0.0315 Class: 0.4126\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 28/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5925 | Val Loss: 4.6679\n",
      "  [Train] Box: 4.1325 Obj: 0.0335 NoObj: 0.0310 Class: 0.3955\n",
      "  [Val]   Box: 4.2005 Obj: 0.0314 NoObj: 0.0290 Class: 0.4070\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 29/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5803 | Val Loss: 4.6520\n",
      "  [Train] Box: 4.1242 Obj: 0.0318 NoObj: 0.0295 Class: 0.3949\n",
      "  [Val]   Box: 4.1921 Obj: 0.0298 NoObj: 0.0279 Class: 0.4023\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 30/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5612 | Val Loss: 4.6399\n",
      "  [Train] Box: 4.1147 Obj: 0.0302 NoObj: 0.0281 Class: 0.3882\n",
      "  [Val]   Box: 4.1871 Obj: 0.0282 NoObj: 0.0265 Class: 0.3981\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 31/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5475 | Val Loss: 4.6296\n",
      "  [Train] Box: 4.1117 Obj: 0.0286 NoObj: 0.0268 Class: 0.3805\n",
      "  [Val]   Box: 4.1832 Obj: 0.0270 NoObj: 0.0254 Class: 0.3941\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 32/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5455 | Val Loss: 4.6133\n",
      "  [Train] Box: 4.1108 Obj: 0.0275 NoObj: 0.0255 Class: 0.3817\n",
      "  [Val]   Box: 4.1750 Obj: 0.0257 NoObj: 0.0239 Class: 0.3887\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 33/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5324 | Val Loss: 4.6081\n",
      "  [Train] Box: 4.0997 Obj: 0.0259 NoObj: 0.0241 Class: 0.3826\n",
      "  [Val]   Box: 4.1733 Obj: 0.0246 NoObj: 0.0230 Class: 0.3873\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 34/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5212 | Val Loss: 4.5988\n",
      "  [Train] Box: 4.0983 Obj: 0.0242 NoObj: 0.0227 Class: 0.3760\n",
      "  [Val]   Box: 4.1710 Obj: 0.0232 NoObj: 0.0217 Class: 0.3829\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 35/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5091 | Val Loss: 4.5926\n",
      "  [Train] Box: 4.0935 Obj: 0.0233 NoObj: 0.0217 Class: 0.3706\n",
      "  [Val]   Box: 4.1693 Obj: 0.0217 NoObj: 0.0202 Class: 0.3814\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 36/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5001 | Val Loss: 4.5785\n",
      "  [Train] Box: 4.0884 Obj: 0.0228 NoObj: 0.0211 Class: 0.3677\n",
      "  [Val]   Box: 4.1602 Obj: 0.0214 NoObj: 0.0200 Class: 0.3769\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 37/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4968 | Val Loss: 4.5710\n",
      "  [Train] Box: 4.0870 Obj: 0.0214 NoObj: 0.0198 Class: 0.3687\n",
      "  [Val]   Box: 4.1590 Obj: 0.0200 NoObj: 0.0187 Class: 0.3733\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 38/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4781 | Val Loss: 4.5694\n",
      "  [Train] Box: 4.0820 Obj: 0.0202 NoObj: 0.0189 Class: 0.3570\n",
      "  [Val]   Box: 4.1597 Obj: 0.0194 NoObj: 0.0181 Class: 0.3722\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 39/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4731 | Val Loss: 4.5486\n",
      "  [Train] Box: 4.0790 Obj: 0.0195 NoObj: 0.0183 Class: 0.3563\n",
      "  [Val]   Box: 4.1471 Obj: 0.0188 NoObj: 0.0178 Class: 0.3650\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 40/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4679 | Val Loss: 4.5460\n",
      "  [Train] Box: 4.0725 Obj: 0.0189 NoObj: 0.0177 Class: 0.3588\n",
      "  [Val]   Box: 4.1477 Obj: 0.0178 NoObj: 0.0170 Class: 0.3635\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 41/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4562 | Val Loss: 4.5421\n",
      "  [Train] Box: 4.0686 Obj: 0.0179 NoObj: 0.0166 Class: 0.3531\n",
      "  [Val]   Box: 4.1477 Obj: 0.0164 NoObj: 0.0155 Class: 0.3625\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 42/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4374 | Val Loss: 4.5352\n",
      "  [Train] Box: 4.0585 Obj: 0.0171 NoObj: 0.0162 Class: 0.3456\n",
      "  [Val]   Box: 4.1441 Obj: 0.0164 NoObj: 0.0156 Class: 0.3591\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 43/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4375 | Val Loss: 4.5222\n",
      "  [Train] Box: 4.0624 Obj: 0.0165 NoObj: 0.0156 Class: 0.3431\n",
      "  [Val]   Box: 4.1354 Obj: 0.0154 NoObj: 0.0147 Class: 0.3566\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 44/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4309 | Val Loss: 4.5158\n",
      "  [Train] Box: 4.0569 Obj: 0.0158 NoObj: 0.0149 Class: 0.3433\n",
      "  [Val]   Box: 4.1344 Obj: 0.0146 NoObj: 0.0139 Class: 0.3530\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 45/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.10s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4295 | Val Loss: 4.5107\n",
      "  [Train] Box: 4.0592 Obj: 0.0154 NoObj: 0.0145 Class: 0.3405\n",
      "  [Val]   Box: 4.1318 Obj: 0.0145 NoObj: 0.0137 Class: 0.3507\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 46/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4194 | Val Loss: 4.5118\n",
      "  [Train] Box: 4.0507 Obj: 0.0149 NoObj: 0.0140 Class: 0.3398\n",
      "  [Val]   Box: 4.1345 Obj: 0.0139 NoObj: 0.0132 Class: 0.3503\n",
      "No mejora en val_loss. Paciencia: 1/10\n",
      "\n",
      "=== Época 47/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4177 | Val Loss: 4.5021\n",
      "  [Train] Box: 4.0523 Obj: 0.0142 NoObj: 0.0133 Class: 0.3380\n",
      "  [Val]   Box: 4.1283 Obj: 0.0132 NoObj: 0.0126 Class: 0.3480\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 48/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.11s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3976 | Val Loss: 4.4953\n",
      "  [Train] Box: 4.0422 Obj: 0.0135 NoObj: 0.0129 Class: 0.3290\n",
      "  [Val]   Box: 4.1251 Obj: 0.0132 NoObj: 0.0125 Class: 0.3445\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 49/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:36<00:00,  1.13s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3984 | Val Loss: 4.4909\n",
      "  [Train] Box: 4.0420 Obj: 0.0132 NoObj: 0.0125 Class: 0.3307\n",
      "  [Val]   Box: 4.1238 Obj: 0.0125 NoObj: 0.0119 Class: 0.3427\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "\n",
      "=== Época 50/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n",
      "Validando: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3914 | Val Loss: 4.4849\n",
      "  [Train] Box: 4.0383 Obj: 0.0128 NoObj: 0.0121 Class: 0.3281\n",
      "  [Val]   Box: 4.1208 Obj: 0.0119 NoObj: 0.0112 Class: 0.3410\n",
      ">> Modelo guardado (mejor Val Loss)\n",
      "Entrenamiento finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo YOLO Version 3\n",
    "\n",
    "# PASO 6: Loop principal de entrenamiento\n",
    "patience = 10  # Epochs sin mejora antes de parar\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "writer = SummaryWriter(log_dir='runs/yolov3_bloodcell')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\n=== Época {epoch}/{EPOCHS} ===\")\n",
    "    t_loss, t_box, t_obj, t_noobj, t_class = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    v_loss, v_box, v_obj, v_noobj, v_class = validate_one_epoch(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    print(f\"Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f}\")\n",
    "    print(f\"  [Train] Box: {t_box:.4f} Obj: {t_obj:.4f} NoObj: {t_noobj:.4f} Class: {t_class:.4f}\")\n",
    "    print(f\"  [Val]   Box: {v_box:.4f} Obj: {v_obj:.4f} NoObj: {v_noobj:.4f} Class: {v_class:.4f}\")\n",
    "\n",
    "    # Early stopping y guardado del mejor modelo\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), \"best_yolov3_bloodcell.pth\")\n",
    "        print(\">> Modelo guardado (mejor Val Loss)\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No mejora en val_loss. Paciencia: {epochs_no_improve}/{patience}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping activado: no hay mejora en la validación.\")\n",
    "            break\n",
    "\n",
    "    # TensorBoard\n",
    "    writer.add_scalar(\"Loss/Train\", t_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Val\", v_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Train\", t_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Val\", v_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Train\", t_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Val\", v_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Train\", t_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Val\", v_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Train\", t_class, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Val\", v_class, epoch)\n",
    "\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
