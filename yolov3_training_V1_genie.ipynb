{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Reproducibilidad total\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(f\"Semilla global fijada en {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5330d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos (celdas de sangre)\n",
    "# Basado en el repositorio de Manuel Garcia UEM Junio 2025\n",
    "\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "\n",
    "# Ruta a los pesos preentrenados yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "print(\"Liberias importadas correctamente\")\n",
    "\n",
    "#from yolov3_model_setup_V2_genie_OK import model, device  # Debe devolver el modelo ya adaptado y en el device\n",
    "#from yolov3_dataset_setup_V4_genie_OK import BloodCellDataset, train_transforms, val_test_transforms, YOLO_INPUT_SIZE\n",
    "#from yolov3_IOU_loss_function_V2_genie_OK import YOLOv3Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecba2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que PyTorch está instalado y la versión\n",
    "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Verificamos que NumPy está instalado y la versión\n",
    "print(f\"Versión de NumPy: {np.__version__}\")\n",
    "\n",
    "# Verificamos que sys está instalado y la versión\n",
    "print(f\"Versión de sys: {sys.version}\")\n",
    "# Verificamos que os está instalado y la versión\n",
    "print(f\"Versión de os: {os.name}\")\n",
    "# Verificamos que la GPU está disponible (si es que se va a usar)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU disponible para PyTorch.\")\n",
    "    print(f\"Dispositivo actual: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No se detecta GPU, se usará la CPU para el entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de configuración YOLOv3: {CONFIG_PATH}\")\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de pesos YOLOv3: {WEIGHTS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer \n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82919b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del modelo\n",
    "    \n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES_YOUR_DATASET = 3\n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = 416 \n",
    "\n",
    "# Definimos los anchor masks para tus 3 clases (placeholder hasta definir los adecuados con KMeans)\n",
    "# YOLOv3 usa 9 anchor boxes en total, divididos en 3 grupos de 3 para cada escala.\n",
    "# Estos son los INDICES de los anchors. Los valores reales los calculaamos con K-Means.\n",
    "# Ejemplo: si los 9 anchors se ordenan de menor a mayor área, los grandes (indices 6,7,8) van a la escala 13x13.\n",
    "#Anchor Boxes Calculadas (Formato para YOLOv3Loss): [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
    "\n",
    "DUMMY_ANCHORS_MASKS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# ... (all your setup code above remains unchanged)\n",
    "\n",
    "# PASO 3: Adapatacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0]\n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels,\n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True\n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer)\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        yolo_layer_index_in_model_yolo_layers += 1\n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# --- NUEVA SECCIÓN: CONGELAR TODAS LAS CAPAS, SOLO DESCONGELAR LAS ULTIMAS Conv2d DE PREDICCIÓN ---\n",
    "print(\"\\nConfigurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\")\n",
    "\n",
    "# Congelar todos los parámetros por defecto\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Solo las Conv2d antes de YOLOLayer quedan como entrenables\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        conv_seq = model.module_list[pred_conv_sequential_idx]\n",
    "        # Buscar la capa Conv2d dentro del Sequential\n",
    "        for layer in conv_seq:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"  Descongelada capa Conv2d antes del YOLOLayer en module_list[{i}]\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\")\n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\\n\")\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta version es la que funciona mejor ya que ademas de preparar el dataset visualiza el resultado OJO OJO OJO\n",
    "# Incluye la version definitiva incluyendo la funcion de visualizacion\n",
    "# Ademas aplica un nivel razonable y seguro de DA ... hemos excluido las transformaciones Geométricas por considerar\n",
    "# que pueden tener efecto distorsionante sobre el dataset (rotaciones, escalados, etc.)\n",
    "# Se ha añadido la posibilidad de visualizar el dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60815898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "        # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "        bboxes_normalized_initial = []\n",
    "        class_labels = [] # class_labels se mantiene\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "        if bboxes_normalized_initial:\n",
    "            print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "            # Se espera que devuelva coordenadas normalizadas también.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "        # Si Albumentations funciona como se espera con format='albumentations',\n",
    "        # este paso ya no es necesario.\n",
    "        bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "        # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO CON AUMENTACIÓN DE COLOR/APARIENCIA ---\n",
    "train_transforms = A.Compose([\n",
    "    # Redimensionamiento y Relleno\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    \n",
    "    # Transformaciones de Color y Apariencia\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1), # Asegúrate de que blur_limit es impar y no demasiado grande\n",
    "    \n",
    "    # Normalización y Conversión a Tensor\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # El formato 'albumentations' espera y devuelve normalizado [0,1]\n",
    "\n",
    "# Las transformaciones de validación/prueba se mantienen minimalistas\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) \n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    #train_dataloader = DataLoader(\n",
    "    #    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "    #val_dataloader = DataLoader(\n",
    "    #    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "    #test_dataloader = DataLoader(\n",
    "    #    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "\n",
    "    PIN_MEMORY = torch.cuda.is_available()  # Solo True si hay GPU\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOU y Funcion de Perdida de YOLOV3\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    else:\n",
    "        raise ValueError(\"box_format debe ser 'midpoint' o 'corners'\")\n",
    "\n",
    "    x1_inter = torch.max(box1_x1, box2_x1)\n",
    "    y1_inter = torch.max(box1_y1, box2_y1)\n",
    "    x2_inter = torch.min(box1_x2, box2_x2)\n",
    "    y2_inter = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2_inter - x1_inter).clamp(0) * (y2_inter - y1_inter).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union = box1_area + box2_area - intersection + 1e-6\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "class YOLOv3Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size=(416, 416),\n",
    "                lambda_coord=1.0, lambda_noobj=1.0, lambda_obj=1.0, lambda_class=1.0,\n",
    "                ignore_iou_threshold=0.5, device=None):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.lambda_class = lambda_class\n",
    "        self.ignore_iou_threshold = ignore_iou_threshold\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]))\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        obj_loss = 0\n",
    "        noobj_loss = 0\n",
    "        box_loss = 0\n",
    "        class_loss = 0\n",
    "\n",
    "        # Convert anchors to tensor only once, on correct device\n",
    "        anchors_tensor = [torch.tensor(a, dtype=torch.float32, device=self.device) for a in self.anchors]\n",
    "\n",
    "        for scale_idx, prediction in enumerate(predictions):\n",
    "            # prediction: (N, 3*(5+C), H, W)\n",
    "            prediction = prediction.permute(0, 2, 3, 1).reshape(\n",
    "                prediction.shape[0], prediction.shape[2], prediction.shape[3], 3, self.num_classes + 5\n",
    "            )\n",
    "\n",
    "            pred_x_y = prediction[..., 0:2]\n",
    "            pred_w_h = prediction[..., 2:4]\n",
    "            pred_obj = prediction[..., 4:5]\n",
    "            pred_class = prediction[..., 5:]\n",
    "\n",
    "            N, grid_h, grid_w, num_anchors, _ = prediction.shape\n",
    "\n",
    "            anchors_current_scale = anchors_tensor[scale_idx].reshape(1, 1, 1, num_anchors, 2)\n",
    "\n",
    "            # Inicializar todo en el device correcto\n",
    "            target_obj_mask = torch.zeros((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            target_noobj_mask = torch.ones((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            tx = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            ty = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            tw = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            th = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            target_class_one_hot = torch.zeros((N, grid_h, grid_w, num_anchors, self.num_classes), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Vectorizar asignación de anchors\n",
    "            if targets.numel() > 0:\n",
    "                # targets: (num_true_boxes_in_batch, 6)\n",
    "                img_ids = targets[:, 0].long()\n",
    "                class_ids = targets[:, 1].long()\n",
    "                x_gt_norm = targets[:, 2]\n",
    "                y_gt_norm = targets[:, 3]\n",
    "                w_gt_norm = targets[:, 4]\n",
    "                h_gt_norm = targets[:, 5]\n",
    "\n",
    "                x_center_grid = x_gt_norm * grid_w\n",
    "                y_center_grid = y_gt_norm * grid_h\n",
    "                cell_x = x_center_grid.long()\n",
    "                cell_y = y_center_grid.long()\n",
    "\n",
    "                # Filtrar targets fuera de grid\n",
    "                grid_mask = (cell_x >= 0) & (cell_x < grid_w) & (cell_y >= 0) & (cell_y < grid_h) & (img_ids >= 0) & (img_ids < N)\n",
    "                img_ids = img_ids[grid_mask]\n",
    "                class_ids = class_ids[grid_mask]\n",
    "                cell_x = cell_x[grid_mask]\n",
    "                cell_y = cell_y[grid_mask]\n",
    "                x_center_grid = x_center_grid[grid_mask]\n",
    "                y_center_grid = y_center_grid[grid_mask]\n",
    "                w_gt_norm = w_gt_norm[grid_mask]\n",
    "                h_gt_norm = h_gt_norm[grid_mask]\n",
    "\n",
    "                # Anchors assignment (vectorized)\n",
    "                w_gt_pix = w_gt_norm * self.img_size[0]\n",
    "                h_gt_pix = h_gt_norm * self.img_size[1]\n",
    "                gt_box_dims = torch.stack([\n",
    "                    torch.zeros_like(w_gt_pix), torch.zeros_like(h_gt_pix), w_gt_pix, h_gt_pix\n",
    "                ], dim=1)  # (num_boxes, 4)\n",
    "\n",
    "                anchor_boxes_for_iou = torch.zeros((num_anchors, 4), device=self.device)\n",
    "                anchor_boxes_for_iou[:, 2] = anchors_current_scale[0,0,0,:,0]\n",
    "                anchor_boxes_for_iou[:, 3] = anchors_current_scale[0,0,0,:,1]\n",
    "\n",
    "                # Expand gt_box_dims for broadcasting\n",
    "                gt_box_dims_exp = gt_box_dims.unsqueeze(1).expand(-1, num_anchors, 4)  # (num_boxes, num_anchors, 4)\n",
    "                anchors_exp = anchor_boxes_for_iou.unsqueeze(0).expand(gt_box_dims.size(0), -1, 4)  # (num_boxes, num_anchors, 4)\n",
    "\n",
    "                ious = intersection_over_union(gt_box_dims_exp, anchors_exp, box_format=\"corners\").squeeze(-1)  # (num_boxes, num_anchors)\n",
    "                best_iou_anchor_idx = torch.argmax(ious, dim=1)  # (num_boxes,)\n",
    "\n",
    "                for idx in range(img_ids.shape[0]):\n",
    "                    i = img_ids[idx]\n",
    "                    c = class_ids[idx]\n",
    "                    cx = cell_x[idx]\n",
    "                    cy = cell_y[idx]\n",
    "                    best_anchor = best_iou_anchor_idx[idx]\n",
    "                    # Masks\n",
    "                    target_obj_mask[i, cy, cx, best_anchor] = 1.0\n",
    "                    target_noobj_mask[i, cy, cx, best_anchor] = 0.0\n",
    "                    # Coordinates\n",
    "                    tx[i, cy, cx, best_anchor] = x_center_grid[idx] - cx\n",
    "                    ty[i, cy, cx, best_anchor] = y_center_grid[idx] - cy\n",
    "                    tw[i, cy, cx, best_anchor] = torch.log(w_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,0] + 1e-16)\n",
    "                    th[i, cy, cx, best_anchor] = torch.log(h_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,1] + 1e-16)\n",
    "                    # Class\n",
    "                    target_class_one_hot[i, cy, cx, best_anchor, c] = 1.0\n",
    "                    # Ignore anchors with high IoU\n",
    "                    for anchor_idx_other in range(num_anchors):\n",
    "                        if anchor_idx_other == best_anchor:\n",
    "                            continue\n",
    "                        if ious[idx, anchor_idx_other] > self.ignore_iou_threshold:\n",
    "                            target_noobj_mask[i, cy, cx, anchor_idx_other] = 0.0\n",
    "\n",
    "            loss_x = self.bce(pred_x_y[..., 0][target_obj_mask.bool()], tx[target_obj_mask.bool()])\n",
    "            loss_y = self.bce(pred_x_y[..., 1][target_obj_mask.bool()], ty[target_obj_mask.bool()])\n",
    "            loss_w = self.mse(pred_w_h[..., 0][target_obj_mask.bool()], tw[target_obj_mask.bool()])\n",
    "            loss_h = self.mse(pred_w_h[..., 1][target_obj_mask.bool()], th[target_obj_mask.bool()])\n",
    "            box_loss += (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "            loss_obj = self.bce(pred_obj[target_obj_mask.bool()], target_obj_mask[target_obj_mask.bool()].float().unsqueeze(-1))\n",
    "            loss_noobj = self.bce(pred_obj[target_noobj_mask.bool()], target_noobj_mask[target_noobj_mask.bool()].float().unsqueeze(-1))\n",
    "            obj_loss += loss_obj\n",
    "            noobj_loss += loss_noobj\n",
    "\n",
    "            loss_class = self.bce(pred_class[target_obj_mask.bool()], target_class_one_hot[target_obj_mask.bool()])\n",
    "            class_loss += loss_class\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + self.lambda_obj * obj_loss\n",
    "            + self.lambda_noobj * noobj_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "        return total_loss, {\"box_loss\": box_loss, \"obj_loss\": obj_loss, \"noobj_loss\": noobj_loss, \"class_loss\": class_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a774cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo YOLO Version 3\n",
    "\n",
    "# ========== 1. Anchors personalizados ==========\n",
    "ANCHORS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # 13x13\n",
    "    [(105, 113), (104, 96), (80, 109)],    # 26x26\n",
    "    [(112, 75), (87, 82), (39, 38)]        # 52x52\n",
    "]\n",
    "\n",
    "# ========== 2. Paths y parámetros ==========\n",
    "\n",
    "DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv')\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4  # Ajusta según tu máquina\n",
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = YOLO_INPUT_SIZE\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# ========== 3. Crear datasets y dataloaders ==========\n",
    "\n",
    "full_df = pd.read_csv(CSV_FILE)\n",
    "all_image_filenames = full_df['filename'].unique().tolist()\n",
    "train_val_filenames, test_filenames = train_test_split(\n",
    "    all_image_filenames, test_size=0.15, random_state=42)\n",
    "train_filenames, val_filenames = train_test_split(\n",
    "    train_val_filenames, test_size=0.15/(1-0.15), random_state=42)\n",
    "\n",
    "train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "train_dataset = BloodCellDataset(DATA_ROOT, train_df, image_size=IMG_SIZE, transform=train_transforms)\n",
    "val_dataset = BloodCellDataset(DATA_ROOT, val_df, image_size=IMG_SIZE, transform=val_test_transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, bboxes = [], []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target)\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()  # Solo True si hay GPU\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "# ========== 4. Pérdida y optimizador ==========\n",
    "loss_fn = YOLOv3Loss(\n",
    "    anchors=ANCHORS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_size=IMG_SIZE,\n",
    "    lambda_coord=1.0,\n",
    "    lambda_noobj=1.0,\n",
    "    lambda_obj=1.0,\n",
    "    lambda_class=1.0,\n",
    "    ignore_iou_threshold=0.5,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Solo parámetros entrenables (recuerda: solo los heads)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# ========== 5. Training Loop ==========\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Entrenando\"):\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        box_loss += components['box_loss'].item()\n",
    "        obj_loss += components['obj_loss'].item()\n",
    "        noobj_loss += components['noobj_loss'].item()\n",
    "        class_loss += components['class_loss'].item()\n",
    "    n = len(loader)\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Validando\"):\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        box_loss += components['box_loss'].item()\n",
    "        obj_loss += components['obj_loss'].item()\n",
    "        noobj_loss += components['noobj_loss'].item()\n",
    "        class_loss += components['class_loss'].item()\n",
    "    n = len(loader)\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "# ========== 6. Loop principal de entrenamiento ==========\n",
    "\n",
    "patience = 10  # Epochs sin mejora antes de parar\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "writer = SummaryWriter(log_dir='runs/yolov3_bloodcell')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # ... (entrenar y validar)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_yolov3_bloodcell.pth\")\n",
    "        print(\">> Modelo guardado (mejor Val Loss)\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No mejora en val_loss. Paciencia: {epochs_no_improve}/{patience}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping activado: no hay mejora en la validación.\")\n",
    "            break\n",
    "        \n",
    "    # Entrenar y validar\n",
    "    print(f\"\\n=== Época {epoch}/{EPOCHS} ===\")\n",
    "    t_loss, t_box, t_obj, t_noobj, t_class = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    v_loss, v_box, v_obj, v_noobj, v_class = validate_one_epoch(model, val_loader, loss_fn, device)\n",
    "    train_loss = (t_loss, t_box, t_obj, t_noobj, t_class)\n",
    "    val_loss = (v_loss, v_box, v_obj, v_noobj, v_class)\n",
    "    # Imprimir pérdidas\n",
    "    print(f\"Época {epoch}/{EPOCHS} - Pérdidas:\")\n",
    "    print(f\"Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f}\")\n",
    "    print(f\"  [Train] Box: {t_box:.4f} Obj: {t_obj:.4f} NoObj: {t_noobj:.4f} Class: {t_class:.4f}\")\n",
    "    print(f\"  [Val]   Box: {v_box:.4f} Obj: {v_obj:.4f} NoObj: {v_noobj:.4f} Class: {v_class:.4f}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_yolov3_bloodcell.pth\")\n",
    "        print(\">> Modelo guardado (mejor Val Loss)\")\n",
    "        \n",
    "    # Registrar las pérdidas en TensorBoard\n",
    "    \n",
    "    t_box, t_obj, t_noobj, t_class = train_loss[1], train_loss[2], train_loss[3], train_loss[4]\n",
    "    v_box, v_obj, v_noobj, v_class = val_loss[1], val_loss[2], val_loss[3], val_loss[4]\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Train\", t_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Val\", v_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Train\", t_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Val\", v_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Train\", t_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Val\", v_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Train\", t_class, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Val\", v_class, epoch)\n",
    "\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "writer.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
