{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos (celdas de sangre)\n",
    "# Basado en el repositorio de Manuel Garcia UEM Junio 2025\n",
    "\n",
    "#1. Reproducibilidad total\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(f\"Semilla global fijada en {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5330d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos (celdas de sangre)\n",
    "# Basado en el repositorio de Manuel Garcia UEM Junio 2025\n",
    "\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "\n",
    "# Ruta a los pesos preentrenados yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "print(\"Liberias importadas correctamente\")\n",
    "\n",
    "#from yolov3_model_setup_V2_genie_OK import model, device  # Debe devolver el modelo ya adaptado y en el device\n",
    "#from yolov3_dataset_setup_V4_genie_OK import BloodCellDataset, train_transforms, val_test_transforms, YOLO_INPUT_SIZE\n",
    "#from yolov3_IOU_loss_function_V2_genie_OK import YOLOv3Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecba2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que PyTorch está instalado y la versión\n",
    "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Verificamos que NumPy está instalado y la versión\n",
    "print(f\"Versión de NumPy: {np.__version__}\")\n",
    "\n",
    "# Verificamos que sys está instalado y la versión\n",
    "print(f\"Versión de sys: {sys.version}\")\n",
    "# Verificamos que os está instalado y la versión\n",
    "print(f\"Versión de os: {os.name}\")\n",
    "# Verificamos que la GPU está disponible (si es que se va a usar)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU disponible para PyTorch.\")\n",
    "    print(f\"Dispositivo actual: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No se detecta GPU, se usará la CPU para el entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de configuración YOLOv3: {CONFIG_PATH}\")\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de pesos YOLOv3: {WEIGHTS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer \n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82919b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del modelo\n",
    "    \n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES_YOUR_DATASET = 3\n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = 416 \n",
    "\n",
    "# Definimos los anchor masks para tus 3 clases (placeholder hasta definir los adecuados con KMeans)\n",
    "# YOLOv3 usa 9 anchor boxes en total, divididos en 3 grupos de 3 para cada escala.\n",
    "# Estos son los INDICES de los anchors. Los valores reales los calculaamos con K-Means.\n",
    "# Ejemplo: si los 9 anchors se ordenan de menor a mayor área, los grandes (indices 6,7,8) van a la escala 13x13.\n",
    "#Anchor Boxes Calculadas (Formato para YOLOv3Loss): [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
    "\n",
    "DUMMY_ANCHORS_MASKS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "ANCHORS = DUMMY_ANCHORS_MASKS\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# ... (all your setup code above remains unchanged)\n",
    "\n",
    "# PASO 3: Adaptacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0]\n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels,\n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True\n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer)\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        yolo_layer_index_in_model_yolo_layers += 1\n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# --- NUEVA SECCIÓN: CONGELAR TODAS LAS CAPAS, SOLO DESCONGELAR LAS ULTIMAS Conv2d DE PREDICCIÓN ---\n",
    "print(\"\\nConfigurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\")\n",
    "\n",
    "# Congelar todos los parámetros por defecto\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Solo las Conv2d antes de YOLOLayer quedan como entrenables\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        conv_seq = model.module_list[pred_conv_sequential_idx]\n",
    "        # Buscar la capa Conv2d dentro del Sequential\n",
    "        for layer in conv_seq:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"  Descongelada capa Conv2d antes del YOLOLayer en module_list[{i}]\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\")\n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\\n\")\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta version es la que funciona mejor ya que ademas de preparar el dataset visualiza el resultado OJO OJO OJO\n",
    "# Incluye la version definitiva incluyendo la funcion de visualizacion\n",
    "# Ademas aplica un nivel razonable y seguro de DA ... hemos excluido las transformaciones Geométricas por considerar\n",
    "# que pueden tener efecto distorsionante sobre el dataset (rotaciones, escalados, etc.)\n",
    "# Se ha añadido la posibilidad de visualizar el dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60815898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            original_h, original_w, _ = image.shape\n",
    "            #print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "            bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "            #print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "            #if bboxes_pixel:\n",
    "            #    print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "            # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "            # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "            bboxes_normalized_initial = []\n",
    "            class_labels = [] # class_labels se mantiene\n",
    "            for bbox_px in bboxes_pixel:\n",
    "                xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "                # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "                xmin_norm = xmin_px / original_w\n",
    "                ymin_norm = ymin_px / original_h\n",
    "                xmax_norm = xmax_px / original_w\n",
    "                ymax_norm = ymax_px / original_h\n",
    "            \n",
    "                bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "                class_labels.append(class_id)\n",
    "        \n",
    "            #print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "            #if bboxes_normalized_initial:\n",
    "            #    print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "            if self.transform:\n",
    "                # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "                # Se espera que devuelva coordenadas normalizadas también.\n",
    "                transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "                image = transformed['image']\n",
    "                bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "                class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "            #print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "            #if bboxes_transformed_raw:\n",
    "            #    print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "            # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "            # Si Albumentations funciona como se espera con format='albumentations',\n",
    "            # este paso ya no es necesario.\n",
    "            bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "            #print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "            #if bboxes:\n",
    "            #    print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "            # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "            # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "            yolo_bboxes = []\n",
    "            for i, bbox in enumerate(bboxes):\n",
    "                x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "                # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "                x_min = max(0.0, min(1.0, x_min))\n",
    "                y_min = max(0.0, min(1.0, y_min))\n",
    "                x_max = max(0.0, min(1.0, x_max))\n",
    "                y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "                center_x = (x_min + x_max) / 2\n",
    "                width = x_max - x_min\n",
    "                center_y = (y_min + y_max) / 2\n",
    "                height = y_max - y_min\n",
    "            \n",
    "                # Este filtrado ya estaba, pero el error ocurría antes\n",
    "                if width <= 0 or height <= 0:\n",
    "                    print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                    continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "                yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "            #print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "            #if yolo_bboxes:\n",
    "            #    print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "            if len(yolo_bboxes) == 0:\n",
    "                # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "                yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "            else:\n",
    "                yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "            return image, yolo_bboxes\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error en __getitem__ para idx {idx}: {e}\")\n",
    "            return torch.zeros((3, *self.image_size)), torch.zeros((0, 5))\n",
    "        \n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO CON AUMENTACIÓN DE COLOR/APARIENCIA ---\n",
    "train_transforms = A.Compose([\n",
    "    # Redimensionamiento y Relleno\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    \n",
    "    # Transformaciones de Color y Apariencia\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1), # Asegúrate de que blur_limit es impar y no demasiado grande\n",
    "    \n",
    "    # Normalización y Conversión a Tensor\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # El formato 'albumentations' espera y devuelve normalizado [0,1]\n",
    "\n",
    "# Las transformaciones de validación/prueba se mantienen minimalistas\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) \n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    #train_dataloader = DataLoader(\n",
    "    #    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "    #val_dataloader = DataLoader(\n",
    "    #    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "    #test_dataloader = DataLoader(\n",
    "    #    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    #    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    #)\n",
    "\n",
    "    PIN_MEMORY = torch.cuda.is_available()  # Solo True si hay GPU\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOU y Funcion de Perdida de YOLOV3\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    else:\n",
    "        raise ValueError(\"box_format debe ser 'midpoint' o 'corners'\")\n",
    "\n",
    "    x1_inter = torch.max(box1_x1, box2_x1)\n",
    "    y1_inter = torch.max(box1_y1, box2_y1)\n",
    "    x2_inter = torch.min(box1_x2, box2_x2)\n",
    "    y2_inter = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2_inter - x1_inter).clamp(0) * (y2_inter - y1_inter).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union = box1_area + box2_area - intersection + 1e-6\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "class YOLOv3Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size=(416, 416),\n",
    "                lambda_coord=1.0, lambda_noobj=1.0, lambda_obj=1.0, lambda_class=1.0,\n",
    "                ignore_iou_threshold=0.5, device=None):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.lambda_class = lambda_class\n",
    "        self.ignore_iou_threshold = ignore_iou_threshold\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]))\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        obj_loss = 0\n",
    "        noobj_loss = 0\n",
    "        box_loss = 0\n",
    "        class_loss = 0\n",
    "\n",
    "        # Convert anchors to tensor only once, on correct device\n",
    "        anchors_tensor = [torch.tensor(a, dtype=torch.float32, device=self.device) for a in self.anchors]\n",
    "\n",
    "        for scale_idx, prediction in enumerate(predictions):\n",
    "            #print(f\"DEBUG: Shape prediction[{scale_idx}]: {prediction.shape}\")\n",
    "            #if prediction.dim() != 4:\n",
    "            #    print(f\"WARNING: prediction[{scale_idx}] tiene {prediction.dim()} dimensiones: {prediction.shape}\")\n",
    "            #continue  # o lanza una excepción explicativa\n",
    "            # ... sigue con permute y reshape ...\n",
    "            # prediction: (N, 3*(5+C), H, W)\n",
    "            prediction = prediction.permute(0, 2, 3, 1).reshape(\n",
    "                prediction.shape[0], prediction.shape[2], prediction.shape[3], 3, self.num_classes + 5\n",
    "            )\n",
    "\n",
    "            pred_x_y = prediction[..., 0:2]\n",
    "            pred_w_h = prediction[..., 2:4]\n",
    "            pred_obj = prediction[..., 4:5]\n",
    "            pred_class = prediction[..., 5:]\n",
    "\n",
    "            N, grid_h, grid_w, num_anchors, _ = prediction.shape\n",
    "\n",
    "            anchors_current_scale = anchors_tensor[scale_idx].reshape(1, 1, 1, num_anchors, 2)\n",
    "\n",
    "            # Inicializar todo en el device correcto\n",
    "            target_obj_mask = torch.zeros((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            target_noobj_mask = torch.ones((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=self.device)\n",
    "            tx = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            ty = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            tw = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            th = torch.zeros((N, grid_h, grid_w, num_anchors), device=self.device)\n",
    "            target_class_one_hot = torch.zeros((N, grid_h, grid_w, num_anchors, self.num_classes), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Vectorizar asignación de anchors\n",
    "            if targets.numel() > 0:\n",
    "                # targets: (num_true_boxes_in_batch, 6)\n",
    "                img_ids = targets[:, 0].long()\n",
    "                class_ids = targets[:, 1].long()\n",
    "                x_gt_norm = targets[:, 2]\n",
    "                y_gt_norm = targets[:, 3]\n",
    "                w_gt_norm = targets[:, 4]\n",
    "                h_gt_norm = targets[:, 5]\n",
    "\n",
    "                x_center_grid = x_gt_norm * grid_w\n",
    "                y_center_grid = y_gt_norm * grid_h\n",
    "                cell_x = x_center_grid.long()\n",
    "                cell_y = y_center_grid.long()\n",
    "\n",
    "                # Filtrar targets fuera de grid\n",
    "                grid_mask = (cell_x >= 0) & (cell_x < grid_w) & (cell_y >= 0) & (cell_y < grid_h) & (img_ids >= 0) & (img_ids < N)\n",
    "                img_ids = img_ids[grid_mask]\n",
    "                class_ids = class_ids[grid_mask]\n",
    "                cell_x = cell_x[grid_mask]\n",
    "                cell_y = cell_y[grid_mask]\n",
    "                x_center_grid = x_center_grid[grid_mask]\n",
    "                y_center_grid = y_center_grid[grid_mask]\n",
    "                w_gt_norm = w_gt_norm[grid_mask]\n",
    "                h_gt_norm = h_gt_norm[grid_mask]\n",
    "\n",
    "                # Anchors assignment (vectorized)\n",
    "                w_gt_pix = w_gt_norm * self.img_size[0]\n",
    "                h_gt_pix = h_gt_norm * self.img_size[1]\n",
    "                gt_box_dims = torch.stack([\n",
    "                    torch.zeros_like(w_gt_pix), torch.zeros_like(h_gt_pix), w_gt_pix, h_gt_pix\n",
    "                ], dim=1)  # (num_boxes, 4)\n",
    "\n",
    "                anchor_boxes_for_iou = torch.zeros((num_anchors, 4), device=self.device)\n",
    "                anchor_boxes_for_iou[:, 2] = anchors_current_scale[0,0,0,:,0]\n",
    "                anchor_boxes_for_iou[:, 3] = anchors_current_scale[0,0,0,:,1]\n",
    "\n",
    "                # Expand gt_box_dims for broadcasting\n",
    "                gt_box_dims_exp = gt_box_dims.unsqueeze(1).expand(-1, num_anchors, 4)  # (num_boxes, num_anchors, 4)\n",
    "                anchors_exp = anchor_boxes_for_iou.unsqueeze(0).expand(gt_box_dims.size(0), -1, 4)  # (num_boxes, num_anchors, 4)\n",
    "\n",
    "                ious = intersection_over_union(gt_box_dims_exp, anchors_exp, box_format=\"corners\").squeeze(-1)  # (num_boxes, num_anchors)\n",
    "                best_iou_anchor_idx = torch.argmax(ious, dim=1)  # (num_boxes,)\n",
    "\n",
    "                for idx in range(img_ids.shape[0]):\n",
    "                    i = img_ids[idx]\n",
    "                    c = class_ids[idx]\n",
    "                    cx = cell_x[idx]\n",
    "                    cy = cell_y[idx]\n",
    "                    best_anchor = best_iou_anchor_idx[idx]\n",
    "                    # Masks\n",
    "                    target_obj_mask[i, cy, cx, best_anchor] = 1.0\n",
    "                    target_noobj_mask[i, cy, cx, best_anchor] = 0.0\n",
    "                    # Coordinates\n",
    "                    tx[i, cy, cx, best_anchor] = x_center_grid[idx] - cx\n",
    "                    ty[i, cy, cx, best_anchor] = y_center_grid[idx] - cy\n",
    "                    tw[i, cy, cx, best_anchor] = torch.log(w_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,0] + 1e-16)\n",
    "                    th[i, cy, cx, best_anchor] = torch.log(h_gt_pix[idx] / anchors_current_scale[0,0,0,best_anchor,1] + 1e-16)\n",
    "                    # Class\n",
    "                    target_class_one_hot[i, cy, cx, best_anchor, c] = 1.0\n",
    "                    # Ignore anchors with high IoU\n",
    "                    for anchor_idx_other in range(num_anchors):\n",
    "                        if anchor_idx_other == best_anchor:\n",
    "                            continue\n",
    "                        if ious[idx, anchor_idx_other] > self.ignore_iou_threshold:\n",
    "                            target_noobj_mask[i, cy, cx, anchor_idx_other] = 0.0\n",
    "\n",
    "            loss_x = self.bce(pred_x_y[..., 0][target_obj_mask.bool()], tx[target_obj_mask.bool()])\n",
    "            loss_y = self.bce(pred_x_y[..., 1][target_obj_mask.bool()], ty[target_obj_mask.bool()])\n",
    "            loss_w = self.mse(pred_w_h[..., 0][target_obj_mask.bool()], tw[target_obj_mask.bool()])\n",
    "            loss_h = self.mse(pred_w_h[..., 1][target_obj_mask.bool()], th[target_obj_mask.bool()])\n",
    "            box_loss += (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "            loss_obj = self.bce(pred_obj[target_obj_mask.bool()], target_obj_mask[target_obj_mask.bool()].float().unsqueeze(-1))\n",
    "            loss_noobj = self.bce(pred_obj[target_noobj_mask.bool()], target_noobj_mask[target_noobj_mask.bool()].float().unsqueeze(-1))\n",
    "            obj_loss += loss_obj\n",
    "            noobj_loss += loss_noobj\n",
    "\n",
    "            loss_class = self.bce(pred_class[target_obj_mask.bool()], target_class_one_hot[target_obj_mask.bool()])\n",
    "            class_loss += loss_class\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + self.lambda_obj * obj_loss\n",
    "            + self.lambda_noobj * noobj_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "        return total_loss, {\"box_loss\": box_loss, \"obj_loss\": obj_loss, \"noobj_loss\": noobj_loss, \"class_loss\": class_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptación de las predicciones para la función de pérdida\n",
    "# Esta función adapta las predicciones del modelo para que coincidan con el formato esperado por la función de pérdida.\n",
    "\n",
    "def adapt_predictions_for_loss(predictions, num_classes):\n",
    "    adapted = []\n",
    "    for idx, p in enumerate(predictions):\n",
    "        if p.dim() != 5:\n",
    "            print(f\"WARNING: predictions[{idx}] tiene {p.dim()} dimensiones y shape {p.shape} (esperado: 5D). Se ignora este tensor.\")\n",
    "            continue  # salta este tensor\n",
    "        B, A, H, W, C = p.shape  # C=5+num_classes\n",
    "        p = p.permute(0, 1, 4, 2, 3).contiguous()  # [B, A, 5+C, H, W]\n",
    "        p = p.view(B, -1, H, W)  # [B, A*(5+C), H, W]\n",
    "        adapted.append(p)\n",
    "    return adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dba2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de decodificación de predicciones YOLOv3\n",
    "# Función de decodificación de predicciones YOLOv3\n",
    "\n",
    "def yolo_decode_outputs(outputs, anchors, num_classes, img_size, conf_thres=0.3, iou_thres=0.5, device='cpu', debug=False):\n",
    "    \"\"\"\n",
    "    Decodifica las predicciones de YOLOv3 (PyTorch) y aplica NMS.\n",
    "    Adapta automáticamente la forma del tensor (aplanado o no).\n",
    "    \"\"\"\n",
    "    from torchvision.ops import nms\n",
    "    batch_size = outputs[0].shape[0]\n",
    "    final_preds = [[] for _ in range(batch_size)]\n",
    "    for scale_idx, output in enumerate(outputs):\n",
    "        if debug:\n",
    "            print(f\"DEBUG: outputs[{scale_idx}].shape = {output.shape}\")\n",
    "        anchor_set = torch.tensor(anchors[scale_idx], dtype=torch.float32, device=device)\n",
    "        nA = len(anchors[scale_idx])\n",
    "        N = output.shape[1] if output.dim() == 3 else None\n",
    "        # Deducimos grid_size\n",
    "        if output.dim() == 3:\n",
    "            # Caso [batch, N, 5+C] (aplanado)\n",
    "            grid_size = int((output.shape[1] // nA) ** 0.5)\n",
    "            if output.shape[1] != nA * grid_size * grid_size:\n",
    "                raise ValueError(f\"Shape inesperado: {output.shape}\")\n",
    "            output = output.view(batch_size, nA, grid_size, grid_size, 5 + num_classes)\n",
    "            if debug:\n",
    "                print(f\" -> reshape a [{batch_size},{nA},{grid_size},{grid_size},{5+num_classes}]\")\n",
    "        elif output.dim() == 5:\n",
    "            # Caso ya en [batch, nA, grid, grid, 5+C]\n",
    "            grid_size = output.shape[2]\n",
    "            # No hace falta reshape\n",
    "        else:\n",
    "            raise ValueError(f\"Shape no soportado: {output.shape}\")\n",
    "        output = output.cpu()  # para facilitar\n",
    "        for b in range(batch_size):\n",
    "            pred = output[b]  # [nA, grid, grid, 5+C]\n",
    "            pred = pred.permute(0, 2, 3, 1).contiguous().view(-1, 5 + num_classes)  # [nA*grid*grid, 5+C]\n",
    "            # Sigmoid para cx, cy, conf, clases\n",
    "            pred_boxes = pred[:, :4]\n",
    "            pred_obj = pred[:, 4].sigmoid()\n",
    "            pred_cls = pred[:, 5:].sigmoid()\n",
    "            pred[:, 4:] = torch.cat([pred_obj.unsqueeze(1), pred_cls], 1)\n",
    "            # Filtra por confianza mínima\n",
    "            mask = pred_obj > conf_thres\n",
    "            pred = pred[mask]\n",
    "            if pred.shape[0] == 0:\n",
    "                continue\n",
    "            # Encuentra clase más probable y conf\n",
    "            class_confs, class_ids = pred[:, 5:].max(1)\n",
    "            scores = pred[:, 4] * class_confs\n",
    "            cx, cy, w, h = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]\n",
    "            # Las coord. suelen estar normalizadas a [0,1]\n",
    "            x1 = cx - w/2\n",
    "            y1 = cy - h/2\n",
    "            x2 = cx + w/2\n",
    "            y2 = cy + h/2\n",
    "            boxes_xyxy = torch.stack([x1, y1, x2, y2], 1)\n",
    "            keep = nms(boxes_xyxy, scores, iou_thres)\n",
    "            for idx in keep:\n",
    "                final_preds[b].append([\n",
    "                    int(class_ids[idx].item()),\n",
    "                    cx[idx].item(),\n",
    "                    cy[idx].item(),\n",
    "                    w[idx].item(),\n",
    "                    h[idx].item(),\n",
    "                    scores[idx].item()\n",
    "                ])\n",
    "    final_preds = [np.array(p) if len(p) else np.zeros((0,6)) for p in final_preds]\n",
    "    return final_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Anchors personalizados ==========\n",
    "ANCHORS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # 13x13\n",
    "    [(105, 113), (104, 96), (80, 109)],    # 26x26\n",
    "    [(112, 75), (87, 82), (39, 38)]        # 52x52\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de calculo de metricas de evaluación IoU, Precisión, Recall y mAP\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # box: (cx, cy, w, h) formato normalizado [0,1]\n",
    "    x1_min = box1[0] - box1[2] / 2\n",
    "    y1_min = box1[1] - box1[3] / 2\n",
    "    x1_max = box1[0] + box1[2] / 2\n",
    "    y1_max = box1[1] + box1[3] / 2\n",
    "\n",
    "    x2_min = box2[0] - box2[2] / 2\n",
    "    y2_min = box2[1] - box2[3] / 2\n",
    "    x2_max = box2[0] + box2[2] / 2\n",
    "    y2_max = box2[1] + box2[3] / 2\n",
    "\n",
    "    inter_xmin = max(x1_min, x2_min)\n",
    "    inter_ymin = max(y1_min, y2_min)\n",
    "    inter_xmax = min(x1_max, x2_max)\n",
    "    inter_ymax = min(y1_max, y2_max)\n",
    "    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - inter_area + 1e-12\n",
    "    return inter_area / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f23e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_epoch_metrics(model, loader, device, anchors=ANCHORS, num_classes=3, iou_threshold=0.5, conf_threshold=0.3):\n",
    "    model.eval()\n",
    "    total_iou = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            # 1. Obtén predicciones del modelo\n",
    "            outputs = model(images)\n",
    "            # 2. Decodifica y filtra predicciones\n",
    "            pred_boxes = yolo_decode_outputs(outputs, anchors, num_classes, img_size=416, conf_thres=conf_threshold, iou_thres=iou_threshold, device=device)\n",
    "            # 3. Para cada imagen del batch:\n",
    "            for i in range(len(targets)):\n",
    "                gt_boxes = targets[i].cpu().numpy()\n",
    "                preds_img = pred_boxes[i]  # [N_pred, 6] (class, cx, cy, w, h, conf)\n",
    "                matched_gt = set()\n",
    "                # Haz matching pred↔gt y calcula IoU, TP, FP, FN igual que antes\n",
    "                for pb in preds_img:\n",
    "                    best_iou = 0\n",
    "                    best_idx = -1\n",
    "                    for idx, gb in enumerate(gt_boxes):\n",
    "                        if int(pb[0]) != int(gb[0]):\n",
    "                            continue\n",
    "                        iou = calculate_iou(pb[1:5], gb[1:5])\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_idx = idx\n",
    "                    total_iou.append(best_iou)\n",
    "                    if best_iou >= iou_threshold and best_idx not in matched_gt:\n",
    "                        tp += 1\n",
    "                        matched_gt.add(best_idx)\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                fn += len(gt_boxes) - len(matched_gt)\n",
    "    mean_iou = np.mean(total_iou) if total_iou else 0.0\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    mAP = mean_iou  # Placeholder mAP\n",
    "    print(f\"IoU: {mean_iou:.4f} - Precisión: {precision:.4f} - Recall: {recall:.4f}\")\n",
    "    return mean_iou, precision, recall, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a774cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo YOLO Version 3\n",
    "\n",
    "# ========== 1. Anchors personalizados ==========\n",
    "ANCHORS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # 13x13\n",
    "    [(105, 113), (104, 96), (80, 109)],    # 26x26\n",
    "    [(112, 75), (87, 82), (39, 38)]        # 52x52\n",
    "]\n",
    "\n",
    "# ========== 2. Paths y parámetros ==========\n",
    "\n",
    "DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv')\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # Ajusta según tu máquina\n",
    "NUM_CLASSES = 3\n",
    "IMG_SIZE = YOLO_INPUT_SIZE\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# ========== 3. Crear datasets y dataloaders ==========\n",
    "\n",
    "full_df = pd.read_csv(CSV_FILE)\n",
    "all_image_filenames = full_df['filename'].unique().tolist()\n",
    "train_val_filenames, test_filenames = train_test_split(\n",
    "    all_image_filenames, test_size=0.15, random_state=42)\n",
    "train_filenames, val_filenames = train_test_split(\n",
    "    train_val_filenames, test_size=0.15/(1-0.15), random_state=42)\n",
    "\n",
    "train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "train_dataset = BloodCellDataset(DATA_ROOT, train_df, image_size=IMG_SIZE, transform=train_transforms)\n",
    "val_dataset = BloodCellDataset(DATA_ROOT, val_df, image_size=IMG_SIZE, transform=val_test_transforms)\n",
    "test_dataset = BloodCellDataset(DATA_ROOT, test_df, image_size=IMG_SIZE, transform=val_test_transforms)\n",
    "\n",
    "print(f\"Total imágenes de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Total imágenes de validación: {len(val_dataset)}\")\n",
    "print(f\"Total imágenes de prueba: {len(test_dataset)}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, bboxes = [], []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target)\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()  # Solo True si hay GPU\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"\\nDataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "# ========== 4. Pérdida y optimizador ==========\n",
    "loss_fn = YOLOv3Loss(\n",
    "    anchors=ANCHORS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_size=IMG_SIZE,\n",
    "    lambda_coord=1.0,\n",
    "    lambda_noobj=1.0,\n",
    "    lambda_obj=1.0,\n",
    "    lambda_class=1.0,\n",
    "    ignore_iou_threshold=0.5,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Solo parámetros entrenables (recuerda: solo los heads)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# ========== 5. Training Loop ==========\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Entrenando\"):\n",
    "        if images.shape[0] == 0:\n",
    "            print(\"Batch vacío saltado (entrenamiento)\")\n",
    "            continue  # Salta este batch vacío\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        if all_targets.numel() == 0:\n",
    "            print(\"Batch de imágenes sin ningún target: saltado.\")\n",
    "            continue\n",
    "        predictions = model(images)\n",
    "        #print([p.shape for p in predictions])\n",
    "        predictions = adapt_predictions_for_loss(predictions, num_classes=NUM_CLASSES)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #running_loss += loss.item()\n",
    "        #box_loss += components['box_loss'].item()\n",
    "        #obj_loss += components['obj_loss'].item()\n",
    "        #noobj_loss += components['noobj_loss'].item()\n",
    "        #class_loss += components['class_loss'].item()\n",
    "        running_loss += float(loss)\n",
    "        box_loss += float(components['box_loss'])\n",
    "        obj_loss += float(components['obj_loss'])\n",
    "        noobj_loss += float(components['noobj_loss'])\n",
    "        class_loss += float(components['class_loss'])\n",
    "    n = len(loader)\n",
    "    if n == 0:\n",
    "        return 0, 0, 0, 0, 0\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss, box_loss, obj_loss, noobj_loss, class_loss = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Validando\"):\n",
    "        if images.shape[0] == 0:\n",
    "            print(\"Batch vacío saltado (validacion)\")\n",
    "            continue  # Salta este batch vacío\n",
    "        images = images.to(device)\n",
    "        all_targets = []\n",
    "        for batch_idx, t in enumerate(targets):\n",
    "            if t.numel() > 0:\n",
    "                img_idx_col = torch.full((t.shape[0], 1), batch_idx, dtype=torch.float32, device=device)\n",
    "                all_targets.append(torch.cat([img_idx_col, t.to(device)], dim=1))\n",
    "        if len(all_targets) > 0:\n",
    "            all_targets = torch.cat(all_targets, dim=0)\n",
    "        else:\n",
    "            all_targets = torch.zeros((0, 6), dtype=torch.float32, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        if all_targets.numel() == 0:\n",
    "            print(\"Batch de imágenes sin ningún target: saltado.\")\n",
    "            continue\n",
    "        predictions = model(images)\n",
    "        #print([p.shape for p in predictions])\n",
    "        predictions = adapt_predictions_for_loss(predictions, num_classes=NUM_CLASSES)\n",
    "        loss, components = loss_fn(predictions, all_targets)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        #running_loss += loss.item()\n",
    "        #box_loss += components['box_loss'].item()\n",
    "        #obj_loss += components['obj_loss'].item()\n",
    "        #noobj_loss += components['noobj_loss'].item()\n",
    "        #class_loss += components['class_loss'].item()\n",
    "        running_loss += float(loss)\n",
    "        box_loss += float(components['box_loss'])\n",
    "        obj_loss += float(components['obj_loss'])\n",
    "        noobj_loss += float(components['noobj_loss'])\n",
    "        class_loss += float(components['class_loss'])\n",
    "    n = len(loader)\n",
    "    if n == 0:\n",
    "        return 0, 0, 0, 0, 0\n",
    "    return (running_loss/n, box_loss/n, obj_loss/n, noobj_loss/n, class_loss/n)\n",
    "\n",
    "# ========== 6. Loop principal de entrenamiento ==========\n",
    "\n",
    "patience = 10  # Epochs sin mejora antes de parar\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "writer = SummaryWriter(log_dir='runs/yolov3_bloodcell')\n",
    "writer = SummaryWriter(log_dir=\"runs/yolov3_metrics\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\n=== Época {epoch}/{EPOCHS} ===\")\n",
    "    t_loss, t_box, t_obj, t_noobj, t_class = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    v_loss, v_box, v_obj, v_noobj, v_class = validate_one_epoch(model, val_loader, loss_fn, device)\n",
    "    mean_iou_train, precision_train, recall_train, mAP_train = simple_epoch_metrics(model, train_loader, device)\n",
    "    mean_iou_val, precision_val, recall_val, mAP_val = simple_epoch_metrics(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f}\")\n",
    "    print(f\"  [Train] Box: {t_box:.4f} Obj: {t_obj:.4f} NoObj: {t_noobj:.4f} Class: {t_class:.4f}\")\n",
    "    print(f\"  [Val]   Box: {v_box:.4f} Obj: {v_obj:.4f} NoObj: {v_noobj:.4f} Class: {v_class:.4f}\")\n",
    "\n",
    "    # Early stopping y guardado del mejor modelo\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), \"best_yolov3_bloodcell.pth\")\n",
    "        print(\">> Modelo guardado (mejor Val Loss)\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No mejora en val_loss. Paciencia: {epochs_no_improve}/{patience}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping activado: no hay mejora en la validación.\")\n",
    "            break\n",
    "\n",
    "    # TensorBoard ... Registra las pérdidas y métricas\n",
    "    writer.add_scalar(\"Loss/Train\", t_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Val\", v_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Train\", t_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Box_Val\", v_box, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Train\", t_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/Obj_Val\", v_obj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Train\", t_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/NoObj_Val\", v_noobj, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Train\", t_class, epoch)\n",
    "    writer.add_scalar(\"Loss/Class_Val\", v_class, epoch)\n",
    "        \n",
    "    writer.add_scalar(\"IoU/Train\", mean_iou_train, epoch)\n",
    "    writer.add_scalar(\"IoU/Val\", mean_iou_val, epoch)\n",
    "    writer.add_scalar(\"Precision/Train\", precision_train, epoch)\n",
    "    writer.add_scalar(\"Precision/Val\", precision_val, epoch)\n",
    "    writer.add_scalar(\"Recall/Train\", recall_train, epoch)\n",
    "    writer.add_scalar(\"Recall/Val\", recall_val, epoch)\n",
    "    writer.add_scalar(\"mAP/Train\", mAP_train, epoch)\n",
    "    writer.add_scalar(\"mAP/Val\", mAP_val, epoch)\n",
    "\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea926074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVALUACIÓN FINAL EN EL SET DE TEST ====\n",
    "print(\"\\n--- Comenzando evaluación final en el set de test ---\")\n",
    "# Vuelve a cargar el mejor modelo (por si el modelo en memoria no es el mejor tras early stopping)\n",
    "model.load_state_dict(torch.load(\"best_yolov3_bloodcell.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Calcula métricas en el test set\n",
    "mean_iou_test, precision_test, recall_test, mAP_test = simple_epoch_metrics(model, test_loader, device)\n",
    "print(f\"[Test] IoU: {mean_iou_test:.4f} | Precision: {precision_test:.4f} | Recall: {recall_test:.4f} | mAP: {mAP_test:.4f}\")\n",
    "\n",
    "# Loguea en TensorBoard (opcional)\n",
    "writer = SummaryWriter(log_dir='runs/yolov3_bloodcell')\n",
    "writer.add_scalar(\"Test/IoU\", mean_iou_test)\n",
    "writer.add_scalar(\"Test/Precision\", precision_test)\n",
    "writer.add_scalar(\"Test/Recall\", recall_test)\n",
    "writer.add_scalar(\"Test/mAP\", mAP_test)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1eb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion incluyendo mAP\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # box1, box2: [cx, cy, w, h], normalizados [0,1]\n",
    "    x1_min, y1_min = box1[0] - box1[2]/2, box1[1] - box1[3]/2\n",
    "    x1_max, y1_max = box1[0] + box1[2]/2, box1[1] + box1[3]/2\n",
    "    x2_min, y2_min = box2[0] - box2[2]/2, box2[1] - box2[3]/2\n",
    "    x2_max, y2_max = box2[0] + box2[2]/2, box2[1] + box2[3]/2\n",
    "\n",
    "    inter_xmin = max(x1_min, x2_min)\n",
    "    inter_ymin = max(y1_min, y2_min)\n",
    "    inter_xmax = min(x1_max, x2_max)\n",
    "    inter_ymax = min(y1_max, y2_max)\n",
    "    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
    "\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - inter_area + 1e-12\n",
    "\n",
    "    return inter_area / union\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    # Calcula AP interpolando (estilo COCO)\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n",
    "    return ap\n",
    "\n",
    "def simple_epoch_metrics(model, loader, device, anchors, num_classes, iou_thresholds=np.arange(0.5, 1.0, 0.05), conf_threshold=0.3):\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    ious_all = []\n",
    "    tp_cum, fp_cum, fn_cum = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(loader, desc=\"Eval métricas\")):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_boxes = yolo_decode_outputs(outputs, anchors, num_classes, img_size=416, conf_thres=conf_threshold, device=device)\n",
    "            for img_idx in range(len(targets)):\n",
    "                gt = targets[img_idx].cpu().numpy()\n",
    "                preds = pred_boxes[img_idx]  # [N_pred, 6]: class, cx, cy, w, h, conf\n",
    "\n",
    "                # Guardar para mAP: (img_idx, class, conf, x, y, w, h)\n",
    "                for pb in preds:\n",
    "                    all_pred.append([batch_idx * loader.batch_size + img_idx, int(pb[0]), pb[5], pb[1], pb[2], pb[3], pb[4]])\n",
    "                for gb in gt:\n",
    "                    all_gt.append([batch_idx * loader.batch_size + img_idx, int(gb[0]), gb[1], gb[2], gb[3], gb[4]])\n",
    "\n",
    "                # Métricas simples (IoU, TP, FP, FN, por threshold .5):\n",
    "                matched_gt = set()\n",
    "                for pb in preds:\n",
    "                    best_iou = 0\n",
    "                    best_idx = -1\n",
    "                    for idx, gb in enumerate(gt):\n",
    "                        if int(pb[0]) != int(gb[0]):\n",
    "                            continue\n",
    "                        iou = calculate_iou(pb[1:5], gb[1:5])\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_idx = idx\n",
    "                    ious_all.append(best_iou)\n",
    "                    if best_iou >= 0.5 and best_idx not in matched_gt:\n",
    "                        tp_cum += 1\n",
    "                        matched_gt.add(best_idx)\n",
    "                    else:\n",
    "                        fp_cum += 1\n",
    "                fn_cum += len(gt) - len(matched_gt)\n",
    "\n",
    "    # IoU, precision, recall simples\n",
    "    mean_iou = np.mean(ious_all) if ious_all else 0.0\n",
    "    precision = tp_cum / (tp_cum + fp_cum + 1e-8)\n",
    "    recall = tp_cum / (tp_cum + fn_cum + 1e-8)\n",
    "\n",
    "    # ==== mAP COCO (mAP@[.5:.95]) ====\n",
    "    all_pred = np.array(all_pred)\n",
    "    all_gt = np.array(all_gt)\n",
    "    ap_per_class = []\n",
    "    for c in range(num_classes):\n",
    "        ap_c = []\n",
    "        for iou_thr in iou_thresholds:\n",
    "            # Preparar predicciones de esta clase, ordenadas por confianza\n",
    "            pred_c = all_pred[all_pred[:,1]==c] if len(all_pred) else np.zeros((0,7))\n",
    "            pred_c = pred_c[np.argsort(-pred_c[:,2])] if len(pred_c) else pred_c\n",
    "            gt_c = all_gt[all_gt[:,1]==c] if len(all_gt) else np.zeros((0,6))\n",
    "            n_gt = len(gt_c)\n",
    "            n_pred = len(pred_c)\n",
    "            tp = np.zeros(n_pred)\n",
    "            fp = np.zeros(n_pred)\n",
    "            gt_img_set = defaultdict(set)\n",
    "            for i in range(n_pred):\n",
    "                pred_box = pred_c[i, 3:7]\n",
    "                pred_img = int(pred_c[i,0])\n",
    "                # Buscar el GT con mayor IoU en la misma imagen y clase\n",
    "                ious = np.array([calculate_iou(pred_box, gb[2:6]) for gb in gt_c if int(gb[0])==pred_img])\n",
    "                if len(ious) > 0 and np.max(ious) >= iou_thr:\n",
    "                    max_idx = np.argmax(ious)\n",
    "                    gt_idx = [idx for idx, gb in enumerate(gt_c) if int(gb[0])==pred_img][max_idx]\n",
    "                    # Evitar duplicados\n",
    "                    if gt_idx not in gt_img_set[pred_img]:\n",
    "                        tp[i] = 1\n",
    "                        gt_img_set[pred_img].add(gt_idx)\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "                else:\n",
    "                    fp[i] = 1\n",
    "            # Cálculo de recall y precision acumulados\n",
    "            tp_cum_ = np.cumsum(tp)\n",
    "            fp_cum_ = np.cumsum(fp)\n",
    "            recall_curve = tp_cum_ / (n_gt + 1e-12)\n",
    "            precision_curve = tp_cum_ / (tp_cum_ + fp_cum_ + 1e-12)\n",
    "            ap = compute_ap(recall_curve, precision_curve)\n",
    "            ap_c.append(ap)\n",
    "        ap_per_class.append(np.mean(ap_c))\n",
    "    mAP = np.mean(ap_per_class)\n",
    "\n",
    "    print(f\"IoU: {mean_iou:.4f} - Precisión: {precision:.4f} - Recall: {recall:.4f} - mAP@[.5:.95]: {mAP:.4f}\")\n",
    "    return mean_iou, precision, recall, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe45261",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou, precision, recall, mAP = simple_epoch_metrics(model, val_loader, device, anchors=ANCHORS, num_classes=NUM_CLASSES)\n",
    "print(f\"[Val] IoU: {mean_iou:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "mean_iou, precision, recall, mAP = simple_epoch_metrics(model, val_loader, device, anchors=ANCHORS, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788571d",
   "metadata": {},
   "source": [
    "¿Dudas sobre la integración o quieres un ejemplo con data dummy? ¡Dímelo! </answer>\n",
    "\n",
    "<answer> La función `simple_epoch_metrics` que te he proporcionado calcula:\n",
    "mAP@[0.5:0.95] (estilo COCO):\n",
    "\n",
    "Calcula el Average Precision (AP) para cada clase, promediando sobre múltiples IoU thresholds (np.arange(0.5, 1.0, 0.05)), es decir: 0.50, 0.55, 0.60, ... 0.95.\n",
    "Calcula el mAP global como el promedio de los AP de todas las clases (lo que se llama mAP@[.5:.95] en COCO).\n",
    "Devuelve (en el return):\n",
    "\n",
    "mean_iou: IoU promedio (umbral fijo 0.5)\n",
    "precision: precisión global (umbral fijo 0.5)\n",
    "recall: recall global (umbral fijo 0.5)\n",
    "mAP: mAP global (promedio de AP de todas las clases y todos los thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7a114",
   "metadata": {},
   "source": [
    "¿Cómo obtener el AP por clase?\n",
    "Ahora mismo, el mAP global se calcula así:\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "ap_per_class = []  # lista con un AP por clase\n",
    "for c in range(num_classes):\n",
    "    ap_c = []\n",
    "    for iou_thr in iou_thresholds:\n",
    "        # ... (cálculo de AP para cada clase y threshold)\n",
    "        ap_c.append(ap)\n",
    "    ap_per_class.append(np.mean(ap_c))\n",
    "mAP = np.mean(ap_per_class)\n",
    "Si quieres obtener también el AP por clase, puedes cambiar el return así:\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "return mean_iou, precision, recall, mAP, ap_per_class\n",
    "Y así tendrás el AP promedio de cada clase.\n",
    "\n",
    "Resumen\n",
    "mAP global: mAP@[0.5:0.95] (promedio de clases y de thresholds).\n",
    "AP por clase: está disponible en la lista ap_per_class, solo debes retornarla si la necesitas.\n",
    "No calcula solo mAP@0.5 (aunque puedes hacerlo pasando iou_thresholds=[0.5]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine\n",
    "¡Aquí tienes la versión completa y robusta de simple_epoch_metrics que calcula y devuelve:\n",
    "\n",
    "IoU medio, precisión y recall global (umbral 0.5)\n",
    "mAP@[.5:.95] global (COCO)\n",
    "mAP@0.5 global (VOC)\n",
    "AP por clase para mAP@[.5:.95] y AP por clase para mAP@0.5\n",
    "Además, el docstring explica todo lo que devuelve.\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # box1, box2: [cx, cy, w, h], normalizados [0,1]\n",
    "    x1_min, y1_min = box1[0] - box1[2]/2, box1[1] - box1[3]/2\n",
    "    x1_max, y1_max = box1[0] + box1[2]/2, box1[1] + box1[3]/2\n",
    "    x2_min, y2_min = box2[0] - box2[2]/2, box2[1] - box2[3]/2\n",
    "    x2_max, y2_max = box2[0] + box2[2]/2, box2[1] + box2[3]/2\n",
    "\n",
    "    inter_xmin = max(x1_min, x2_min)\n",
    "    inter_ymin = max(y1_min, y2_min)\n",
    "    inter_xmax = min(x1_max, x2_max)\n",
    "    inter_ymax = min(y1_max, y2_max)\n",
    "    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
    "\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - inter_area + 1e-12\n",
    "\n",
    "    return inter_area / union\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    # Calcula AP interpolando (estilo COCO)\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n",
    "    return ap\n",
    "\n",
    "def simple_epoch_metrics(\n",
    "    model, loader, device, anchors, num_classes,\n",
    "    iou_thresholds=np.arange(0.5, 1.0, 0.05), conf_threshold=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "    - mean_iou: IoU medio global (umbral 0.5)\n",
    "    - precision: precisión global (umbral 0.5)\n",
    "    - recall: recall global (umbral 0.5)\n",
    "    - mAP_coco: mAP@[.5:.95] global (promedio sobre clases y thresholds)\n",
    "    - mAP_voc: mAP@.5 global (VOC)\n",
    "    - ap_per_class_coco: array [num_classes] con AP COCO para cada clase\n",
    "    - ap_per_class_voc: array [num_classes] con AP@.5 para cada clase\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    ious_all = []\n",
    "    tp_cum, fp_cum, fn_cum = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(loader, desc=\"Eval métricas\")):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_boxes = yolo_decode_outputs(outputs, anchors, num_classes, img_size=416, conf_thres=conf_threshold, device=device)\n",
    "            for img_idx in range(len(targets)):\n",
    "                gt = targets[img_idx].cpu().numpy()\n",
    "                preds = pred_boxes[img_idx]  # [N_pred, 6]: class, cx, cy, w, h, conf\n",
    "\n",
    "                # Guardar para mAP: (img_idx, class, conf, x, y, w, h)\n",
    "                for pb in preds:\n",
    "                    all_pred.append([batch_idx * loader.batch_size + img_idx, int(pb[0]), pb[5], pb[1], pb[2], pb[3], pb[4]])\n",
    "                for gb in gt:\n",
    "                    all_gt.append([batch_idx * loader.batch_size + img_idx, int(gb[0]), gb[1], gb[2], gb[3], gb[4]])\n",
    "\n",
    "                # Métricas simples (IoU, TP, FP, FN, por threshold .5):\n",
    "                matched_gt = set()\n",
    "                for pb in preds:\n",
    "                    best_iou = 0\n",
    "                    best_idx = -1\n",
    "                    for idx, gb in enumerate(gt):\n",
    "                        if int(pb[0]) != int(gb[0]):\n",
    "                            continue\n",
    "                        iou = calculate_iou(pb[1:5], gb[1:5])\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_idx = idx\n",
    "                    ious_all.append(best_iou)\n",
    "                    if best_iou >= 0.5 and best_idx not in matched_gt:\n",
    "                        tp_cum += 1\n",
    "                        matched_gt.add(best_idx)\n",
    "                    else:\n",
    "                        fp_cum += 1\n",
    "                fn_cum += len(gt) - len(matched_gt)\n",
    "\n",
    "    # IoU, precision, recall simples\n",
    "    mean_iou = np.mean(ious_all) if ious_all else 0.0\n",
    "    precision = tp_cum / (tp_cum + fp_cum + 1e-8)\n",
    "    recall = tp_cum / (tp_cum + fn_cum + 1e-8)\n",
    "\n",
    "    # ==== mAP COCO (mAP@[.5:.95]) y mAP VOC (mAP@.5) ====\n",
    "    all_pred = np.array(all_pred)\n",
    "    all_gt = np.array(all_gt)\n",
    "    ap_per_class_coco = []\n",
    "    ap_per_class_voc = []\n",
    "    for c in range(num_classes):\n",
    "        ap_c = []\n",
    "        ap_c_voc = []\n",
    "        for iou_thr in iou_thresholds:\n",
    "            # Preparar predicciones de esta clase, ordenadas por confianza\n",
    "            pred_c = all_pred[all_pred[:,1]==c] if len(all_pred) else np.zeros((0,7))\n",
    "            pred_c = pred_c[np.argsort(-pred_c[:,2])] if len(pred_c) else pred_c\n",
    "            gt_c = all_gt[all_gt[:,1]==c] if len(all_gt) else np.zeros((0,6))\n",
    "            n_gt = len(gt_c)\n",
    "            n_pred = len(pred_c)\n",
    "            tp = np.zeros(n_pred)\n",
    "            fp = np.zeros(n_pred)\n",
    "            gt_img_set = defaultdict(set)\n",
    "            for i in range(n_pred):\n",
    "                pred_box = pred_c[i, 3:7]\n",
    "                pred_img = int(pred_c[i,0])\n",
    "                # Buscar el GT con mayor IoU en la misma imagen y clase\n",
    "                ious = np.array([calculate_iou(pred_box, gb[2:6]) for gb in gt_c if int(gb[0])==pred_img])\n",
    "                if len(ious) > 0 and np.max(ious) >= iou_thr:\n",
    "                    max_idx = np.argmax(ious)\n",
    "                    gt_idx = [idx for idx, gb in enumerate(gt_c) if int(gb[0])==pred_img][max_idx]\n",
    "                    # Evitar duplicados\n",
    "                    if gt_idx not in gt_img_set[pred_img]:\n",
    "                        tp[i] = 1\n",
    "                        gt_img_set[pred_img].add(gt_idx)\n",
    "                    else:\n",
    "                        fp[i] = 1\n",
    "                else:\n",
    "                    fp[i] = 1\n",
    "            # Cálculo de recall y precision acumulados\n",
    "            tp_cum_ = np.cumsum(tp)\n",
    "            fp_cum_ = np.cumsum(fp)\n",
    "            recall_curve = tp_cum_ / (n_gt + 1e-12)\n",
    "            precision_curve = tp_cum_ / (tp_cum_ + fp_cum_ + 1e-12)\n",
    "            ap = compute_ap(recall_curve, precision_curve)\n",
    "            ap_c.append(ap)\n",
    "            if iou_thr == 0.5:\n",
    "                ap_c_voc.append(ap)\n",
    "        ap_per_class_coco.append(np.mean(ap_c))\n",
    "        ap_per_class_voc.append(np.mean(ap_c_voc) if ap_c_voc else 0.0)\n",
    "    mAP_coco = np.mean(ap_per_class_coco)\n",
    "    mAP_voc = np.mean(ap_per_class_voc)\n",
    "\n",
    "    print(f\"IoU: {mean_iou:.4f} - Precisión: {precision:.4f} - Recall: {recall:.4f} - mAP@[.5:.95]: {mAP_coco:.4f} - mAP@.5: {mAP_voc:.4f}\")\n",
    "    for idx, (ap_coco, ap_voc) in enumerate(zip(ap_per_class_coco, ap_per_class_voc)):\n",
    "        print(f\" Clase {idx}: AP@[.5:.95]={ap_coco:.4f} | AP@.5={ap_voc:.4f}\")\n",
    "\n",
    "    return mean_iou, precision, recall, mAP_coco, mAP_voc, ap_per_class_coco, ap_per_class_voc\n",
    "¿Cómo usarla?\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "mean_iou, precision, recall, mAP_coco, mAP_voc, ap_coco_cls, ap_voc_cls = simple_epoch_metrics(\n",
    "    model, val_loader, device, anchors=ANCHORS, num_classes=NUM_CLASSES)\n",
    "mAP_coco es el mAP@[.5:.95] global\n",
    "mAP_voc es el mAP@.5 global\n",
    "ap_coco_cls es una lista con AP por clase (COCO)\n",
    "ap_voc_cls es una lista con AP por clase (VOC)\n",
    "¡Ya tienes métricas completas y listas para análisis profesional y para loguear en TensorBoard!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "Creo que ya nos falta solo poder visualizar las predicciones sobre las imagenes de test ... puede generar el codigo que lo hace por favor ?\n",
    "\n",
    "Cosine\n",
    "¡Por supuesto!\n",
    "Aquí tienes una función lista para visualizar las predicciones del modelo YOLOv3 sobre imágenes del set de test, mostrando las bounding boxes predichas (y opcionalmente las GT) con color y etiqueta de clase:\n",
    "\n",
    "python\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_yolo_predictions(\n",
    "    model, dataloader, anchors, num_classes, class_names,\n",
    "    device, img_size=416, conf_threshold=0.3, iou_threshold=0.5,\n",
    "    max_images=10, show_gt=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones del modelo YOLOv3 sobre imágenes reales.\n",
    "    - model: tu modelo\n",
    "    - dataloader: DataLoader (test o val)\n",
    "    - anchors, num_classes: como en tu pipeline\n",
    "    - class_names: lista de strings de nombres de clase\n",
    "    - device: cpu o cuda\n",
    "    - img_size: tamaño de entrada del modelo\n",
    "    - conf_threshold: umbral de confianza para mostrar predicciones\n",
    "    - iou_threshold: NMS\n",
    "    - max_images: máximo de imágenes a visualizar\n",
    "    - show_gt: si True, dibuja también las GT\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    with torch.no_grad():\n",
    "        shown = 0\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_boxes = yolo_decode_outputs(\n",
    "                outputs, anchors, num_classes, img_size=img_size,\n",
    "                conf_thres=conf_threshold, iou_thres=iou_threshold, device=device\n",
    "            )\n",
    "            for i in range(images.shape[0]):\n",
    "                img_np = images[i].cpu().permute(1,2,0).numpy()\n",
    "                # Desnormaliza si usaste normalización estándar\n",
    "                img_np = img_np * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])\n",
    "                img_np = (img_np * 255).clip(0,255).astype(np.uint8)\n",
    "                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Dibuja predicciones\n",
    "                for pb in pred_boxes[i]:\n",
    "                    class_id = int(pb[0])\n",
    "                    cx, cy, w, h, conf = pb[1], pb[2], pb[3], pb[4], pb[5]\n",
    "                    x1 = int((cx - w/2) * img_bgr.shape[1])\n",
    "                    y1 = int((cy - h/2) * img_bgr.shape[0])\n",
    "                    x2 = int((cx + w/2) * img_bgr.shape[1])\n",
    "                    y2 = int((cy + h/2) * img_bgr.shape[0])\n",
    "                    color = tuple([int(255*x) for x in cmap(class_id)[:3]])\n",
    "                    cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 2)\n",
    "                    label = f\"{class_names[class_id]} {conf:.2f}\"\n",
    "                    cv2.putText(img_bgr, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "                # Dibuja GT (opcional)\n",
    "                if show_gt and len(targets[i]) > 0:\n",
    "                    for gt in targets[i]:\n",
    "                        class_id = int(gt[0].item())\n",
    "                        cx, cy, w, h = gt[1:].tolist()\n",
    "                        x1 = int((cx - w/2) * img_bgr.shape[1])\n",
    "                        y1 = int((cy - h/2) * img_bgr.shape[0])\n",
    "                        x2 = int((cx + w/2) * img_bgr.shape[1])\n",
    "                        y2 = int((cy + h/2) * img_bgr.shape[0])\n",
    "                        color = (0,255,0)  # Verde para GT (puedes cambiarlo)\n",
    "                        cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 1)\n",
    "                        cv2.putText(img_bgr, class_names[class_id], (x1, y2+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                # Muestra la imagen\n",
    "                plt.figure(figsize=(8,8))\n",
    "                plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "                plt.title('Predicciones (rectángulo grueso) / GT (verde fino)')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                shown += 1\n",
    "                if shown >= max_images:\n",
    "                    return\n",
    "¿Cómo usarla?\n",
    "python\n",
    "\n",
    "\n",
    "class_names = ['RBC', 'WBC', 'Platelets']\n",
    "show_yolo_predictions(\n",
    "    model, test_loader, anchors=ANCHORS, num_classes=NUM_CLASSES,\n",
    "    class_names=class_names, device=device, img_size=416,\n",
    "    conf_threshold=0.3, iou_threshold=0.5, max_images=5, show_gt=True\n",
    ")\n",
    "Cambia test_loader por el DataLoader que quieras visualizar.\n",
    "Ajusta max_images para ver más o menos imágenes.\n",
    "Las predicciones aparecen con rectángulo grueso y la clase + score.\n",
    "Las GT (si show_gt=True) aparecen con rectángulo fino verde y la clase.\n",
    "Puedes desactivar las GT poniendo show_gt=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_yolo_predictions(\n",
    "    model, dataloader, anchors, num_classes, class_names,\n",
    "    device, img_size=416, conf_threshold=0.3, iou_threshold=0.5,\n",
    "    max_images=10, show_gt=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones del modelo YOLOv3 sobre imágenes reales.\n",
    "    - model: tu modelo\n",
    "    - dataloader: DataLoader (test o val)\n",
    "    - anchors, num_classes: como en tu pipeline\n",
    "    - class_names: lista de strings de nombres de clase\n",
    "    - device: cpu o cuda\n",
    "    - img_size: tamaño de entrada del modelo\n",
    "    - conf_threshold: umbral de confianza para mostrar predicciones\n",
    "    - iou_threshold: NMS\n",
    "    - max_images: máximo de imágenes a visualizar\n",
    "    - show_gt: si True, dibuja también las GT\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    with torch.no_grad():\n",
    "        shown = 0\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_boxes = yolo_decode_outputs(\n",
    "                outputs, anchors, num_classes, img_size=img_size,\n",
    "                conf_thres=conf_threshold, iou_thres=iou_threshold, device=device\n",
    "            )\n",
    "            for i in range(images.shape[0]):\n",
    "                img_np = images[i].cpu().permute(1,2,0).numpy()\n",
    "                # Desnormaliza si usaste normalización estándar\n",
    "                img_np = img_np * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])\n",
    "                img_np = (img_np * 255).clip(0,255).astype(np.uint8)\n",
    "                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Dibuja predicciones\n",
    "                for pb in pred_boxes[i]:\n",
    "                    class_id = int(pb[0])\n",
    "                    cx, cy, w, h, conf = pb[1], pb[2], pb[3], pb[4], pb[5]\n",
    "                    x1 = int((cx - w/2) * img_bgr.shape[1])\n",
    "                    y1 = int((cy - h/2) * img_bgr.shape[0])\n",
    "                    x2 = int((cx + w/2) * img_bgr.shape[1])\n",
    "                    y2 = int((cy + h/2) * img_bgr.shape[0])\n",
    "                    color = tuple([int(255*x) for x in cmap(class_id)[:3]])\n",
    "                    cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 2)\n",
    "                    label = f\"{class_names[class_id]} {conf:.2f}\"\n",
    "                    cv2.putText(img_bgr, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "                # Dibuja GT (opcional)\n",
    "                if show_gt and len(targets[i]) > 0:\n",
    "                    for gt in targets[i]:\n",
    "                        class_id = int(gt[0].item())\n",
    "                        cx, cy, w, h = gt[1:].tolist()\n",
    "                        x1 = int((cx - w/2) * img_bgr.shape[1])\n",
    "                        y1 = int((cy - h/2) * img_bgr.shape[0])\n",
    "                        x2 = int((cx + w/2) * img_bgr.shape[1])\n",
    "                        y2 = int((cy + h/2) * img_bgr.shape[0])\n",
    "                        color = (0,255,0)  # Verde para GT (puedes cambiarlo)\n",
    "                        cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 1)\n",
    "                        cv2.putText(img_bgr, class_names[class_id], (x1, y2+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                # Muestra la imagen\n",
    "                plt.figure(figsize=(8,8))\n",
    "                plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "                plt.title('Predicciones (rectángulo grueso) / GT (verde fino)')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                shown += 1\n",
    "                if shown >= max_images:\n",
    "                    return\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "class_names = ['RBC', 'WBC', 'Platelets']\n",
    "show_yolo_predictions(\n",
    "    model, test_loader, anchors=ANCHORS, num_classes=NUM_CLASSES,\n",
    "    class_names=class_names, device=device, img_size=416,\n",
    "    conf_threshold=0.3, iou_threshold=0.5, max_images=5, show_gt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd170c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
