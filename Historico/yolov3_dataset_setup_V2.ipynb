{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419feb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta version se supone que es equivalente a la anterior\n",
    "# pereo intenta mostrar las imagenes  (algunas de ellas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d77b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae7387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = row['cell_type']\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax'])\n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # --- NUEVO FILTRADO AQUÍ ---\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "                # --- FIN NUEVO FILTRADO ---\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        \n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        if self.transform:\n",
    "            # El check_bbox de Albumentations ocurre AQUÍ dentro de self.transform\n",
    "            transformed = self.transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            class_labels = transformed['class_labels']\n",
    "            \n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                continue \n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        if len(yolo_bboxes) == 0:\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3669429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.RandomCrop(height=YOLO_INPUT_SIZE[1], width=YOLO_INPUT_SIZE[0], p=0.8),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2), \n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0   ), \n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) \n",
    "\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dfb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio en los que estan almacenadas las imagenes y las anotaciones \n",
    "# C:\\Users\\gtoma\\Master_AI_Aplicada\\GitHubRep\\PyTorch-YOLOv3\\dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f050685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset/annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "Tamaño del lote 2: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "Tamaño del lote 3: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "Tamaño del lote 4: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "Tamaño del lote 5: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "\n",
      "No se encontró ninguna imagen con bounding boxes en los primeros 5 lotes.\n",
      "Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Mofificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset/annotations.csv'\n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15   # 15% para el conjunto de prueba\n",
    "    VAL_SPLIT_RATIO = 0.15    # 15% para el conjunto de validación (del resto de los datos)\n",
    "    RANDOM_SEED = 42          # Semilla para la reproducibilidad de la división\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 5 # Intenta revisar más lotes si el primero no tiene cajas\n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\") \n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if len(targets[img_idx]) > 0: \n",
    "                print(f\"--- Encontrada imagen con cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0]) \n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                img_display_rgb = images[img_idx].permute(1, 2, 0).cpu().numpy() * 255\n",
    "                img_display_rgb = img_display_rgb.astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)}\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    xmin_norm = cx - w/2\n",
    "                    ymin_norm = cy - h/2\n",
    "                    xmax_norm = cx + w/2\n",
    "                    ymax_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(xmin_norm * img_w)\n",
    "                    y_min_px = int(ymin_norm * img_h)\n",
    "                    x_max_px = int(xmax_norm * img_w)\n",
    "                    y_max_px = int(ymax_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255))\n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break # Romper el bucle de imágenes del lote\n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break # Romper el bucle de lotes si encontramos una imagen o revisamos suficientes\n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
