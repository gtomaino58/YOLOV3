{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESta version solo procesa el dataset y da mensajes \n",
    "# No visualiza ninguna imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d77b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root (str): Ruta al directorio raíz que contiene la carpeta 'BCCD' (tus imágenes).\n",
    "            annotations_df (pd.DataFrame): DataFrame de pandas con las anotaciones ya cargadas y pre-filtradas para este split (train/val/test).\n",
    "            image_size (tuple): Tamaño (ancho, alto) al que se redimensionarán las imágenes para el modelo.\n",
    "            transform (albumentations.Compose, optional): Transformaciones a aplicar.\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD') # Asumiendo que las imágenes están en data_root/BCCD\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Mapeo de nombres de clase a IDs numéricos (¡AJUSTA EL ORDEN SI ES NECESARIO!)\n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0,        # Glóbulo Rojo\n",
    "            'WBC': 1,        # Glóbulo Blanco\n",
    "            'Platelets': 2   # Plaquetas\n",
    "        }\n",
    "        \n",
    "        # --- NUEVO: Pre-procesar el DataFrame para este split ---\n",
    "        # Agrupar anotaciones por filename\n",
    "        self.image_annotations = {}\n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = row['cell_type']\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax'])\n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys()) # Lista de imágenes para este split\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        \n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            class_labels = transformed['class_labels']\n",
    "            \n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            if width <= 0 or height <= 0:\n",
    "                continue \n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        if len(yolo_bboxes) == 0:\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3669429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.RandomCrop(height=YOLO_INPUT_SIZE[1], width=YOLO_INPUT_SIZE[0], p=0.8),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2), \n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0   ), \n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) \n",
    "\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dfb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio en los que estan almacenadas las imagenes y las anotaciones \n",
    "# C:\\Users\\gtoma\\Master_AI_Aplicada\\GitHubRep\\PyTorch-YOLOv3\\dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6420daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset/annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "Tamaño del lote de imágenes: torch.Size([8, 3, 416, 416])\n",
      "Número de imágenes en el lote (lista de targets): 8\n",
      "Primera imagen del lote de entrenamiento sin cajas anotadas (o filtradas).\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset/annotations.csv'\n",
    "    \n",
    "    # Parámetros de la división en train/val/test\n",
    "    TEST_SPLIT_RATIO = 0.15   # 15% para el conjunto de prueba\n",
    "    VAL_SPLIT_RATIO = 0.15    # 15% para el conjunto de validación (del resto de los datos)\n",
    "    \n",
    "    # Semilla para la división aleatoria\n",
    "    RANDOM_SEED = 1234        # Semilla para la reproducibilidad de la división\n",
    "\n",
    "    # Parámetros del DataLoader\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # Cargando todas las anotaciones y obteniendo nombres de archivo únicos\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obteniendo la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # Dividiendo los nombres de archivo en entrenamiento y test\n",
    "    # Primero separamos el conjunto de prueba\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Luego, dividimos el conjunto de entrenamiento/validación en entrenamiento y validación\n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), # Proporción relativa al conjunto restante\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # Creamos DataFrames de anotaciones para cada split\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # Creamos instancias del Dataset y DataLoader para cada split\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, # Pasar el DataFrame filtrado\n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, # Pasar el DataFrame filtrado\n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms # Usar transformaciones sin aumento para validación\n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, # Pasar el DataFrame filtrado\n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms # Usar transformaciones sin aumento para prueba\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Verificamos la carga de un lote de entrenamiento\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    for images, targets in train_dataloader:\n",
    "        print(f\"Tamaño del lote de imágenes: {images.shape}\")\n",
    "        print(f\"Número de imágenes en el lote (lista de targets): {len(targets)}\") \n",
    "        if len(targets[0]) > 0:\n",
    "            print(f\"Ejemplo de target para la primera imagen (clase, cx, cy, w, h normalizados):\")\n",
    "            print(targets[0][0]) \n",
    "        else:\n",
    "            print(\"Primera imagen del lote de entrenamiento sin cajas anotadas (o filtradas).\")\n",
    "        break \n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
