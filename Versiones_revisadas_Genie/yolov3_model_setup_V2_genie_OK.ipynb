{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889c0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberias importadas correctamente\n",
      "Ruta del repositorio YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
      "Ruta de los modelos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Rutas añadidas al PYTHONPATH: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/ y C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Trabajando en el dispositivo: cpu\n",
      "Cargando la arquitectura del modelo desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config/yolov3.cfg (con classes=80)\n",
      "Modelo YOLOv3 cargado correctamente en el dispositivo:  cpu\n",
      "Intentando cargar pesos pre-entrenados desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/yolov3.weights\n",
      "Pesos pre-entrenados cargados con éxito.\n",
      "\n",
      "Adaptando las capas de predicción a 3 clases...\n",
      "Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\n",
      "\n",
      "Configurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[82]\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[94]\n",
      "  Descongelada capa Conv2d antes del YOLOLayer en module_list[106]\n",
      "\n",
      "Verificación de capas que se entrenarán ('requires_grad=True'):\n",
      "module_list.81.0.weight\n",
      "module_list.81.0.bias\n",
      "module_list.93.0.weight\n",
      "module_list.93.0.bias\n",
      "module_list.105.0.weight\n",
      "module_list.105.0.bias\n",
      "\n",
      "Total de parámetros entrenables: 0.04 M\n",
      "Total de parámetros congelados: 61.49 M\n",
      "Total de parámetros en el modelo: 61.53 M\n",
      "\n",
      "Realizando una pasada hacia adelante para verificar la configuración del modelo...\n",
      "\n",
      "Shape de la salida del modelo después de cargar pesos y adaptar a 3 clases (en modo EVAL):\n",
      "  Escala 13x13: torch.Size([1, 507, 8]) (Esperado: [N, 3*13*13, 5+C])\n",
      "  Escala 26x26: torch.Size([1, 2028, 8]) (Esperado: [N, 3*26*26, 5+C])\n",
      "  Escala 52x52: torch.Size([1, 8112, 8]) (Esperado: [N, 3*52*52, 5+C])\n",
      "¡Las dimensiones de salida para 3 clases son correctas en modo EVAL!\n",
      "\n",
      "\n",
      "--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\n"
     ]
    }
   ],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos\n",
    "\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "\n",
    "# Ruta a los pesos preentrenados yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "print(\"Liberias importadas correctamente\")\n",
    "\n",
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer \n",
    "\n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES_YOUR_DATASET = 3\n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = 416 \n",
    "\n",
    "# Definimos los anchor masks para tus 3 clases (placeholder hasta definir los adecuados con KMeans)\n",
    "# YOLOv3 usa 9 anchor boxes en total, divididos en 3 grupos de 3 para cada escala.\n",
    "# Estos son los INDICES de los anchors. Los valores reales los calculaamos con K-Means.\n",
    "# Ejemplo: si los 9 anchors se ordenan de menor a mayor área, los grandes (indices 6,7,8) van a la escala 13x13.\n",
    "#Anchor Boxes Calculadas (Formato para YOLOv3Loss): [[(227, 210), (179, 155), (124, 111)], [(105, 113), (104, 96), (80, 109)], [(112, 75), (87, 82), (39, 38)]]\n",
    "\n",
    "DUMMY_ANCHORS_MASKS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# ... (all your setup code above remains unchanged)\n",
    "\n",
    "# PASO 3: Adapatacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0]\n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels,\n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True\n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer)\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        yolo_layer_index_in_model_yolo_layers += 1\n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# --- NUEVA SECCIÓN: CONGELAR TODAS LAS CAPAS, SOLO DESCONGELAR LAS ULTIMAS Conv2d DE PREDICCIÓN ---\n",
    "print(\"\\nConfigurando capas para Fine-Tuning (solo las capas Conv2d justo antes de cada YOLOLayer serán entrenables):\")\n",
    "\n",
    "# Congelar todos los parámetros por defecto\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Solo las Conv2d antes de YOLOLayer quedan como entrenables\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        pred_conv_sequential_idx = i - 1\n",
    "        conv_seq = model.module_list[pred_conv_sequential_idx]\n",
    "        # Buscar la capa Conv2d dentro del Sequential\n",
    "        for layer in conv_seq:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"  Descongelada capa Conv2d antes del YOLOLayer en module_list[{i}]\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\")\n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\\n\")\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
