{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a774cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberias importadas correctamente\n",
      "Ruta del repositorio YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
      "Ruta de los modelos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Ruta del archivo de configuración YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config/yolov3.cfg\n",
      "Ruta del archivo de pesos YOLOv3: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/yolov3.weights\n",
      "Rutas añadidas al PYTHONPATH: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/ y C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/pytorchyolo\n",
      "Trabajando en el dispositivo: cpu\n",
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00132.jpg. Bboxes iniciales (píxeles): 9\n",
      "DEBUG: Primer bbox pixel: [177, 154, 302, 272, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 9\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.2765625, 0.32083333333333336, 0.471875, 0.5666666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 9\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.2765625, 0.36562500000000003, 0.47187499999999993, 0.5499999999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 9\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.2765625, 0.36562500000000003, 0.47187499999999993, 0.5499999999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 9\n",
      "DEBUG: Primer bbox YOLO: [0, 0.37421874999999993, 0.45781249999999996, 0.19531249999999994, 0.1843749999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00120.jpg. Bboxes iniciales (píxeles): 10\n",
      "DEBUG: Primer bbox pixel: [422, 191, 536, 291, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 10\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.659375, 0.39791666666666664, 0.8375, 0.60625], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 10\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.659375, 0.42343749999999997, 0.8375000000000001, 0.5796874999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 10\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.659375, 0.42343749999999997, 0.8375000000000001, 0.5796874999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 10\n",
      "DEBUG: Primer bbox YOLO: [0, 0.7484375000000001, 0.5015624999999999, 0.1781250000000001, 0.15624999999999994]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00134.jpg. Bboxes iniciales (píxeles): 3\n",
      "DEBUG: Primer bbox pixel: [250, 196, 405, 337, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 3\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.390625, 0.4083333333333333, 0.6328125, 0.7020833333333333], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 3\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.390625, 0.43124999999999997, 0.6328125, 0.6515624999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 3\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.390625, 0.43124999999999997, 0.6328125, 0.6515624999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 3\n",
      "DEBUG: Primer bbox YOLO: [1, 0.51171875, 0.54140625, 0.2421875, 0.22031249999999997]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00107.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [298, 22, 416, 130, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 14\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.465625, 0.04583333333333333, 0.65, 0.2708333333333333], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.46562500000000007, 0.159375, 0.6500000000000001, 0.328125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 14\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.46562500000000007, 0.159375, 0.6500000000000001, 0.328125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.5578125, 0.24375, 0.18437500000000007, 0.16875]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00148.jpg. Bboxes iniciales (píxeles): 6\n",
      "DEBUG: Primer bbox pixel: [404, 253, 511, 356, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 6\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.63125, 0.5270833333333333, 0.7984375, 0.7416666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 6\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.6312499999999999, 0.5203125000000001, 0.7984375000000001, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 6\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.6312499999999999, 0.5203125000000001, 0.7984375000000001, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 6\n",
      "DEBUG: Primer bbox YOLO: [0, 0.71484375, 0.60078125, 0.16718750000000027, 0.16093749999999984]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00253.jpg. Bboxes iniciales (píxeles): 10\n",
      "DEBUG: Primer bbox pixel: [47, 222, 134, 312, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 10\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.0734375, 0.4625, 0.209375, 0.65], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 10\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.0734375, 0.47187500000000004, 0.20937500000000003, 0.6125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 10\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.0734375, 0.47187500000000004, 0.20937500000000003, 0.6125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 10\n",
      "DEBUG: Primer bbox YOLO: [0, 0.14140625, 0.5421875, 0.13593750000000004, 0.140625]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00230.jpg. Bboxes iniciales (píxeles): 11\n",
      "DEBUG: Primer bbox pixel: [154, 246, 270, 356, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 11\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.240625, 0.5125, 0.421875, 0.7416666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 11\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.24062500000000003, 0.5093749999999999, 0.421875, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 11\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.24062500000000003, 0.5093749999999999, 0.421875, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 11\n",
      "DEBUG: Primer bbox YOLO: [0, 0.33125000000000004, 0.5953124999999999, 0.18124999999999997, 0.171875]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00078.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [400, 247, 504, 345, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 14\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.625, 0.5145833333333333, 0.7875, 0.71875], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.625, 0.5109374999999999, 0.7874999999999999, 0.6640625)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 14\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.625, 0.5109374999999999, 0.7874999999999999, 0.6640625)\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.7062499999999999, 0.5874999999999999, 0.16249999999999987, 0.15312500000000007]\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 9 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([0.0000, 0.3742, 0.4578, 0.1953, 0.1844])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n",
      "Usando dispositivo: cpu\n",
      "\n",
      "--- Iniciando el Bucle de Entrenamiento Principal ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (Train):   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00356.jpg. Bboxes iniciales (píxeles): 17\n",
      "DEBUG: Primer bbox pixel: [279, 310, 396, 422, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 17\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.4359375, 0.6458333333333334, 0.61875, 0.8791666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 17\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.4359375, 0.609375, 0.6187500000000001, 0.784375)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 17\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.4359375, 0.609375, 0.6187500000000001, 0.784375)\n",
      "DEBUG: Bboxes finales en formato YOLO: 17\n",
      "DEBUG: Primer bbox YOLO: [0, 0.52734375, 0.696875, 0.18281250000000016, 0.17500000000000004]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00313.jpg. Bboxes iniciales (píxeles): 12\n",
      "DEBUG: Primer bbox pixel: [24, 94, 131, 202, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 12\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.0375, 0.19583333333333333, 0.2046875, 0.42083333333333334], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 12\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.0375, 0.271875, 0.20468749999999997, 0.44062500000000004)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 12\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.0375, 0.271875, 0.20468749999999997, 0.44062500000000004)\n",
      "DEBUG: Bboxes finales en formato YOLO: 12\n",
      "DEBUG: Primer bbox YOLO: [0, 0.12109374999999999, 0.35625, 0.16718749999999996, 0.16875000000000007]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00212.jpg. Bboxes iniciales (píxeles): 15\n",
      "DEBUG: Primer bbox pixel: [226, 141, 338, 240, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 15\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.353125, 0.29375, 0.528125, 0.5], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 15\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.353125, 0.3453125, 0.528125, 0.5)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 15\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.353125, 0.3453125, 0.528125, 0.5)\n",
      "DEBUG: Bboxes finales en formato YOLO: 15\n",
      "DEBUG: Primer bbox YOLO: [0, 0.440625, 0.42265625, 0.17499999999999993, 0.15468749999999998]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00100.jpg. Bboxes iniciales (píxeles): 17\n",
      "DEBUG: Primer bbox pixel: [229, 238, 342, 330, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 17\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.3578125, 0.49583333333333335, 0.534375, 0.6875], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 17\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.3578125, 0.49687500000000007, 0.534375, 0.640625)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 17\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.3578125, 0.49687500000000007, 0.534375, 0.640625)\n",
      "DEBUG: Bboxes finales en formato YOLO: 17\n",
      "DEBUG: Primer bbox YOLO: [0, 0.44609375, 0.5687500000000001, 0.17656250000000007, 0.14374999999999993]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00098.jpg. Bboxes iniciales (píxeles): 15\n",
      "DEBUG: Primer bbox pixel: [371, 278, 481, 380, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 15\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.5796875, 0.5791666666666667, 0.7515625, 0.7916666666666666], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 15\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.5796875, 0.5593750000000001, 0.7515625000000001, 0.71875)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 15\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.5796875, 0.5593750000000001, 0.7515625000000001, 0.71875)\n",
      "DEBUG: Bboxes finales en formato YOLO: 15\n",
      "DEBUG: Primer bbox YOLO: [0, 0.6656250000000001, 0.6390625000000001, 0.1718750000000001, 0.15937499999999993]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00379.jpg. Bboxes iniciales (píxeles): 19\n",
      "DEBUG: Primer bbox pixel: [437, 29, 531, 114, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 19\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.6828125, 0.06041666666666667, 0.8296875, 0.2375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 19\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.6828125, 0.17031249999999998, 0.8296875000000001, 0.303125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 19\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.6828125, 0.17031249999999998, 0.8296875000000001, 0.303125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 19\n",
      "DEBUG: Primer bbox YOLO: [0, 0.7562500000000001, 0.23671874999999998, 0.1468750000000001, 0.1328125]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00263.jpg. Bboxes iniciales (píxeles): 15\n",
      "DEBUG: Primer bbox pixel: [115, 358, 206, 446, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 15\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.1796875, 0.7458333333333333, 0.321875, 0.9291666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 15\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.1796875, 0.6843750000000001, 0.321875, 0.8218750000000001)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 15\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.1796875, 0.6843750000000001, 0.321875, 0.8218750000000001)\n",
      "DEBUG: Bboxes finales en formato YOLO: 15\n",
      "DEBUG: Primer bbox YOLO: [0, 0.25078125, 0.753125, 0.14218750000000002, 0.13750000000000007]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00262.jpg. Bboxes iniciales (píxeles): 10\n",
      "DEBUG: Primer bbox pixel: [524, 279, 640, 385, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 10\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.81875, 0.58125, 1.0, 0.8020833333333334], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 10\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.8187499999999999, 0.5609375000000001, 1.0, 0.7265625)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 10\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.8187499999999999, 0.5609375000000001, 1.0, 0.7265625)\n",
      "DEBUG: Bboxes finales en formato YOLO: 10\n",
      "DEBUG: Primer bbox YOLO: [0, 0.9093749999999999, 0.64375, 0.18125000000000013, 0.1656249999999999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 674\u001b[39m\n\u001b[32m    672\u001b[39m images = images.to(device)\n\u001b[32m    673\u001b[39m \u001b[38;5;66;03m# targets ahora es un tensor, así que también lo movemos al dispositivo\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m targets = \u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device) \u001b[38;5;66;03m# <--- CORRECCIÓN: Mover targets al dispositivo\u001b[39;00m\n\u001b[32m    676\u001b[39m outputs = model(images)\n\u001b[32m    677\u001b[39m loss, _ = loss_fn(outputs, targets) \n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Modelo YOLO Version 3 para detección de objetos\n",
    "# Ruta al repositorio \n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/\n",
    "# Ruta al fichero de configuracion yolov3.cfg\n",
    "#C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/config\n",
    "# Ruta a los pesos prenetrandos yolov3.weights\n",
    "# C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/weights/\n",
    "\n",
    "# Importamos librerias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "\n",
    "import cv2\n",
    "print(\"Liberias importadas correctamente\")\n",
    "\n",
    "# Configuración de rutas\n",
    "# Ruta donde hemos clonado el repositorio de Erik Lindernoren.\n",
    "YOLOV3_REPO_PATH = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/'\n",
    "YOLOV3_MODELS_PATH = os.path.join(YOLOV3_REPO_PATH, 'pytorchyolo')\n",
    "print(f\"Ruta del repositorio YOLOv3: {YOLOV3_REPO_PATH}\")\n",
    "print(f\"Ruta de los modelos YOLOv3: {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Rutas de Archivos Específicos\n",
    "# Archivo de configuracion yolov3.cfg\n",
    "CONFIG_PATH = os.path.join(YOLOV3_REPO_PATH, 'config', 'yolov3.cfg')\n",
    "CONFIG_PATH = CONFIG_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de configuración YOLOv3: {CONFIG_PATH}\")\n",
    "\n",
    "# Archivo de pesos .weights descargado de https://github.com/patrick013/Object-Detection---Yolov3.git\n",
    "WEIGHTS_PATH = os.path.join(YOLOV3_REPO_PATH, 'yolov3.weights')\n",
    "WEIGHTS_PATH = WEIGHTS_PATH.replace('\\\\', '/')  # Asegúrate de usar barras normales para evitar problemas en Linux/Mac\n",
    "print(f\"Ruta del archivo de pesos YOLOv3: {WEIGHTS_PATH}\")\n",
    "\n",
    "# Añadimos esta ruta al PYTHONPATH para que Python pueda encontrar los módulos.\n",
    "sys.path.append(YOLOV3_REPO_PATH)\n",
    "sys.path.append(YOLOV3_MODELS_PATH)\n",
    "print(f\"Rutas añadidas al PYTHONPATH: {YOLOV3_REPO_PATH} y {YOLOV3_MODELS_PATH}\")\n",
    "\n",
    "# Importamos las clases necesarias del repositorio.\n",
    "# Darknet y YOLOLayer son las clases principales del modelo.\n",
    "from models import Darknet, YOLOLayer \n",
    "\n",
    "# Detección del Dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando en el dispositivo: {device}\")\n",
    "\n",
    "# DEFINICIONES DE CLASES Y FUNCIONES\n",
    "\n",
    "# Definicion de la función intersection_over_union\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calcula la Intersection Over Union (IoU) entre bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes_preds (tensor): Bounding boxes predichas de forma (N, 4) o (batch_size, 4)\n",
    "                            donde N es el número de cajas o 1 si es una sola caja.\n",
    "                            Formato de las cajas: (x, y, w, h) o (x1, y1, x2, y2).\n",
    "        boxes_labels (tensor): Bounding boxes ground truth de forma (N, 4) o (batch_size, 4).\n",
    "        box_format (str): Formato de las cajas de entrada.\n",
    "                        \"midpoint\" si es (x_center, y_center, width, height)\n",
    "                        \"corners\" si es (x1, y1, x2, y2).\n",
    "\n",
    "    Returns:\n",
    "        tensor: IoU para cada par de cajas, de forma (N, 1) o (batch_size, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        # Convertir de (x_center, y_center, width, height) a (x1, y1, x2, y2)\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        \n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        # Asumir que ya están en (x1, y1, x2, y2)\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        \n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    else:\n",
    "        raise ValueError(\"box_format debe ser 'midpoint' o 'corners'\")\n",
    "\n",
    "    # Calcular las coordenadas del rectángulo de intersección\n",
    "    x1_inter = torch.max(box1_x1, box2_x1)\n",
    "    y1_inter = torch.max(box1_y1, box2_y1)\n",
    "    x2_inter = torch.min(box1_x2, box2_x2)\n",
    "    y2_inter = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Calcular el área de intersección\n",
    "    intersection = (x2_inter - x1_inter).clamp(0) * \\\n",
    "                (y2_inter - y1_inter).clamp(0)\n",
    "\n",
    "    # Calcular el área de cada bounding box\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    # Calcular el IoU\n",
    "    union = box1_area + box2_area - intersection + 1e-6 # Añadir epsilon para evitar división por cero\n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Definición de la clase YOLOv3Loss\n",
    "class YOLOv3Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size=(416, 416), \n",
    "                lambda_coord=1.0, lambda_noobj=1.0, lambda_obj=1.0, lambda_class=1.0, \n",
    "                ignore_iou_threshold=0.5): # Umbral para ignorar anchors en noobj loss\n",
    "        super().__init__()\n",
    "        self.anchors = anchors \n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.lambda_coord = lambda_coord \n",
    "        self.lambda_noobj = lambda_noobj \n",
    "        self.lambda_obj = lambda_obj     \n",
    "        self.lambda_class = lambda_class \n",
    "        self.ignore_iou_threshold = ignore_iou_threshold \n",
    "\n",
    "        self.mse = nn.MSELoss() \n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0])) \n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        obj_loss = 0\n",
    "        noobj_loss = 0\n",
    "        box_loss = 0\n",
    "        class_loss = 0 \n",
    "\n",
    "        for scale_idx, prediction in enumerate(predictions):\n",
    "            prediction = prediction.permute(0, 2, 3, 1).reshape(\n",
    "                prediction.shape[0], prediction.shape[2], prediction.shape[3], 3, self.num_classes + 5\n",
    "            )\n",
    "            \n",
    "            pred_x_y = prediction[..., 0:2] \n",
    "            pred_w_h = prediction[..., 2:4]                 \n",
    "            pred_obj = prediction[..., 4:5]               \n",
    "            pred_class = prediction[..., 5:]               \n",
    "\n",
    "            N, grid_h, grid_w, num_anchors, _ = prediction.shape\n",
    "            \n",
    "            anchors_current_scale = torch.tensor(self.anchors[scale_idx], device=targets.device).reshape(1, 1, 1, num_anchors, 2)\n",
    "            \n",
    "            target_obj_mask = torch.zeros((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=targets.device)\n",
    "            target_noobj_mask = torch.ones((N, grid_h, grid_w, num_anchors), dtype=torch.float32, device=targets.device)\n",
    "            \n",
    "            tx = torch.zeros((N, grid_h, grid_w, num_anchors), device=targets.device)\n",
    "            ty = torch.zeros((N, grid_h, grid_w, num_anchors), device=targets.device)\n",
    "            tw = torch.zeros((N, grid_h, grid_w, num_anchors), device=targets.device) \n",
    "            th = torch.zeros((N, grid_h, grid_w, num_anchors), device=targets.device) \n",
    "            \n",
    "            target_class_one_hot = torch.zeros((N, grid_h, grid_w, num_anchors, self.num_classes), dtype=torch.float32, device=targets.device) \n",
    "\n",
    "            for box_idx in range(targets.shape[0]):\n",
    "                img_id, class_id, x_gt_norm, y_gt_norm, w_gt_norm, h_gt_norm = targets[box_idx].tolist()\n",
    "                img_id = int(img_id) \n",
    "\n",
    "                x_center_grid = x_gt_norm * grid_w\n",
    "                y_center_grid = y_gt_norm * grid_h\n",
    "                \n",
    "                cell_x = int(x_center_grid)\n",
    "                cell_y = int(y_center_grid)\n",
    "\n",
    "                if cell_x >= grid_w or cell_y >= grid_h or cell_x < 0 or cell_y < 0:\n",
    "                    continue\n",
    "                \n",
    "                w_gt_abs_pixels = w_gt_norm * self.img_size[0]\n",
    "                h_gt_abs_pixels = h_gt_norm * self.img_size[1]\n",
    "                \n",
    "                gt_box_dims = torch.tensor([0, 0, w_gt_abs_pixels, h_gt_abs_pixels], device=targets.device)\n",
    "\n",
    "                anchor_boxes_for_iou = torch.zeros((num_anchors, 4), device=targets.device)\n",
    "                anchor_boxes_for_iou[:, 2] = anchors_current_scale[0,0,0,:,0] \n",
    "                anchor_boxes_for_iou[:, 3] = anchors_current_scale[0,0,0,:,1] \n",
    "                \n",
    "                ious = intersection_over_union(\n",
    "                    gt_box_dims.unsqueeze(0), \n",
    "                    anchor_boxes_for_iou,     \n",
    "                    box_format=\"corners\"      \n",
    "                ) \n",
    "                \n",
    "                best_iou_anchor_idx = torch.argmax(ious).item() \n",
    "                \n",
    "                target_obj_mask[img_id, cell_y, cell_x, best_iou_anchor_idx] = 1.0 \n",
    "                target_noobj_mask[img_id, cell_y, cell_x, best_iou_anchor_idx] = 0.0 \n",
    "                \n",
    "                tx[img_id, cell_y, cell_x, best_iou_anchor_idx] = x_center_grid - cell_x\n",
    "                ty[img_id, cell_y, cell_x, best_iou_anchor_idx] = y_center_grid - cell_y\n",
    "                \n",
    "                tw[img_id, cell_y, cell_x, best_iou_anchor_idx] = torch.log(w_gt_abs_pixels / anchors_current_scale[0,0,0,best_iou_anchor_idx,0] + 1e-16) \n",
    "                th[img_id, cell_y, cell_x, best_iou_anchor_idx] = torch.log(h_gt_abs_pixels / anchors_current_scale[0,0,0,best_iou_anchor_idx,1] + 1e-16) \n",
    "                \n",
    "                target_class_one_hot[img_id, cell_y, cell_x, best_iou_anchor_idx, int(class_id)] = 1.0 \n",
    "\n",
    "                for anchor_idx_other, iou_val in enumerate(ious[0]): \n",
    "                    if anchor_idx_other == best_iou_anchor_idx:\n",
    "                        continue \n",
    "                    \n",
    "                    if iou_val > self.ignore_iou_threshold:\n",
    "                        target_noobj_mask[img_id, cell_y, cell_x, anchor_idx_other] = 0.0 \n",
    "            \n",
    "            loss_x = self.bce(pred_x_y[..., 0][target_obj_mask.bool()], tx[target_obj_mask.bool()])\n",
    "            loss_y = self.bce(pred_x_y[..., 1][target_obj_mask.bool()], ty[target_obj_mask.bool()])\n",
    "\n",
    "            loss_w = self.mse(pred_w_h[..., 0][target_obj_mask.bool()], tw[target_obj_mask.bool()]) \n",
    "            loss_h = self.mse(pred_w_h[..., 1][target_obj_mask.bool()], th[target_obj_mask.bool()]) \n",
    "            \n",
    "            box_loss += (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "            loss_obj = self.bce(pred_obj[target_obj_mask.bool()], target_obj_mask[target_obj_mask.bool()].float().unsqueeze(-1))\n",
    "            loss_noobj = self.bce(pred_obj[target_noobj_mask.bool()], target_noobj_mask[target_noobj_mask.bool()].float().unsqueeze(-1))\n",
    "            \n",
    "            obj_loss += loss_obj\n",
    "            noobj_loss += loss_noobj\n",
    "\n",
    "            loss_class = self.bce(pred_class[target_obj_mask.bool()], target_class_one_hot[target_obj_mask.bool()])\n",
    "            class_loss += loss_class\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + self.lambda_obj * obj_loss\n",
    "            + self.lambda_noobj * noobj_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "        return total_loss, {\"box_loss\": box_loss, \"obj_loss\": obj_loss, \"noobj_loss\": noobj_loss, \"class_loss\": class_loss}\n",
    "\n",
    "# Definición de la clase BloodCellDataset\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "        # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "        bboxes_normalized_initial = []\n",
    "        class_labels = [] # class_labels se mantiene\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "        if bboxes_normalized_initial:\n",
    "            print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "            # Se espera que devuelva coordenadas normalizadas también.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "        # Si Albumentations funciona como se espera con format='albumentations',\n",
    "        # este paso ya no es necesario.\n",
    "        bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "        # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# INICIO DE LA CONFIGURACIÓN DEL MODELO YOLOv3\n",
    "\n",
    "# PASO 1: Instanciar el Modelo YOLOv3 (para 80 clases, usando el .cfg original)\n",
    "# La clase Darknet de Erik Lindernoren construye el modelo leyendo el archivo yolov3.cfg.\n",
    "# Esto crea el modelo con la arquitectura esperada por el archivo yolov3.weights.\n",
    "print(f\"Cargando la arquitectura del modelo desde: {CONFIG_PATH} (con classes=80)\")\n",
    "model = Darknet(CONFIG_PATH)\n",
    "model.to(device) # Mueve el modelo al dispositivo (GPU/CPU)\n",
    "print(\"Modelo YOLOv3 cargado correctamente en el dispositivo: \", device)\n",
    "\n",
    "# PASO 2: Cargamos los Pesos Pre-entrenados\n",
    "# El método load_darknet_weights() es el encargado de leer el archivo yolov3.weights.\n",
    "try:\n",
    "    print(f\"Intentando cargar pesos pre-entrenados desde: {WEIGHTS_PATH}\")\n",
    "    model.load_darknet_weights(WEIGHTS_PATH)\n",
    "    print(\"Pesos pre-entrenados cargados con éxito.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo de pesos en {WEIGHTS_PATH}.\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al cargar los pesos pre-entrenados: {e}\")\n",
    "    print(\"El modelo se inicializará con pesos aleatorios (NO se usará transfer learning).\")\n",
    "    print(\"¡ADVERTENCIA! Entrenar desde cero con solo 300 imágenes será extremadamente difícil.\")\n",
    "\n",
    "# PASO 3: Adapatacion del modelo para las 3 clases (FINE-TUNING EN MEMORIA)\n",
    "# Esto debemos hacerlo DESPUÉS de haber cargado los pesos del modelo de 80 clases.\n",
    "\n",
    "print(\"\\nAdaptando las capas de predicción a 3 clases...\")\n",
    "\n",
    "yolo_layer_index_in_model_yolo_layers = 0 # Para asignar los nuevos YOLOLayer a la lista correcta\n",
    "\n",
    "for i, module_def in enumerate(model.module_defs):\n",
    "    if module_def[\"type\"] == \"yolo\":\n",
    "        # i es el índice de la capa YOLOLayer en model.module_defs y module_list\n",
    "        \n",
    "        # 1. Reemplazar la capa Conv2d de predicción final\n",
    "        pred_conv_sequential_idx = i - 1 \n",
    "        pred_conv_layer_old = model.module_list[pred_conv_sequential_idx][0] \n",
    "        \n",
    "        yolo_layer_old_instance = model.yolo_layers[yolo_layer_index_in_model_yolo_layers]\n",
    "        \n",
    "        new_out_channels = len(yolo_layer_old_instance.anchors) * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "        \n",
    "        new_pred_conv_layer = nn.Conv2d(pred_conv_layer_old.in_channels, new_out_channels, \n",
    "                                        kernel_size=pred_conv_layer_old.kernel_size,\n",
    "                                        stride=pred_conv_layer_old.stride,\n",
    "                                        padding=pred_conv_layer_old.padding,\n",
    "                                        bias=True \n",
    "                                        )\n",
    "        model.module_list[pred_conv_sequential_idx] = nn.Sequential(new_pred_conv_layer)\n",
    "        \n",
    "        # 2. Reemplazar la instancia de YOLOLayer en model.module_list y model.yolo_layers\n",
    "        # ¡CORREGIDO! La YOLOLayer está en module_list[i]\n",
    "        \n",
    "        # Obtenemos los anchors y stride de la instancia antigua para la nueva YOLOLayer\n",
    "        # Esto es correcto ya que estos atributos sí existen en yolo_layer_old_instance\n",
    "        anchors_for_new_layer = yolo_layer_old_instance.anchors.tolist()\n",
    "        stride_for_new_layer = yolo_layer_old_instance.stride\n",
    "        \n",
    "        # Creamos una NUEVA instancia de YOLOLayer\n",
    "        new_yolo_layer = YOLOLayer(anchors_for_new_layer, NUM_CLASSES_YOUR_DATASET, new_coords=False)\n",
    "        \n",
    "        # Sustituimos la YOLOLayer antigua en el `module_list` del modelo\n",
    "        # Esto es crucial porque el forward de Darknet itera sobre module_list\n",
    "        model.module_list[i] = nn.Sequential(new_yolo_layer) # Reemplaza el Sequential que contiene la YOLOLayer antigua\n",
    "        \n",
    "        # También actualizamos la referencia en `model.yolo_layers`\n",
    "        model.yolo_layers[yolo_layer_index_in_model_yolo_layers] = new_yolo_layer\n",
    "        \n",
    "        yolo_layer_index_in_model_yolo_layers += 1 \n",
    "\n",
    "print(\"Capas YOLOLayer y sus capas de predicción Conv2d adaptadas para 3 clases.\")\n",
    "\n",
    "# PASO 4: Congelamos las capas para Fine-Tuning\n",
    "# Es CRUCIAL congelar la mayoría de las capas del backbone (Darknet-53)\n",
    "# y dejar entrenables las capas del head (las que predicen las cajas).\n",
    "# Esto evita que el modelo \"olvide\" lo que aprendió en el dataset grande.\n",
    "\n",
    "print(\"\\nConfigurando capas para Fine-Tuning:\")\n",
    "# Iteramos a través de los módulos y parámetros del modelo.\n",
    "# Las primeras ~74-75 capas de su `module_list` corresponden al backbone (Darknet-53 puro).\n",
    "# Las capas posteriores (más de 75) son parte del head de YOLOv3 y deben ser entrenables.\n",
    "# Las capas YOLOLayer en sí no tienen parámetros entrenables, pero sus capas `conv` previas sí.\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if i < 75:  # Índices de las capas del backbone (heurístico, basado en estructura Darknet-53)\n",
    "        param.requires_grad = False  # Congelar\n",
    "    else:\n",
    "        param.requires_grad = True   # Descongelar (para el head)\n",
    "    \n",
    "    # Línea para depuración: puedes descomentarla para ver el estado de cada capa\n",
    "    # print(f\"  Capa: {name}, Entrenable: {param.requires_grad}\")\n",
    "\n",
    "# --- Verificación de Capas Entrenables ---\n",
    "print(\"\\nVerificación de capas que se entrenarán ('requires_grad=True'):\")\n",
    "trainable_params_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal de parámetros entrenables: {trainable_params_count / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros congelados: {(total_params - trainable_params_count) / 1e6:.2f} M\")\n",
    "print(f\"Total de parámetros en el modelo: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# PASO 5: Prueba Final de la Pasada hacia Adelante (sanity check)\n",
    "# En modo eval(), YOLOLayer devuelve predicciones decodificadas y aplanadas.\n",
    "print(\"\\nRealizando una pasada hacia adelante para verificar la configuración del modelo...\")\n",
    "# Modo EVAL: No se entrena, solo se evalúa la salida del modelo.\n",
    "\n",
    "IMG_SIZE = 416  # Tamaño de imagen esperado por el modelo (debe coincidir con el tamaño de entrada del modelo)\n",
    "NUM_CLASSES_YOUR_DATASET = 3  # Número de clases en tu dataset (BCCD: RBC, WBC, Plaquetas)\n",
    "    \n",
    "model.eval() \n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device) \n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nShape de la salida del modelo después de cargar pesos y adaptar a {NUM_CLASSES_YOUR_DATASET} clases (en modo EVAL):\")\n",
    "# Ajustamos las expectativas de forma para reflejar la salida aplanada y decodificada\n",
    "print(f\"  Escala 13x13: {predictions[0].shape} (Esperado: [N, 3*13*13, 5+C])\") \n",
    "print(f\"  Escala 26x26: {predictions[1].shape} (Esperado: [N, 3*26*26, 5+C])\")\n",
    "print(f\"  Escala 52x52: {predictions[2].shape} (Esperado: [N, 3*52*52, 5+C])\")\n",
    "\n",
    "# No necesitamos los asserts basados en el formato crudo aquí, ya que el formato de evaluación es diferente\n",
    "# Los asserts que teníamos antes son para el formato crudo (en modo train)\n",
    "# expected_output_channels = 3 * (5 + NUM_CLASSES_YOUR_DATASET)\n",
    "# assert predictions[0].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "# assert predictions[1].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "# assert predictions[2].shape[1] == expected_output_channels # Esto no es verdad en modo eval()\n",
    "print(f\"¡Las dimensiones de salida para {NUM_CLASSES_YOUR_DATASET} clases son correctas en modo EVAL!\")\n",
    "\n",
    "print(\"\\n--- ¡Fase de Configuración del Modelo YOLOv3 Completada Exitosamente! ---\")\n",
    "\n",
    "# INICIO DE LA CONFIGURACIÓN DEL ENTRENAMIENTO\n",
    "# Preparando el entrenamiento de YOLOv3\n",
    "\n",
    "# Hiperparámetros de training\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50 \n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRADIENT_CLIP_VALUE = 0.1\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Parámetros Generales\n",
    "# Número de clases del dataset BCCD (Glóbulos Rojos, Glóbulos Blancos, Plaquetas).\n",
    "NUM_CLASSES = 3 \n",
    "# Tamaño de la imagen de entrada para el modelo YOLOv3 (típicamente 416x416 o 608x608).\n",
    "IMG_SIZE = (416, 416)\n",
    "\n",
    "# Anchor Boxes de YOLOv3 calculadas con K-means para el dataset de BCCD\n",
    "ANCHORS = [\n",
    "    [(227, 210), (179, 155), (124, 111)],  # Anchors para la escala más grande ... Escala 0 (grid 13x13) (stride 32, detecta objetos grandes)\n",
    "    [(105, 113), (104, 96), (80, 109)],    # Anchors para la escala media ... Escala 1 (grid 26X26) (stride 16, detecta objetos medianos)\n",
    "    [(112, 75), (87, 82), (39, 38)]        # Anchors para la escala más pequeña ... Escala 2 (grid 52x52) (stride 8, detecta objetos pequeños)\n",
    "]\n",
    "\n",
    "# Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# Las transformaciones para el set de training incluyen aumentacion de COLOR/APARIENCIA\n",
    "train_transforms = A.Compose([\n",
    "    # Redimensionamiento y Relleno\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    \n",
    "    # Transformaciones de Color y Apariencia\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1), # Asegúrate de que blur_limit es impar y no demasiado grande\n",
    "    \n",
    "    # Normalización y Conversión a Tensor\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # El formato 'albumentations' espera y devuelve normalizado [0,1]\n",
    "\n",
    "# Las transformaciones de validación/prueba se mantienen minimalistas\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) \n",
    "\n",
    "# Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")\n",
    "\n",
    "# Bucle de Entrenamiento Principal\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Listas para almacenar el historial de pérdidas\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    model = DummyYOLOv3Model(num_classes=NUM_CLASSES).to(device)\n",
    "    \n",
    "    loss_fn = YOLOv3Loss(\n",
    "        anchors=ANCHORS, \n",
    "        num_classes=NUM_CLASSES,\n",
    "        img_size=IMG_SIZE,\n",
    "        lambda_coord=1.0, lambda_noobj=1.0, lambda_obj=1.0, lambda_class=1.0, \n",
    "        ignore_iou_threshold=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    print(\"\\n--- Iniciando el Bucle de Entrenamiento Principal ---\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() # Modo entrenamiento\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        loop = tqdm(train_dataloader, leave=True, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (Train)\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(loop):\n",
    "            images = images.to(device)\n",
    "            # targets ahora es un tensor, así que también lo movemos al dispositivo\n",
    "            targets = targets.to(device) # <--- CORRECCIÓN: Mover targets al dispositivo\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss, _ = loss_fn(outputs, targets) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE) \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            loop.set_postfix(loss=total_train_loss/(batch_idx+1)) \n",
    "            \n",
    "        scheduler.step() \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss) \n",
    "        \n",
    "        print(f\"Época {epoch+1} - Tasa de Aprendizaje: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"Época {epoch+1} - Pérdida de Entrenamiento Promedio: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Fase de Validación ---\n",
    "        model.eval() # Modo evaluación\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad(): \n",
    "            for batch_idx_val, (images_val, targets_val) in enumerate(val_dataloader):\n",
    "                images_val = images_val.to(device)\n",
    "                # targets_val también debe moverse al dispositivo\n",
    "                targets_val = targets_val.to(device) # <--- CORRECCIÓN: Mover targets_val al dispositivo\n",
    "                \n",
    "                outputs_val = model(images_val)\n",
    "                val_loss, _ = loss_fn(outputs_val, targets_val)\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss) \n",
    "        \n",
    "        print(f\"Época {epoch+1} - Pérdida de Validación Promedio: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Entrenamiento Finalizado ---\")\n",
    "    print(\"El modelo ha completado el entrenamiento simulado.\")\n",
    "    print(\"Recuerda: Los valores de pérdida son para un modelo DUMMY con predicciones aleatorias.\")\n",
    "    print(\"Para un entrenamiento real, reemplaza el DummyYOLOv3Model con tu modelo real.\")\n",
    "\n",
    "    print(\"\\nHistorial de Pérdidas de Entrenamiento por Época:\", train_losses)\n",
    "    print(\"Historial de Pérdidas de Validación por Época:\", val_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
