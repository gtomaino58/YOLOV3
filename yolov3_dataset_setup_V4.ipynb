{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f6c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta version es la que funciona mejor ya que ademas de preparar el dataset visualiza el resultado OJO OJO OJO\n",
    "# Incluye la version definitiva incluyendo la funcion de visualizacion\n",
    "# Ademas aplica un nivel razonable y seguro de DA ... hemos excluido las transformaciones Geométricas por considerar\n",
    "# que pueden tener efecto distorsionante sobre el dataset (rotaciones, escalados, etc.)\n",
    "# Se ha añadido la posibilidad de visualizar el dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c554a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00201.jpg. Bboxes iniciales (píxeles): 12\n",
      "DEBUG: Primer bbox pixel: [302, 9, 406, 100, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 12\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.471875, 0.01875, 0.634375, 0.20833333333333334], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 12\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.47187499999999993, 0.1390625, 0.6343750000000001, 0.28125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 12\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.47187499999999993, 0.1390625, 0.6343750000000001, 0.28125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 12\n",
      "DEBUG: Primer bbox YOLO: [0, 0.5531250000000001, 0.21015625, 0.1625000000000002, 0.1421875]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00106.jpg. Bboxes iniciales (píxeles): 17\n",
      "DEBUG: Primer bbox pixel: [82, 1, 214, 79, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 17\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.128125, 0.0020833333333333333, 0.334375, 0.16458333333333333], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 17\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.128125, 0.1265625, 0.334375, 0.24843749999999998)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 17\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.128125, 0.1265625, 0.334375, 0.24843749999999998)\n",
      "DEBUG: Bboxes finales en formato YOLO: 17\n",
      "DEBUG: Primer bbox YOLO: [0, 0.23124999999999998, 0.1875, 0.20625, 0.12187499999999998]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00262.jpg. Bboxes iniciales (píxeles): 10\n",
      "DEBUG: Primer bbox pixel: [524, 279, 640, 385, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 10\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.81875, 0.58125, 1.0, 0.8020833333333334], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 10\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.8187499999999999, 0.5609375000000001, 1.0, 0.7265625)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 10\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.8187499999999999, 0.5609375000000001, 1.0, 0.7265625)\n",
      "DEBUG: Bboxes finales en formato YOLO: 10\n",
      "DEBUG: Primer bbox YOLO: [0, 0.9093749999999999, 0.64375, 0.18125000000000013, 0.1656249999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00249.jpg. Bboxes iniciales (píxeles): 3\n",
      "DEBUG: Primer bbox pixel: [70, 329, 277, 480, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 3\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.109375, 0.6854166666666667, 0.4328125, 1.0], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 3\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.109375, 0.6390625000000001, 0.43281249999999993, 0.875)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 3\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.109375, 0.6390625000000001, 0.43281249999999993, 0.875)\n",
      "DEBUG: Bboxes finales en formato YOLO: 3\n",
      "DEBUG: Primer bbox YOLO: [1, 0.27109374999999997, 0.75703125, 0.32343749999999993, 0.2359374999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00379.jpg. Bboxes iniciales (píxeles): 19\n",
      "DEBUG: Primer bbox pixel: [437, 29, 531, 114, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 19\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.6828125, 0.06041666666666667, 0.8296875, 0.2375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 19\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.6828125, 0.17031249999999998, 0.8296875000000001, 0.303125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 19\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.6828125, 0.17031249999999998, 0.8296875000000001, 0.303125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 19\n",
      "DEBUG: Primer bbox YOLO: [0, 0.7562500000000001, 0.23671874999999998, 0.1468750000000001, 0.1328125]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00043.jpg. Bboxes iniciales (píxeles): 16\n",
      "DEBUG: Primer bbox pixel: [76, 106, 217, 228, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 16\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.11875, 0.22083333333333333, 0.3390625, 0.475], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 16\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.11875, 0.29062499999999997, 0.33906249999999993, 0.48124999999999996)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 16\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.11875, 0.29062499999999997, 0.33906249999999993, 0.48124999999999996)\n",
      "DEBUG: Bboxes finales en formato YOLO: 16\n",
      "DEBUG: Primer bbox YOLO: [1, 0.22890624999999998, 0.38593749999999993, 0.22031249999999994, 0.190625]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00091.jpg. Bboxes iniciales (píxeles): 10\n",
      "DEBUG: Primer bbox pixel: [185, 138, 294, 236, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 10\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.2890625, 0.2875, 0.459375, 0.49166666666666664], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 10\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.2890625, 0.34062499999999996, 0.459375, 0.49375)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 10\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.2890625, 0.34062499999999996, 0.459375, 0.49375)\n",
      "DEBUG: Bboxes finales en formato YOLO: 10\n",
      "DEBUG: Primer bbox YOLO: [0, 0.37421875, 0.4171875, 0.17031249999999998, 0.15312500000000007]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00057.jpg. Bboxes iniciales (píxeles): 11\n",
      "DEBUG: Primer bbox pixel: [488, 252, 596, 357, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 11\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.7625, 0.525, 0.93125, 0.74375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 11\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.7625, 0.51875, 0.9312500000000001, 0.6828125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 11\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.7625, 0.51875, 0.9312500000000001, 0.6828125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 11\n",
      "DEBUG: Primer bbox YOLO: [0, 0.846875, 0.60078125, 0.16875000000000018, 0.1640625]\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 12 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([0.0000, 0.5531, 0.2102, 0.1625, 0.1422])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n"
     ]
    }
   ],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "        # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "        bboxes_normalized_initial = []\n",
    "        class_labels = [] # class_labels se mantiene\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "        if bboxes_normalized_initial:\n",
    "            print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "            # Se espera que devuelva coordenadas normalizadas también.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "        # Si Albumentations funciona como se espera con format='albumentations',\n",
    "        # este paso ya no es necesario.\n",
    "        bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "        # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO CON AUMENTACIÓN DE COLOR/APARIENCIA ---\n",
    "train_transforms = A.Compose([\n",
    "    # Redimensionamiento y Relleno\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    \n",
    "    # Transformaciones de Color y Apariencia\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1), # Asegúrate de que blur_limit es impar y no demasiado grande\n",
    "    \n",
    "    # Normalización y Conversión a Tensor\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # El formato 'albumentations' espera y devuelve normalizado [0,1]\n",
    "\n",
    "# Las transformaciones de validación/prueba se mantienen minimalistas\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) \n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecc35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
