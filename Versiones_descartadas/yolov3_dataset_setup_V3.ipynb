{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f6c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta version es la que funciona mejor ya que ademas de preparar el dataset visualiza el resultado OJO OJO OJO\n",
    "# Incluye dos versiones:ambas funcionan pero se supone que la segunda esta mas depurada ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216d35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00200.jpg. Bboxes iniciales (píxeles): 11\n",
      "DEBUG: Primer bbox pixel: [115, 1, 237, 97, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 11\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [115, 1, 237, 97], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 11\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (74.75, 52.65, 154.04999999999998, 115.05000000000001)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 11\n",
      "DEBUG: Primer bbox normalizada: [0.1796875, 0.1265625, 0.37031249999999993, 0.27656250000000004]\n",
      "DEBUG: Bboxes finales en formato YOLO: 11\n",
      "DEBUG: Primer bbox YOLO: [0, 0.27499999999999997, 0.20156250000000003, 0.19062499999999993, 0.15000000000000005]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00274.jpg. Bboxes iniciales (píxeles): 8\n",
      "DEBUG: Primer bbox pixel: [18, 350, 124, 456, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 8\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [18, 350, 124, 456], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 8\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (11.700000000000001, 279.5, 80.60000000000001, 348.4)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 8\n",
      "DEBUG: Primer bbox normalizada: [0.028125000000000004, 0.671875, 0.19375000000000003, 0.8374999999999999]\n",
      "DEBUG: Bboxes finales en formato YOLO: 8\n",
      "DEBUG: Primer bbox YOLO: [0, 0.11093750000000002, 0.7546875, 0.16562500000000002, 0.1656249999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00333.jpg. Bboxes iniciales (píxeles): 17\n",
      "DEBUG: Primer bbox pixel: [183, 144, 260, 287, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 17\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [183, 144, 260, 287], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 17\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (118.95, 145.6, 169.0, 238.54999999999998)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 17\n",
      "DEBUG: Primer bbox normalizada: [0.2859375, 0.35, 0.40625, 0.5734374999999999]\n",
      "DEBUG: Bboxes finales en formato YOLO: 17\n",
      "DEBUG: Primer bbox YOLO: [0, 0.34609375, 0.46171874999999996, 0.12031249999999999, 0.22343749999999996]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00218.jpg. Bboxes iniciales (píxeles): 11\n",
      "DEBUG: Primer bbox pixel: [166, 236, 284, 336, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 11\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [166, 236, 284, 336], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 11\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (107.9, 205.4, 184.6, 270.4)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 11\n",
      "DEBUG: Primer bbox normalizada: [0.259375, 0.49375, 0.44375, 0.6499999999999999]\n",
      "DEBUG: Bboxes finales en formato YOLO: 11\n",
      "DEBUG: Primer bbox YOLO: [0, 0.3515625, 0.5718749999999999, 0.18437499999999996, 0.1562499999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00273.jpg. Bboxes iniciales (píxeles): 15\n",
      "DEBUG: Primer bbox pixel: [227, 265, 329, 359, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 15\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [227, 265, 329, 359], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 15\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (147.54999999999998, 224.25, 213.85, 285.35)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 15\n",
      "DEBUG: Primer bbox normalizada: [0.35468749999999993, 0.5390625, 0.5140625, 0.6859375000000001]\n",
      "DEBUG: Bboxes finales en formato YOLO: 15\n",
      "DEBUG: Primer bbox YOLO: [0, 0.43437499999999996, 0.6125, 0.15937500000000004, 0.1468750000000001]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00256.jpg. Bboxes iniciales (píxeles): 6\n",
      "DEBUG: Primer bbox pixel: [455, 132, 596, 259, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 6\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [455, 132, 596, 259], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 6\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (295.75, 137.8, 387.40000000000003, 220.35)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 6\n",
      "DEBUG: Primer bbox normalizada: [0.7109375, 0.33125000000000004, 0.9312500000000001, 0.5296875]\n",
      "DEBUG: Bboxes finales en formato YOLO: 6\n",
      "DEBUG: Primer bbox YOLO: [0, 0.8210937500000001, 0.43046875, 0.22031250000000013, 0.19843749999999993]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00247.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [374, 16, 479, 112, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 14\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [374, 16, 479, 112], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (243.1, 62.4, 311.34999999999997, 124.8)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 14\n",
      "DEBUG: Primer bbox normalizada: [0.584375, 0.15, 0.7484374999999999, 0.3]\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.6664062499999999, 0.22499999999999998, 0.1640624999999999, 0.15]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00382.jpg. Bboxes iniciales (píxeles): 16\n",
      "DEBUG: Primer bbox pixel: [1, 345, 70, 441, 0]\n",
      "DEBUG: Bboxes para Albumentations (píxeles): 16\n",
      "DEBUG: Primer bbox para Albumentations (píxeles): [1, 345, 70, 441], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): 16\n",
      "DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): (0.65, 276.25, 45.5, 338.65)\n",
      "DEBUG: Dimensiones de la imagen transformada (H, W): (416, 416)\n",
      "DEBUG: Bboxes normalizadas (post-Albumentations): 16\n",
      "DEBUG: Primer bbox normalizada: [0.0015625, 0.6640625, 0.109375, 0.8140624999999999]\n",
      "DEBUG: Bboxes finales en formato YOLO: 16\n",
      "DEBUG: Primer bbox YOLO: [0, 0.05546875, 0.7390625, 0.1078125, 0.1499999999999999]\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 11 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([0.0000, 0.2750, 0.2016, 0.1906, 0.1500])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n"
     ]
    }
   ],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- MODIFICACIÓN CLAVE: Preparar bboxes en PÍXELES para Albumentations ---\n",
    "        # Albumentations necesita las coordenadas de las bboxes en el formato especificado en bbox_params.\n",
    "        # Si bbox_params.format es 'pascal_voc', espera [xmin, ymin, xmax, ymax] en píxeles.\n",
    "        bboxes_for_alb = []\n",
    "        class_labels_for_alb = []\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            bboxes_for_alb.append([xmin_px, ymin_px, xmax_px, ymax_px]) # Píxeles\n",
    "            class_labels_for_alb.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes para Albumentations (píxeles): {len(bboxes_for_alb)}\")\n",
    "        if bboxes_for_alb:\n",
    "            print(f\"DEBUG: Primer bbox para Albumentations (píxeles): {bboxes_for_alb[0]}, clase: {class_labels_for_alb[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas en píxeles y las transformará.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_for_alb, class_labels=class_labels_for_alb)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (ahora en píxeles de la imagen transformada)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, píxeles transformados): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, píxeles transformados): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- NUEVO PASO: Normalizar las bboxes de Albumentations a [0, 1] ---\n",
    "        # Obtener las dimensiones de la imagen después de Albumentations\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            transformed_h, transformed_w = image.shape[1], image.shape[2]\n",
    "        else: # numpy array\n",
    "            transformed_h, transformed_w = image.shape[0], image.shape[1]\n",
    "\n",
    "        print(f\"DEBUG: Dimensiones de la imagen transformada (H, W): ({transformed_h}, {transformed_w})\")\n",
    "\n",
    "        bboxes = [] # Reset bboxes to store normalized ones\n",
    "        for i, bbox_px_transformed in enumerate(bboxes_transformed_raw):\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px = bbox_px_transformed\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones de la imagen transformada\n",
    "            xmin_norm = xmin_px / transformed_w\n",
    "            ymin_norm = ymin_px / transformed_h\n",
    "            xmax_norm = xmax_px / transformed_w\n",
    "            ymax_norm = ymax_px / transformed_h\n",
    "\n",
    "            bboxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (post-Albumentations): {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox normalizada: {bboxes[0]}\")\n",
    "        # --- FIN NUEVO PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO ULTRA-MÍNIMAS PARA DEPURACIÓN ---\n",
    "train_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) # <--- FORMATO CORREGIDO A 'pascal_voc'\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) # <--- FORMATO CORREGIDO A 'pascal_voc'\n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eb89f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00029.jpg. Bboxes iniciales (píxeles): 24\n",
      "DEBUG: Primer bbox pixel: [75, 179, 224, 315, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 24\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.1171875, 0.3729166666666667, 0.35, 0.65625], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 24\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.1171875, 0.40468750000000003, 0.35, 0.6171875)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 24\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.1171875, 0.40468750000000003, 0.35, 0.6171875)\n",
      "DEBUG: Bboxes finales en formato YOLO: 24\n",
      "DEBUG: Primer bbox YOLO: [1, 0.23359375, 0.5109375, 0.23281249999999998, 0.21249999999999997]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00225.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [41, 272, 137, 371, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 14\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.0640625, 0.5666666666666667, 0.2140625, 0.7729166666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.0640625, 0.5499999999999999, 0.2140625, 0.7046874999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 14\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.0640625, 0.5499999999999999, 0.2140625, 0.7046874999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.13906249999999998, 0.6273437499999999, 0.15, 0.15468749999999998]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00378.jpg. Bboxes iniciales (píxeles): 16\n",
      "DEBUG: Primer bbox pixel: [26, 217, 124, 319, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 16\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.040625, 0.45208333333333334, 0.19375, 0.6645833333333333], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 16\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.04062500000000001, 0.46406250000000004, 0.19375000000000003, 0.6234375000000001)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 16\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.04062500000000001, 0.46406250000000004, 0.19375000000000003, 0.6234375000000001)\n",
      "DEBUG: Bboxes finales en formato YOLO: 16\n",
      "DEBUG: Primer bbox YOLO: [0, 0.11718750000000003, 0.5437500000000001, 0.153125, 0.15937500000000004]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00148.jpg. Bboxes iniciales (píxeles): 6\n",
      "DEBUG: Primer bbox pixel: [404, 253, 511, 356, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 6\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.63125, 0.5270833333333333, 0.7984375, 0.7416666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 6\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.6312499999999999, 0.5203125000000001, 0.7984375000000001, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 6\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.6312499999999999, 0.5203125000000001, 0.7984375000000001, 0.6812499999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 6\n",
      "DEBUG: Primer bbox YOLO: [0, 0.71484375, 0.60078125, 0.16718750000000027, 0.16093749999999984]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00236.jpg. Bboxes iniciales (píxeles): 9\n",
      "DEBUG: Primer bbox pixel: [514, 187, 633, 293, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 9\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.803125, 0.38958333333333334, 0.9890625, 0.6104166666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 9\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.8031249999999999, 0.41718750000000004, 0.9890625, 0.5828125000000001)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 9\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.8031249999999999, 0.41718750000000004, 0.9890625, 0.5828125000000001)\n",
      "DEBUG: Bboxes finales en formato YOLO: 9\n",
      "DEBUG: Primer bbox YOLO: [0, 0.8960937499999999, 0.5, 0.1859375000000001, 0.16562500000000002]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00135.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [20, 243, 105, 328, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 14\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.03125, 0.50625, 0.1640625, 0.6833333333333333], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.03125, 0.5046875, 0.1640625, 0.6375000000000001)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 14\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.03125, 0.5046875, 0.1640625, 0.6375000000000001)\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.09765625, 0.57109375, 0.1328125, 0.1328125000000001]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00307.jpg. Bboxes iniciales (píxeles): 14\n",
      "DEBUG: Primer bbox pixel: [81, 18, 194, 115, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 14\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.1265625, 0.0375, 0.303125, 0.23958333333333334], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 14\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.1265625, 0.153125, 0.303125, 0.3046875)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 14\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.1265625, 0.153125, 0.303125, 0.3046875)\n",
      "DEBUG: Bboxes finales en formato YOLO: 14\n",
      "DEBUG: Primer bbox YOLO: [0, 0.21484375, 0.22890625, 0.17656249999999998, 0.1515625]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00268.jpg. Bboxes iniciales (píxeles): 1\n",
      "DEBUG: Primer bbox pixel: [124, 1, 328, 172, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 1\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.19375, 0.0020833333333333333, 0.5125, 0.35833333333333334], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 1\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.19375000000000003, 0.1265625, 0.5125, 0.39375000000000004)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 1\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.19375000000000003, 0.1265625, 0.5125, 0.39375000000000004)\n",
      "DEBUG: Bboxes finales en formato YOLO: 1\n",
      "DEBUG: Primer bbox YOLO: [1, 0.353125, 0.26015625000000003, 0.3187499999999999, 0.2671875]\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 24 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([1.0000, 0.2336, 0.5109, 0.2328, 0.2125])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n"
     ]
    }
   ],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "        # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "        bboxes_normalized_initial = []\n",
    "        class_labels = [] # class_labels se mantiene\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "        if bboxes_normalized_initial:\n",
    "            print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "            # Se espera que devuelva coordenadas normalizadas también.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "        # Si Albumentations funciona como se espera con format='albumentations',\n",
    "        # este paso ya no es necesario.\n",
    "        bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "        # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO ULTRA-MÍNIMAS PARA DEPURACIÓN ---\n",
    "train_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # <--- FORMATO CAMBIADO A 'albumentations'\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # <--- FORMATO CAMBIADO A 'albumentations'\n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4816df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando todas las anotaciones desde: C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset\\annotations.csv\n",
      "Total de 364 imágenes únicas encontradas en el CSV.\n",
      "Imágenes para entrenamiento: 254\n",
      "Imágenes para validación: 55\n",
      "Imágenes para prueba: 55\n",
      "Dataset inicializado con 254 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "Dataset inicializado con 55 imágenes.\n",
      "\n",
      "Dataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\n",
      "\n",
      "Verificando la carga de un lote de entrenamiento...\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00004.jpg. Bboxes iniciales (píxeles): 13\n",
      "DEBUG: Primer bbox pixel: [109, 134, 324, 321, 1]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 13\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.1703125, 0.2791666666666667, 0.50625, 0.66875], clase: 1\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 13\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.17031250000000003, 0.33437500000000003, 0.50625, 0.6265624999999999)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 13\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.17031250000000003, 0.33437500000000003, 0.50625, 0.6265624999999999)\n",
      "DEBUG: Bboxes finales en formato YOLO: 13\n",
      "DEBUG: Primer bbox YOLO: [1, 0.33828125, 0.48046875, 0.33593749999999994, 0.2921874999999999]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00410.jpg. Bboxes iniciales (píxeles): 18\n",
      "DEBUG: Primer bbox pixel: [233, 368, 338, 452, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 18\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.3640625, 0.7666666666666667, 0.528125, 0.9416666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 18\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.36406250000000007, 0.7000000000000001, 0.528125, 0.83125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 18\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.36406250000000007, 0.7000000000000001, 0.528125, 0.83125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 18\n",
      "DEBUG: Primer bbox YOLO: [0, 0.44609375, 0.765625, 0.1640624999999999, 0.13124999999999998]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00191.jpg. Bboxes iniciales (píxeles): 15\n",
      "DEBUG: Primer bbox pixel: [353, 1, 462, 101, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 15\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.5515625, 0.0020833333333333333, 0.721875, 0.21041666666666667], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 15\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.5515625, 0.1265625, 0.721875, 0.2828125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 15\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.5515625, 0.1265625, 0.721875, 0.2828125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 15\n",
      "DEBUG: Primer bbox YOLO: [0, 0.63671875, 0.20468750000000002, 0.1703125000000001, 0.15625000000000003]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00271.jpg. Bboxes iniciales (píxeles): 13\n",
      "DEBUG: Primer bbox pixel: [507, 337, 613, 455, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 13\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.7921875, 0.7020833333333333, 0.9578125, 0.9479166666666666], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 13\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.7921875, 0.6515624999999999, 0.9578125, 0.8359375)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 13\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.7921875, 0.6515624999999999, 0.9578125, 0.8359375)\n",
      "DEBUG: Bboxes finales en formato YOLO: 13\n",
      "DEBUG: Primer bbox YOLO: [0, 0.875, 0.7437499999999999, 0.1656249999999999, 0.18437500000000007]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00192.jpg. Bboxes iniciales (píxeles): 13\n",
      "DEBUG: Primer bbox pixel: [137, 352, 243, 453, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 13\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.2140625, 0.7333333333333333, 0.3796875, 0.94375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 13\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.2140625, 0.6749999999999999, 0.37968750000000007, 0.8328125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 13\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.2140625, 0.6749999999999999, 0.37968750000000007, 0.8328125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 13\n",
      "DEBUG: Primer bbox YOLO: [0, 0.296875, 0.75390625, 0.16562500000000008, 0.15781250000000002]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00319.jpg. Bboxes iniciales (píxeles): 13\n",
      "DEBUG: Primer bbox pixel: [416, 371, 512, 478, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 13\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.65, 0.7729166666666667, 0.8, 0.9958333333333333], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 13\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.6500000000000001, 0.7046874999999999, 0.8, 0.871875)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 13\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.6500000000000001, 0.7046874999999999, 0.8, 0.871875)\n",
      "DEBUG: Bboxes finales en formato YOLO: 13\n",
      "DEBUG: Primer bbox YOLO: [0, 0.7250000000000001, 0.7882812499999999, 0.1499999999999999, 0.16718750000000004]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00156.jpg. Bboxes iniciales (píxeles): 16\n",
      "DEBUG: Primer bbox pixel: [169, 326, 296, 405, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 16\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.2640625, 0.6791666666666667, 0.4625, 0.84375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 16\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.2640625, 0.6343749999999999, 0.4625, 0.7578125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 16\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.2640625, 0.6343749999999999, 0.4625, 0.7578125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 16\n",
      "DEBUG: Primer bbox YOLO: [0, 0.36328125, 0.69609375, 0.19843750000000004, 0.12343750000000009]\n",
      "DEBUG: Imagen original (H, W): (480, 640)\n",
      "DEBUG: __getitem__ para BloodImage_00057.jpg. Bboxes iniciales (píxeles): 11\n",
      "DEBUG: Primer bbox pixel: [488, 252, 596, 357, 0]\n",
      "DEBUG: Bboxes normalizadas (iniciales): 11\n",
      "DEBUG: Primer bbox normalizada (inicial): [0.7625, 0.525, 0.93125, 0.74375], clase: 0\n",
      "DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): 11\n",
      "DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): (0.7625, 0.51875, 0.9312500000000001, 0.6828125)\n",
      "DEBUG: Bboxes finales antes de YOLO format: 11\n",
      "DEBUG: Primer bbox final antes de YOLO format: (0.7625, 0.51875, 0.9312500000000001, 0.6828125)\n",
      "DEBUG: Bboxes finales en formato YOLO: 11\n",
      "DEBUG: Primer bbox YOLO: [0, 0.846875, 0.60078125, 0.16875000000000018, 0.1640625]\n",
      "Tamaño del lote 1: Imágenes: torch.Size([8, 3, 416, 416]), Targets: 8\n",
      "--- Encontrada imagen con 13 cajas en el lote 1, imagen 1 ---\n",
      "Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\n",
      "tensor([1.0000, 0.3383, 0.4805, 0.3359, 0.2922])\n",
      "\n",
      "Visualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\n"
     ]
    }
   ],
   "source": [
    "# --- Importar las librerías necesarias ---\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PASO 1: Definición de la clase BloodCellDataset\n",
    "# Esta clase debe manejar la carga de imágenes y anotaciones, así como las transformaciones necesarias.\n",
    "# Modificada para que filtre las bounding boxes degeneradas o inválidas.\n",
    "\n",
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, data_root, annotations_df, image_size=(416, 416), transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_folder = os.path.join(data_root, 'BCCD')\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.class_name_to_id = {\n",
    "            'RBC': 0, 'WBC': 1, 'Platelets': 2\n",
    "        }\n",
    "        self.class_id_to_name = {\n",
    "            0: 'RBC', 1: 'WBC', 2: 'Platelets'\n",
    "        }\n",
    "        \n",
    "        self.image_annotations = {}\n",
    "        # Filtrar el DataFrame de anotaciones para eliminar filas con valores NaN en columnas clave\n",
    "        annotations_df = annotations_df.dropna(subset=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'cell_type'])\n",
    "        \n",
    "        for filename, group in annotations_df.groupby('filename'):\n",
    "            bboxes_pixel_list = []\n",
    "            for idx, row in group.iterrows():\n",
    "                cell_type = str(row['cell_type']) # Asegurarse de que sea string\n",
    "                xmin = int(row['xmin'])\n",
    "                xmax = int(row['xmax'])\n",
    "                ymin = int(row['ymin'])\n",
    "                ymax = int(row['ymax']) \n",
    "                \n",
    "                class_id = self.class_name_to_id.get(cell_type)\n",
    "                if class_id is None:\n",
    "                    print(f\"Advertencia: Tipo de célula desconocido '{cell_type}' en el archivo {filename}. Saltando anotación.\")\n",
    "                    continue\n",
    "\n",
    "                # Asegurarse de que xmin < xmax y ymin < ymax antes de guardar\n",
    "                if xmin >= xmax or ymin >= ymax:\n",
    "                    # print(f\"Advertencia: Bounding box degenerado o inválido en {filename}: ({xmin}, {ymin}, {xmax}, {ymax}). Saltando.\")\n",
    "                    continue # Saltar esta bbox inválida\n",
    "\n",
    "                bboxes_pixel_list.append([xmin, ymin, xmax, ymax, class_id])\n",
    "            \n",
    "            # Solo añadir la imagen si tiene al menos una bbox válida\n",
    "            if bboxes_pixel_list:\n",
    "                self.image_annotations[filename] = bboxes_pixel_list\n",
    "        \n",
    "        self.image_files = list(self.image_annotations.keys())\n",
    "        print(f\"Dataset inicializado con {len(self.image_files)} imágenes.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_h, original_w, _ = image.shape\n",
    "        print(f\"DEBUG: Imagen original (H, W): ({original_h}, {original_w})\")\n",
    "\n",
    "        bboxes_pixel = self.image_annotations.get(img_name, [])\n",
    "        print(f\"DEBUG: __getitem__ para {img_name}. Bboxes iniciales (píxeles): {len(bboxes_pixel)}\")\n",
    "        if bboxes_pixel:\n",
    "            print(f\"DEBUG: Primer bbox pixel: {bboxes_pixel[0]}\")\n",
    "        \n",
    "        # --- NORMALIZAR BBOXES A [0, 1] ANTES DE ALBUMENTATIONS ---\n",
    "        # Albumentations espera bboxes normalizadas si format='albumentations'\n",
    "        bboxes_normalized_initial = []\n",
    "        class_labels = [] # class_labels se mantiene\n",
    "        for bbox_px in bboxes_pixel:\n",
    "            xmin_px, ymin_px, xmax_px, ymax_px, class_id = bbox_px\n",
    "            \n",
    "            # Normalizar las coordenadas a [0, 1] usando las dimensiones originales\n",
    "            xmin_norm = xmin_px / original_w\n",
    "            ymin_norm = ymin_px / original_h\n",
    "            xmax_norm = xmax_px / original_w\n",
    "            ymax_norm = ymax_px / original_h\n",
    "            \n",
    "            bboxes_normalized_initial.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            class_labels.append(class_id)\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes normalizadas (iniciales): {len(bboxes_normalized_initial)}\")\n",
    "        if bboxes_normalized_initial:\n",
    "            print(f\"DEBUG: Primer bbox normalizada (inicial): {bboxes_normalized_initial[0]}, clase: {class_labels[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # Albumentations ahora recibe coordenadas normalizadas y las transformará.\n",
    "            # Se espera que devuelva coordenadas normalizadas también.\n",
    "            transformed = self.transform(image=image, bboxes=bboxes_normalized_initial, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes_transformed_raw = transformed['bboxes'] # Bboxes después de Albumentations (deberían estar normalizadas)\n",
    "            class_labels = transformed['class_labels'] # Las etiquetas de clase se mantienen\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes después de Albumentations (raw, deberían estar normalizadas): {len(bboxes_transformed_raw)}\")\n",
    "        if bboxes_transformed_raw:\n",
    "            print(f\"DEBUG: Primer bbox después de Albumentations (raw, deberían estar normalizadas): {bboxes_transformed_raw[0]}\")\n",
    "\n",
    "        # --- ELIMINAR PASO DE RE-NORMALIZACIÓN HEURÍSTICA ---\n",
    "        # Si Albumentations funciona como se espera con format='albumentations',\n",
    "        # este paso ya no es necesario.\n",
    "        bboxes = bboxes_transformed_raw # Usar las bboxes directamente de Albumentations\n",
    "        \n",
    "        print(f\"DEBUG: Bboxes finales antes de YOLO format: {len(bboxes)}\")\n",
    "        if bboxes:\n",
    "            print(f\"DEBUG: Primer bbox final antes de YOLO format: {bboxes[0]}\")\n",
    "        # --- FIN ELIMINAR PASO ---\n",
    "\n",
    "\n",
    "        # Si ToTensorV2 ya se aplicó, la imagen es un tensor. Si no, convertirla.\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        yolo_bboxes = []\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            \n",
    "            # Asegurar que las coordenadas estén dentro de [0, 1]\n",
    "            x_min = max(0.0, min(1.0, x_min))\n",
    "            y_min = max(0.0, min(1.0, y_min))\n",
    "            x_max = max(0.0, min(1.0, x_max))\n",
    "            y_max = max(0.0, min(1.0, y_max))\n",
    "\n",
    "            center_x = (x_min + x_max) / 2\n",
    "            width = x_max - x_min\n",
    "            center_y = (y_min + y_max) / 2\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # Este filtrado ya estaba, pero el error ocurría antes\n",
    "            if width <= 0 or height <= 0:\n",
    "                print(f\"DEBUG: Bbox filtrada por width/height <= 0: {bbox}\")\n",
    "                continue # Saltar esta bbox inválida después de transformación\n",
    "\n",
    "            yolo_bboxes.append([class_labels[i], center_x, center_y, width, height])\n",
    "            \n",
    "        print(f\"DEBUG: Bboxes finales en formato YOLO: {len(yolo_bboxes)}\")\n",
    "        if yolo_bboxes:\n",
    "            print(f\"DEBUG: Primer bbox YOLO: {yolo_bboxes[0]}\")\n",
    "\n",
    "        if len(yolo_bboxes) == 0:\n",
    "            # Devuelve un tensor vacío si no hay bboxes válidas\n",
    "            yolo_bboxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            yolo_bboxes = torch.tensor(yolo_bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return image, yolo_bboxes\n",
    "\n",
    "# PASO 2: Definición de las transformaciones de Albumentations\n",
    "# Define el tamaño de entrada de tu modelo YOLOv3 (416x416)\n",
    "\n",
    "YOLO_INPUT_SIZE = (416, 416) \n",
    "\n",
    "# --- TRANSFORMACIONES DE ENTRENAMIENTO ULTRA-MÍNIMAS PARA DEPURACIÓN ---\n",
    "train_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # <--- FORMATO CAMBIADO A 'albumentations'\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=YOLO_INPUT_SIZE[0], p=1.0), \n",
    "    A.PadIfNeeded(min_height=YOLO_INPUT_SIZE[0], min_width=YOLO_INPUT_SIZE[1], border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels'])) # <--- FORMATO CAMBIADO A 'albumentations'\n",
    "\n",
    "# PASO 3: Definición de la función Collate_fn para el DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, bbox_target in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_target) \n",
    "    images = torch.stack(images, 0)\n",
    "    return images, bboxes\n",
    "\n",
    "# PASO 4: Definición de la Lógica de División del Dataset y Creación de DataLoaders\n",
    "# Modificada para que visualice las imágenes con las bounding boxes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # RUTAS A LOS DATOS\n",
    "    DATA_ROOT = 'C:/Users/gtoma/Master_AI_Aplicada/GitHubRep/PyTorch-YOLOv3/dataset'\n",
    "    CSV_FILE = os.path.join(DATA_ROOT, 'annotations.csv') \n",
    "        \n",
    "    # Parámetros de la división\n",
    "    TEST_SPLIT_RATIO = 0.15    \n",
    "    VAL_SPLIT_RATIO = 0.15     \n",
    "    RANDOM_SEED = 42           \n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0 # Deja en 0 para depuración, luego puedes aumentarlo a 4-8\n",
    "\n",
    "    # --- Cargar todas las anotaciones y obtener nombres de archivo únicos ---\n",
    "    print(f\"Cargando todas las anotaciones desde: {CSV_FILE}\")\n",
    "    full_df = pd.read_csv(CSV_FILE)\n",
    "    \n",
    "    # Obtener la lista de nombres de archivo únicos presentes en el CSV\n",
    "    all_image_filenames = full_df['filename'].unique().tolist()\n",
    "    print(f\"Total de {len(all_image_filenames)} imágenes únicas encontradas en el CSV.\")\n",
    "\n",
    "    # --- Dividir los nombres de archivo en entrenamiento y test ---\n",
    "    train_val_filenames, test_filenames = train_test_split(\n",
    "        all_image_filenames, \n",
    "        test_size=TEST_SPLIT_RATIO, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_filenames, val_filenames = train_test_split(\n",
    "        train_val_filenames, \n",
    "        test_size=VAL_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO), \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Imágenes para entrenamiento: {len(train_filenames)}\")\n",
    "    print(f\"Imágenes para validación: {len(val_filenames)}\")\n",
    "    print(f\"Imágenes para prueba: {len(test_filenames)}\")\n",
    "\n",
    "    # --- Crear DataFrames de anotaciones para cada split ---\n",
    "    train_df = full_df[full_df['filename'].isin(train_filenames)].copy()\n",
    "    val_df = full_df[full_df['filename'].isin(val_filenames)].copy()\n",
    "    test_df = full_df[full_df['filename'].isin(test_filenames)].copy()\n",
    "\n",
    "    # --- Crear instancias del Dataset y DataLoader para cada split ---\n",
    "    train_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=train_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    val_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=val_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "    test_dataset = BloodCellDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        annotations_df=test_df, \n",
    "        image_size=YOLO_INPUT_SIZE,\n",
    "        transform=val_test_transforms \n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset y DataLoaders de entrenamiento, validación y prueba configurados exitosamente.\")\n",
    "\n",
    "    # --- Verificación de la carga de un lote de entrenamiento ---\n",
    "    print(\"\\nVerificando la carga de un lote de entrenamiento...\")\n",
    "    MAX_BATCHES_TO_CHECK = 10 \n",
    "    found_image_with_boxes = False\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        print(f\"Tamaño del lote {batch_idx+1}: Imágenes: {images.shape}, Targets: {len(targets)}\")\n",
    "        \n",
    "        # Buscar una imagen con cajas en el lote actual\n",
    "        for img_idx in range(len(targets)):\n",
    "            if targets[img_idx].numel() > 0: \n",
    "                print(f\"--- Encontrada imagen con {targets[img_idx].shape[0]} cajas en el lote {batch_idx+1}, imagen {img_idx+1} ---\")\n",
    "                print(f\"Ejemplo de target para esta imagen (clase, cx, cy, w, h normalizados):\")\n",
    "                print(targets[img_idx][0])\n",
    "                \n",
    "                # --- Lógica de visualización ---\n",
    "                mean = torch.tensor((0.485, 0.456, 0.406)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                std = torch.tensor((0.229, 0.224, 0.225)).view(3, 1, 1).to(images[img_idx].device)\n",
    "                \n",
    "                img_display_rgb = (images[img_idx] * std + mean) * 255\n",
    "                img_display_rgb = img_display_rgb.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "                img_display_bgr = cv2.cvtColor(img_display_rgb, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                img_h, img_w = img_display_bgr.shape[:2]\n",
    "                \n",
    "                CLASS_ID_TO_NAME_MAP = {0: 'RBC', 1: 'WBC', 2: 'Platelets'}\n",
    "                CLASS_COLORS_MAP = {0: (0, 0, 255), 1: (0, 255, 0), 2: (255, 0, 0)} # BGR\n",
    "\n",
    "                print(\"\\nVisualizando la imagen con GT Boxes (presiona cualquier tecla para cerrar)...\")\n",
    "                for bbox_yolo in targets[img_idx].tolist():\n",
    "                    class_id, cx, cy, w, h = bbox_yolo\n",
    "                    \n",
    "                    x_min_norm = cx - w/2\n",
    "                    y_min_norm = cy - h/2\n",
    "                    x_max_norm = cx + w/2\n",
    "                    y_max_norm = cy + h/2\n",
    "\n",
    "                    x_min_px = int(x_min_norm * img_w)\n",
    "                    y_min_px = int(y_min_norm * img_h)\n",
    "                    x_max_px = int(x_max_norm * img_w)\n",
    "                    y_max_px = int(y_max_norm * img_h)\n",
    "\n",
    "                    color = CLASS_COLORS_MAP.get(int(class_id), (255, 255, 255)) \n",
    "                    cv2.rectangle(img_display_bgr, (x_min_px, y_min_px), (x_max_px, y_max_px), color, 2)\n",
    "\n",
    "                    label_text = f\"{CLASS_ID_TO_NAME_MAP.get(int(class_id), 'Unknown')}\"\n",
    "                    text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 5 if y_min_px - 5 > 5 else y_min_px + text_size[1] + 5\n",
    "                    \n",
    "                    cv2.rectangle(img_display_bgr, (text_x, text_y - text_size[1] - 5), \n",
    "                                  (text_x + text_size[0] + 5, text_y + 5), color, -1)\n",
    "                    cv2.putText(img_display_bgr, label_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                cv2.imshow(\"Imagen con GT Boxes\", img_display_bgr)\n",
    "                cv2.waitKey(0) \n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                found_image_with_boxes = True\n",
    "                break \n",
    "        \n",
    "        if found_image_with_boxes or batch_idx + 1 >= MAX_BATCHES_TO_CHECK:\n",
    "            break \n",
    "\n",
    "    if not found_image_with_boxes:\n",
    "        print(f\"\\nNo se encontró ninguna imagen con bounding boxes en los primeros {MAX_BATCHES_TO_CHECK} lotes.\")\n",
    "        print(\"Esto podría deberse a que todas las imágenes mostradas no tenían bboxes o fueron filtradas.\")\n",
    "        print(\"Considera revisar:\")\n",
    "        print(\"1. El contenido de 'annotations.csv' para asegurar que hay bboxes válidas.\")\n",
    "        print(\"2. Los filtros en BloodCellDataset (xmin >= xmax, etc.).\")\n",
    "        print(\"3. Los parámetros de bbox en Albumentations (min_area, min_visibility).\")\n",
    "        print(\"4. Si RandomCrop está eliminando demasiadas bboxes si son pequeñas o están en los bordes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecc35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_13042025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
